[
  {
    "objectID": "chap01.html",
    "href": "chap01.html",
    "title": "1  Conceitos preliminares",
    "section": "",
    "text": "Definição 1.1 (População alvo) Define-se população alvo, ou simplesmente população, a um conjunto de unidades experimentais.\n\n\nDefinição 1.2 (Amostra aleatória) A sequência \\(X_1,X_2,\\ldots,X_n\\) de variáveis aleatórias independentes e identicamente distribuídas define uma amostra aleatória de tamanho \\(n\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSe as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) configuram uma amostra aleatória de uma população com distribuição contínua, então a função de densidade conjunta é dada por \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\cdots f(x_n)=\\prod_{i=1}^nf(x_i), \\tag{1.1}\\] onde \\(f(\\cdot)\\) representa a função de densidade marginal para cada variável aleatória \\(X_i\\), com \\(i=1,2,\\ldots,n\\). Nesse caso a Equação 1.1 representa a distribuição da amostra aleatória.\n\n\n\nExemplo 1.1 Seja \\(X\\) uma variável aleatória de uma população com densidade \\(f(\\cdot)\\). Seja \\(X_1,X_2\\) uma amostra aleatória de tamanho \\(2\\). Por definição, a distribuição conjunta de \\(X_1,X_2\\) é dada por \\[f(x_1,x_2)=f(x_1)f(x_2).\\]\n\n\nExemplo 1.2 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\), i.e.,\n\\[\\mathbb{P}[X=x]=p^x(1-p)^{1-x}\\mathbb{I}_{\\{0,1\\}}(x).\\] Daí,\n\n\\[\\begin{align*}\n    \\mathbb{P}[X_1=x_1,X_2=x_2]&=\\mathbb{P}[X_1=x_1]\\mathbb{P}[X_2=x_2]\\\\\n    &=p^{x_1}(1-p)^{1-x_1}\\mathbb{I}_{\\{0,1\\}}(x_1)p^{x_2}(1-p)^{1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_2)\\\\\n    &=p^{x_1+x_2}(1-p)^{2-x_1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_1)\\mathbb{I}_{\\{0,1\\}}(x_2).\n  \\end{align*}\\]\n\nNesse caso, a distribuição conjunta da amostra está definida no pares \\((x_1,x_2)\\in\\{(0,0), (0,1), (1,0), (1,1)\\}\\).\nCabe ressaltar que, a distribuição conjunta da amostra não é igual à distribuição obtida considerando os elementos de uma distribuição tipo binomial. No caso da distribuição binomial, seja \\(Y=X_1+X_2\\) (com prob. \\(1\\)) então \\[\\mathbb{P}[Y=y]=\\binom{2}{y}p^y(1-p)^{2-y}\\mathbb{I}_{\\{0,1,2\\}}(y).\\] Dessa forma, a distribuição binomial é definida para uma única variável que representa o número de sucessos, e difere da distribuição conjunta da amostra porque a binomial não considera a ordem da coleta, por exemplo, para a distribuição conjunta da amostra, \\(\\mathbb{P}[X_1=0,X_2=1]\\) representa a probabilidade de coletar primeiro \\(0\\) e depois \\(1\\)."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Introdução",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. \"Literate Programming\". Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "chap02.html#sec-momentos_amostrais",
    "href": "chap02.html#sec-momentos_amostrais",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.1 Momentos amostrais",
    "text": "2.1 Momentos amostrais\n\nDefinição 2.2 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 2.3 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 2.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a Definição 2.1, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\n\n\n\n\n\nLembre que…\n\n\n\n\n\nSe \\(X\\) é uma variável aleatória, o \\(r\\)-ésimo momento de \\(X\\) define-se como \\(\\mathbb{E}[X^r]=\\mu_r'\\), se existe. Também pode-se definir o \\(r\\)-ésimo momento central como \\(\\mathbb{E}[(X-\\mu_X)^r]=\\mu_r\\), onde \\(\\mu_X=\\mathbb{E}[X]=\\mu_1'\\).\nPara \\(r=2\\), temos que \\(\\mathbb{E}[X-\\mu_X]^2=\\mathrm{Var}[X]\\).\n\n\n\n\nExemplo 2.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r>-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 2.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 2.1, sabemos da Definição 2.2 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 2.4 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n>1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 2.5 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n>1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n>1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 2.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 2.4. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n>1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^k\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n>1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i<k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]"
  },
  {
    "objectID": "chap03.html#sec-prop_estimadores",
    "href": "chap03.html#sec-prop_estimadores",
    "title": "3  Estimação pontual",
    "section": "3.1 Propriedades dos estimadores",
    "text": "3.1 Propriedades dos estimadores\n\nDefinição 3.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Billingsley, Patrick. 1986. Probability and Measure. Second.\nJohn Wiley; Sons.\n\n\nCurtiss, J. H. 1942. “A Note on the Theory Moment Generating\nFunctions.” The Annals of the Mathematical Statistics 13\n(4): 430–33.\n\n\nDudewicz, E., and S. Mishra. 1988. Modern Mathematical\nStatistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An Introduction to Probability Theory and Its\nApplications. 2nd ed. Vol. II. Wiley.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMood, A., F. Graybill, and D. Boes. 1974. Introduction to the Theory\nof Statistics. 3rd ed. McGraw-Hill Higher Education.\n\n\nShohat, J. A., and J. D. Tamarkin. 1970. The Problem of\nMoments. Vol. I. American Mathematical Society."
  },
  {
    "objectID": "matematica.html",
    "href": "matematica.html",
    "title": "Apêndice A — Resultados matemáticos",
    "section": "",
    "text": "A.1 Somas e Produtos\nAlgumas fórmulas envolvendo somas são listadas a seguir:",
    "crumbs": [
      "Apêndices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Resultados matemáticos</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animações em Estatística com python",
    "section": "",
    "text": "Prefácio\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\nMostrar script\n1 + 1\n\n\n[1] 2",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "matematica.html#somas-e-produtos",
    "href": "matematica.html#somas-e-produtos",
    "title": "Resultados matemáticos",
    "section": "Somas e Produtos",
    "text": "Somas e Produtos\nAlgumas fórmulas envolvendo somas são listadas a seguir:\n\n\\(\\sum_{i=1}^ni=\\frac{n(n+1)}{2}\\);\n\\(\\sum_{i=1}^ni^2=\\frac{n(n+1)(2n+1)}{6}\\);\n\\(\\sum_{i=1}^ni^3=\\left[\\frac{n(n+1)}{2}\\right]^2\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^nb_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ib_j\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)^2=\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^na_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ia_j\\);\nTeorema multinomial: \\(\\left(\\sum_{i=1}^na_i\\right)^n=\\sum_{k_1,k_2,\\ldots,k_m}^n\\frac{n!}{k_1!k_2!\\cdots k_m!}x_1^{k_1}x_2^{k_2}\\cdots x_m^{k_m}\\), onde \\(\\sum_{i=1}^mk_i=n\\);\n\nBinomio de Newton: \\((a+b)^n=\\sum_{i=0}^n\\binom{n}{i}a^ib^{n-j}\\);\nTrinomio de Newton: \\((a+b+c)^n=\\sum_{i=0}^n\\sum_{j=0}^n\\frac{n!}{(n-i)!(i-j)!j!}a^{n-i}b^{i-j}c^j\\);"
  },
  {
    "objectID": "matematica.html#fatoriais-e-combinatórias",
    "href": "matematica.html#fatoriais-e-combinatórias",
    "title": "Appendix A — Resultados matemáticos",
    "section": "A.2 Fatoriais e Combinatórias",
    "text": "A.2 Fatoriais e Combinatórias\n\nFunção gamma: \\(\\Gamma(x)=\\int_0^\\infty t^{x-1}e^{-t}\\,dt\\), para \\(x>0\\). Dessa forma, \\(\\Gamma(x+1)=x\\Gamma(x)\\);\n\nSe \\(n\\) é inteiro, então \\(\\Gamma(n+1)=n!\\), onde \\(n!=1\\cdot2\\cdot3\\cdots (n-1)\\cdot n\\);\nSe \\(n\\) é inteiro, \\(\\Gamma\\left(n+\\frac12\\right)=\\frac{1\\cdot3\\cdot5\\cdots(2n-1)}{2^n}\\sqrt\\pi\\). Em particular, \\(\\Gamma\\left(\\frac32\\right)=\\frac12\\Gamma\\left(\\frac12\\right)=\\frac12\\sqrt\\pi\\) ou \\(\\Gamma\\left(\\frac12\\right)=\\sqrt\\pi\\);\nFórmula de Stirling: \\(n!\\approx\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n\\)\n\nFunção beta: \\(B(a,b)=\\int_0^1x^{a-1}(1-x)^{b-1}\\,dx\\), para \\(a>0\\) e \\(b>0\\). De outra forma, \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\);\n\\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\);\n\n\\(\\binom{n}{0}=\\binom{n}{n}=1\\);\n\\(\\binom{n}{k}=\\binom{n}{n-k}\\);\n\\(\\binom{n+1}{k}=\\binom{n}{k}+\\binom{n}{k-1}\\), para \\(n=1,2,\\ldots\\) e \\(k=0,\\pm1,\\pm2,\\ldots\\);\n\\(\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots(-n-k+1)}{k!}=(-1)^k\\binom{n+k-1}{k}\\);\n\n\n\n\n\n\n\nMood, A., F. Graybill, e D. Boes. 1974. Introduction to the theory of statistics. 3rd ed. McGraw-Hill Higher Education."
  },
  {
    "objectID": "matematica.html#sec-somas",
    "href": "matematica.html#sec-somas",
    "title": "Apêndice A — Resultados matemáticos",
    "section": "",
    "text": "\\(\\sum_{i=1}^ni=\\frac{n(n+1)}{2}\\);\n\\(\\sum_{i=1}^ni^2=\\frac{n(n+1)(2n+1)}{6}\\);\n\\(\\sum_{i=1}^ni^3=\\left[\\frac{n(n+1)}{2}\\right]^2\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^nb_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ib_j\\);\n\\(\\left(\\sum_{i=1}^na_i\\right)^2=\\left(\\sum_{i=1}^na_i\\right)\\left(\\sum_{j=1}^na_j\\right)=\\sum_{i=1}^n\\sum_{j=1}^na_ia_j\\);\nTeorema multinomial: \\(\\left(\\sum_{i=1}^na_i\\right)^n=\\sum_{k_1,k_2,\\ldots,k_m}^n\\frac{n!}{k_1!k_2!\\cdots k_m!}x_1^{k_1}x_2^{k_2}\\cdots x_m^{k_m}\\), onde \\(\\sum_{i=1}^mk_i=n\\);\n\nBinomio de Newton: \\((a+b)^n=\\sum_{i=0}^n\\binom{n}{i}a^ib^{n-j}\\);\nTrinomio de Newton: \\((a+b+c)^n=\\sum_{i=0}^n\\sum_{j=0}^n\\frac{n!}{(n-i)!(i-j)!j!}a^{n-i}b^{i-j}c^j\\);",
    "crumbs": [
      "Apêndices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Resultados matemáticos</span>"
    ]
  },
  {
    "objectID": "chap02.html",
    "href": "chap02.html",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "",
    "text": "3 Tipos de convergência"
  },
  {
    "objectID": "chap02.html#chap-convergencia",
    "href": "chap02.html#chap-convergencia",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Tipos de convergência",
    "text": "2.2 Tipos de convergência"
  },
  {
    "objectID": "chap04.html#sec-prop_estimadores",
    "href": "chap04.html#sec-prop_estimadores",
    "title": "4  Estimação pontual",
    "section": "4.1 Propriedades dos estimadores",
    "text": "4.1 Propriedades dos estimadores\n\nDefinição 4.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "chap02.html#sec-exercicios_chap02",
    "href": "chap02.html#sec-exercicios_chap02",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Exercícios sugeridos",
    "text": "2.2 Exercícios sugeridos"
  },
  {
    "objectID": "chap02.html#exercícios-sugeridos",
    "href": "chap02.html#exercícios-sugeridos",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nSeja \\(X\\) uma variável aletória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\). Verifique que \\(\\mathbb{E}[X]=\\mathcal{M}'_X(1)=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}\\) e \\(\\mathrm{Var}[X]=\\mathcal{M}''_X(1)+\\mathcal{M}'_X(1)-\\left[\\mathcal{M}'_X(1)\\right]^2\\).\nSejam \\(X\\) e \\(Y\\) variáveis aleatórias com funções geradoras de probabilidade \\(\\mathcal{M}_X(t)\\) e \\(\\mathcal{M}_Y(t)\\), respectivamente. Mostre que \\(\\mathcal{M}_X(t)=\\mathcal{M}_Y(t)\\) se e somente se \\(\\mathbb{P}[X=k]=\\mathbb{P}[Y=k]\\).\nVerifique que \\(\\mathbb{E}[X^r]=\\sum_{j=0}^r{n\\brace k}\\mathbb{E}[(X)_j]\\), onde \\({n\\brace k}=\\frac{1}{k!}\\sum_{i=0}^k(-1)^{k-i}\\binom{k}{i}i^n=\\sum_{i=0}^k\\frac{(-1)^{k-i}i^n}{(k-i)!i!}\\) são chamados de números de Stirling tipo II.\nSeja \\(X\\) uma variável aleatória de uma população com distribuição hipergeométrica com parâmetros \\(N, K\\) e \\(n\\), i.e., \\(\\mathbb{P}[X=x]=\\frac{\\dbinom{K}{x}\\dbinom{N-K}{n-x}}{\\dbinom{N}{n}}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Mostre que \\(\\mathbb{E}[(X)_r]=\\frac{\\dbinom{K}{r}\\dbinom{n}{r}}{\\dbinom{N}{r}}r!\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\). Mostre que \\(\\mathbb{E}[X-\\lambda]^3=\\lambda\\) e \\(\\mathbb{E}[X-\\lambda]^4=\\lambda+3\\lambda^3\\).\nSeja \\(X\\) uma variável aleatória com função geradora de momentos \\(m(t)=\\exp\\{4(e^{t}-1)\\}\\). Qual o valor de \\(\\mathbb{P}[\\mu-2\\sigma<X<\\mu+2\\sigma]\\)?\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Irwin-Hall com parâmetro \\(n\\), i.e.\n\n\\[f(x;n)=\\frac{1}{(n-1)!}\\sum_{k=0}^n(-1)^k\\binom{n}{k}(x-k)_+^{n-1}\\mathbb{I}_{[0,n]}(x),\\] onde \\((x-k)_+^{n-1}=\\begin{cases}x-k, & x-k\\ge0;\\\\ 0, & x-k<0.\\end{cases}\\)\nUtilize a FGM para calcular a variância de \\(X\\).\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão e seja \\(Y=e^X\\). Calcule a FGM e o \\(r\\)-ésimo momento da distribuição de \\(Y\\).\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\) com média \\(\\mu\\) e variância \\(\\sigma^2\\). Suponha que \\(\\mathbb{E}[X^r]=\\mu'_r\\) e \\(\\mathbb{E}[(X-\\mu)^r]=\\mu_r\\) existem, para todo \\(r=1,2,\\ldots\\). Mostre que\n\n\\(\\mathbb{E}\\left[\\bar{X}_n^3\\right]=\\frac{1}{n^2}\\left(\\mu'_3+3(n-1)\\mu'_2\\,\\mu+(n-1)(n-2)\\mu^3\\right)\\);\n\\(\\mathbb{E}\\left[\\bar{X}_n^4\\right]=\\frac{1}{n^3}\\left(\\mu'_4+4(n-1)\\mu'_3\\,\\mu+6(n-1)(n-2)\\mu'_2\\,\\mu^2 +3(n-1)\\mu'^{\\,2}_2+(n-1)(n-2)(n-3)\\mu^4\\right)\\).\n\nSeja \\(U\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\([0,1]\\). Utilize a FGM para calcular a média e a variância de \\(Y=\\tan\\left(\\left(U-\\frac12\\right)\\pi\\right)\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão. Mostre que\n\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r=&\n  \\begin{cases}\n  2^{r/2}\\frac{\\Gamma\\left(\\frac{r+1}{2}\\right)}{\\Gamma\\left(\\frac12\\right)}, & r \\text{ par};\\\\\n  0, & r \\text{ ímpar}.\n  \\end{cases}\n\\end{align*}\\]\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição binomial com parâmetros \\(n\\) e \\(p\\). Mostre que\n\n\\(\\mu_{r+1}=p(1-p)\\left[\\frac{\\partial\\mu_r}{\\partial p}+nr\\mu_{r-1}\\right]\\);\n\\(\\mathbb{P}[X=x+1]=\\frac{n-x}{x+1}\\frac{p}{1-p}\\mathbb{P}[X=x]\\), para \\(x=0,1,2,\\ldots,n-1\\).\n\n\n\n\n\n\nBillingsley, Patrick. 1986. Probability and Measure. Second. John Wiley; Sons.\n\n\nCurtiss, J. H. 1942. \"A note on the theory moment generating functions\". The annals of the mathematical statistics 13 (4): 430–33.\n\n\nDudewicz, E., e S. Mishra. 1988. Modern mathematical statistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An introduction to probability theory and its applications. 2nd ed. Vol. II. Wiley.\n\n\nShohat, J. A., e J. D. Tamarkin. 1970. The problem of moments. Vol. I. American Mathematical Society."
  },
  {
    "objectID": "chap02.html#sec-fgm",
    "href": "chap02.html#sec-fgm",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.2 Funções geradoras de momentos",
    "text": "2.2 Funções geradoras de momentos\n\nDefinição 2.5 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h<t<h\\), para algum \\(h>0\\).\n\n\n\nTeorema 2.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|<h\\), para algum \\(h>0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide Apêndice A), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 2.5, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|<h\\), para algum \\(h>0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 2.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|<h\\), para algum \\(h>0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha>0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t>\\beta\\), a quantidade \\(-(\\beta-t)>0\\), então \\(e^{-(\\beta-t)x}>x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t<\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 2.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.7 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com densidade conjunta dada por \\[f(x; a)=a^2e^{-ax_2}, \\quad 0<x_1<x_2<\\infty, \\quad a>0.\\] Encontre \\(\\mathbb{E}X^r_1\\), \\(\\mathbb{E}X^r_2\\), para \\(r=1,2,3,\\ldots\\), e \\(\\mathrm{Corr}[X_1,X_2]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nTemos que,\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_1&=\\int_0^\\infty\\int_0^{x_2}x_1^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\int_0^{x_2}x_1^r\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2e^{-ax_2}\\frac{x_1^{r+1}}{r+1}\\Biggr|_{0}^{x_2}\\,dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\frac{x_2^{r+1}}{r+1}\\,dx_2\\\\\n  &=\\frac{\\Gamma(r+1)}{a^r}=\\frac{r!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_2&=\\int_0^\\infty\\int_0^{x_2}x_2^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2^re^{-ax_2}\\int_0^{x_2}\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2x_2^{r+1}e^{-ax_2}\\,dx_2=\\frac{\\Gamma(r+2)}{a^r}=\\frac{(r+1)!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nO momento cruzado é calculado como\n\n\\[\\begin{align*}\n  \\mathbb{E}X_1X_2&=\\int_0^\\infty\\int_0^{x_2}x_1x_2a^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2e^{-ax_2}\\int_0^{x_2}x_1\\,dx_1dx_2\\\\\n  &=\\frac{a^2}{2}\\int_0^\\infty x_2^3e^{-ax_2}dx_2\n  =\\frac{\\Gamma(4)}{2a^2}\\int_0^\\infty \\frac{a^4}{\\Gamma(4)}x_2^3e^{-ax_2}dx_2=\\frac{3}{a^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, a correlação é dada por\n\n\\[\\begin{align*}\n  \\mathrm{Corr}[X_1,X_2]&=\\frac{\\mathbb{E}X_1X_2-\\mathbb{E}X_1\\mathbb{E}X_2}{\\sqrt{\\mathrm{Var}[X_1]\\mathrm{Var}[X_2]}}\n  =\\frac{\\frac{3}{a^2}-\\frac1a\\frac2a}{\\sqrt{\\frac{1}{a^2}\\frac{2}{a^2}}}=\\frac{1}{\\sqrt{2}}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t<1\\) ou \\(t<-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 2.9 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha>0\\) e \\(\\beta>0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.5, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta>r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta>2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|<\\frac{h}{|b|}\\), \\(h>0\\).\n\n\n\nExemplo 2.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b>a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 2.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h>0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|<h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 2.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988) [pp. 261], Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 2.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|<h\\), , para algum \\(h>0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 2.5 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 2.11 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.12 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 2.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t<\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 2.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 2.7, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t<\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 2.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 2.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\n  =\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n>1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 2.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n2.2.1 Momentos fatoriais\n\nDefinição 2.6 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 2.13 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.14 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|<\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da Seção A.2 do Apêndice A.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 2.15 Considere o cenário do Exemplo 2.13. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 2.13, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n2.2.2 Cumulantes\n\nDefinição 2.7 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|<h\\), \\(h>0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 2.16 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 2.7, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.17 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.6, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 2.13, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r>1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 2.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|<h\\), para algum \\(h>0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|<h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc."
  },
  {
    "objectID": "matematica.html#sec-combinatorias",
    "href": "matematica.html#sec-combinatorias",
    "title": "Apêndice A — Resultados matemáticos",
    "section": "A.2 Fatoriais e Combinatórias",
    "text": "A.2 Fatoriais e Combinatórias\n\nFunção gamma: \\(\\Gamma(x)=\\int_0^\\infty t^{x-1}e^{-t}\\,dt\\), para \\(x&gt;0\\). Dessa forma, \\(\\Gamma(x+1)=x\\Gamma(x)\\);\n\nSe \\(n\\) é inteiro, então \\(\\Gamma(n+1)=n!\\), onde \\(n!=1\\cdot2\\cdot3\\cdots (n-1)\\cdot n\\);\nSe \\(n\\) é inteiro, \\(\\Gamma\\left(n+\\frac12\\right)=\\frac{1\\cdot3\\cdot5\\cdots(2n-1)}{2^n}\\sqrt\\pi\\). Em particular, \\(\\Gamma\\left(\\frac32\\right)=\\frac12\\Gamma\\left(\\frac12\\right)=\\frac12\\sqrt\\pi\\) ou \\(\\Gamma\\left(\\frac12\\right)=\\sqrt\\pi\\);\nFórmula de Stirling: \\(n!\\approx\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n\\)\n\nFunção beta: \\(B(a,b)=\\int_0^1x^{a-1}(1-x)^{b-1}\\,dx\\), para \\(a&gt;0\\) e \\(b&gt;0\\). De outra forma, \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\);\n\\(\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\\);\n\n\\(\\binom{n}{0}=\\binom{n}{n}=1\\);\nTeorema das combinações complementares: \\(\\binom{n}{k}=\\binom{n}{n-k}\\) ;\nRelação de Stifel: \\(\\binom{n+1}{k}=\\binom{n}{k}+\\binom{n}{k-1}\\), para \\(n=1,2,\\ldots\\) e \\(k=0,\\pm1,\\pm2,\\ldots\\);\n\\(\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots(-n-k+1)}{k!}=(-1)^k\\binom{n+k-1}{k}\\);\n\\(\\binom{n}{k}\\binom{n-k}{m-k}=\\binom{n}{m}\\binom{m}{k}\\);\n\nSérie binomial negativa: Sendo \\(n\\) inteiro negativo, para todo \\(|x|&lt;a\\), temos que \\[(x+a)^{-n}=\\sum_{k=0}^\\infty\\binom{-n}{k}x^ka^{-n-k}=\\sum_{k=0}^\\infty(-1)^k\\binom{n+k-1}{k}x^ka^{-n-k};\\]",
    "crumbs": [
      "Apêndices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Resultados matemáticos</span>"
    ]
  },
  {
    "objectID": "matematica.html#constantes",
    "href": "matematica.html#constantes",
    "title": "Apêndice A — Resultados matemáticos",
    "section": "A.3 Constantes",
    "text": "A.3 Constantes\n\n\\(\\pi=4\\sum_{j=0}^\\infty\\frac{(-1)^j}{2j+1}\\);\n\\(e=\\sum_{j=0}^\\infty\\frac{1}{j!}\\);\n\\(\\log2=\\sum_{j=0}^\\infty\\frac{(-1)^{j-1}}{j}\\)\n\n\n\n\n\nMood, A., F. Graybill, e D. Boes. 1974. Introduction to the theory of statistics. 3rd ed. McGraw-Hill Higher Education.",
    "crumbs": [
      "Apêndices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Resultados matemáticos</span>"
    ]
  },
  {
    "objectID": "chap02.html#função-característica",
    "href": "chap02.html#função-característica",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função característica",
    "text": "2.3 Função característica"
  },
  {
    "objectID": "chap02.html#função-geradora-de-moemntos-conjunta",
    "href": "chap02.html#função-geradora-de-moemntos-conjunta",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função geradora de moemntos conjunta",
    "text": "2.3 Função geradora de moemntos conjunta"
  },
  {
    "objectID": "chap02.html#função-geradora-de-momentos-conjunta",
    "href": "chap02.html#função-geradora-de-momentos-conjunta",
    "title": "2  Estatísticas e momentos amostrais",
    "section": "2.3 Função geradora de momentos conjunta",
    "text": "2.3 Função geradora de momentos conjunta\n\nDefinição 2.8 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i>0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 2.18 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1>0, x_2>0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.8, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.19 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*<\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12.\n\\end{align*}\\]\n\nPelo @-fgm_unicidade, temos que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\).\n\n\n\n\nExemplo 2.20 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Calcule a FGMC do vetor \\((Y,Z)\\), onde \\(Y=X_1+X_2\\) e \\(Z=X_1^2+X_2^2\\). As variáveis \\(Y\\) e \\(Z\\) são correlacionadas?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[e^{t_1Y+t_2Z}\\right]=\\mathbb{E}\\left[e^{t_1(X_1+X_2)+t_2\\left(X_1^2+X_2^2\\right)}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_1X_2+t_2X_1^2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right].\n\\end{align*}\\]\n\nPor outra parte, temos que\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]&=\\int_{-\\infty}^\\infty e^{t_1x_1+t_2x_1^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12x_1^2}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{t_1x_1+\\left(t_2-\\frac12\\right)x_1^2\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left(x_1^2-\\frac{2t_1}{1-2t_2}x_1\\right)\\right\\}\\,dx_1, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right)\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left[\\left(x_1-\\frac{t_1}{1-2t_2}\\right)^2-\\frac{t_1^2}{(1-2t_2)^2}\\right]\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}. \\quad \\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\left(\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}\\right)^2\\\\\n      &=\\frac{1}{1-2t_2}\\exp\\left\\{\\frac{t_1^2}{1-2t_2}\\right\\}, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right). \\quad\\checkmark\n\\end{align*}\\]\n\nObserve que, \\(M(t_1,0)=M(t_1)=e^{t_1^2}\\), para \\(t_1\\in\\mathbb{R}\\), i.e. \\(Y\\) é uma variável aleatória de uma população com distribuição normal com média \\(0\\) e variância \\(2\\). Por otro lado, \\(M(,t_2)=M(t_2)=\\frac{1}{1-2t_2}\\), para \\(-\\infty<t_2<\\frac12\\), i.e., \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade.\nPara verificar se \\(Y\\) e \\(Z\\) são correlacionadas, calcula-se a correlação entre as duas variáveis aleatórias. Assim,\n\n\\[\\begin{align*}\n      \\mathbb{E}YZ=\\frac{\\partial^2}{\\partial t_1\\partial t_2}M(t_1,t_2)\\Biggr|_{(t_1,t_2)=(0,0)}\n      &=\\frac{\\partial}{\\partial t_1}\\left[\\frac{\\partial}{\\partial t_2}M(t_1,t_2)\\right]\\Biggr|_{(t_1,t_2)=(0,0)}\\\\       &=\\frac{4t_1\\exp\\left\\{\\frac{t^2_1}{1-2t_2}\\right\\}(-4t_2+t_1^2+2)}{(1-2t_2)^4}\\Biggr|_{(t_1,t_2)=(0,0)}\\\\\n      &=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, \\(\\mathrm{Cov}[Y,Z]=\\mathbb{E}YZ-\\mathbb{E}Y\\mathbb{E}Z=0\\). Daí, as variáveis \\(Y\\) e \\(Z\\) são não-correlacionadas."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introdução",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming”. Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Introdução"
    ]
  },
  {
    "objectID": "prob/p1chap01.html#sec-prop_estimadores",
    "href": "prob/p1chap01.html#sec-prop_estimadores",
    "title": "1  Conjuntos",
    "section": "1.1 Propriedades dos estimadores",
    "text": "1.1 Propriedades dos estimadores\n\nDefinição 1.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "prob/p1chap02.html#sec-prop_estimadores",
    "href": "prob/p1chap02.html#sec-prop_estimadores",
    "title": "2  Estimação pontual",
    "section": "",
    "text": "Definição 2.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente.",
    "crumbs": [
      "I. Noções de probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "p2chap01.html#sec-prop_estimadores",
    "href": "p2chap01.html#sec-prop_estimadores",
    "title": "4  Estimação pontual",
    "section": "4.1 Propriedades dos estimadores",
    "text": "4.1 Propriedades dos estimadores\n\nDefinição 4.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "p2chap02.html#sec-prop_estimadores",
    "href": "p2chap02.html#sec-prop_estimadores",
    "title": "5  Estimação pontual",
    "section": "5.1 Propriedades dos estimadores",
    "text": "5.1 Propriedades dos estimadores\n\nDefinição 5.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "p2chap03.html#sec-prop_estimadores",
    "href": "p2chap03.html#sec-prop_estimadores",
    "title": "6  Estimação pontual",
    "section": "6.1 Propriedades dos estimadores",
    "text": "6.1 Propriedades dos estimadores\n\nDefinição 6.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "p3chap01.html",
    "href": "p3chap01.html",
    "title": "7  Conceitos preliminares",
    "section": "",
    "text": "Definição 7.1 (População alvo) Define-se população alvo, ou simplesmente população, a um conjunto de unidades experimentais.\n\n\nDefinição 7.2 (Amostra aleatória) A sequência \\(X_1,X_2,\\ldots,X_n\\) de variáveis aleatórias independentes e identicamente distribuídas define uma amostra aleatória de tamanho \\(n\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSe as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) configuram uma amostra aleatória de uma população com distribuição contínua, então a função de densidade conjunta é dada por \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\cdots f(x_n)=\\prod_{i=1}^nf(x_i), \\tag{7.1}\\] onde \\(f(\\cdot)\\) representa a função de densidade marginal para cada variável aleatória \\(X_i\\), com \\(i=1,2,\\ldots,n\\). Nesse caso a Equação 7.1 representa a distribuição da amostra aleatória.\n\n\n\nExemplo 7.1 Seja \\(X\\) uma variável aleatória de uma população com densidade \\(f(\\cdot)\\). Seja \\(X_1,X_2\\) uma amostra aleatória de tamanho \\(2\\). Por definição, a distribuição conjunta de \\(X_1,X_2\\) é dada por \\[f(x_1,x_2)=f(x_1)f(x_2).\\]\n\n\nExemplo 7.2 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\), i.e.,\n\\[\\mathbb{P}[X=x]=p^x(1-p)^{1-x}\\mathbb{I}_{\\{0,1\\}}(x).\\] Daí,\n\n\\[\\begin{align*}\n    \\mathbb{P}[X_1=x_1,X_2=x_2]&=\\mathbb{P}[X_1=x_1]\\mathbb{P}[X_2=x_2]\\\\\n    &=p^{x_1}(1-p)^{1-x_1}\\mathbb{I}_{\\{0,1\\}}(x_1)p^{x_2}(1-p)^{1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_2)\\\\\n    &=p^{x_1+x_2}(1-p)^{2-x_1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_1)\\mathbb{I}_{\\{0,1\\}}(x_2).\n  \\end{align*}\\]\n\nNesse caso, a distribuição conjunta da amostra está definida no pares \\((x_1,x_2)\\in\\{(0,0), (0,1), (1,0), (1,1)\\}\\).\nCabe ressaltar que, a distribuição conjunta da amostra não é igual à distribuição obtida considerando os elementos de uma distribuição tipo binomial. No caso da distribuição binomial, seja \\(Y=X_1+X_2\\) (com prob. \\(1\\)) então \\[\\mathbb{P}[Y=y]=\\binom{2}{y}p^y(1-p)^{2-y}\\mathbb{I}_{\\{0,1,2\\}}(y).\\] Dessa forma, a distribuição binomial é definida para uma única variável que representa o número de sucessos, e difere da distribuição conjunta da amostra porque a binomial não considera a ordem da coleta, por exemplo, para a distribuição conjunta da amostra, \\(\\mathbb{P}[X_1=0,X_2=1]\\) representa a probabilidade de coletar primeiro \\(0\\) e depois \\(1\\)."
  },
  {
    "objectID": "p3chap02.html#sec-momentos_amostrais",
    "href": "p3chap02.html#sec-momentos_amostrais",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "8.1 Momentos amostrais",
    "text": "8.1 Momentos amostrais\n\nDefinição 8.2 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 8.3 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 8.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a Definição 8.1, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\n\n\n\n\n\nLembre que…\n\n\n\n\n\nSe \\(X\\) é uma variável aleatória, o \\(r\\)-ésimo momento de \\(X\\) define-se como \\(\\mathbb{E}[X^r]=\\mu_r'\\), se existe. Também pode-se definir o \\(r\\)-ésimo momento central como \\(\\mathbb{E}[(X-\\mu_X)^r]=\\mu_r\\), onde \\(\\mu_X=\\mathbb{E}[X]=\\mu_1'\\).\nPara \\(r=2\\), temos que \\(\\mathbb{E}[X-\\mu_X]^2=\\mathrm{Var}[X]\\).\n\n\n\n\nExemplo 8.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r>-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 8.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 8.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 8.1, sabemos da Definição 8.2 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 8.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 8.4 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n>1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 8.5 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n>1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n>1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 8.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 8.4. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n>1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^k\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n>1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i<k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]"
  },
  {
    "objectID": "p3chap02.html#sec-fgm",
    "href": "p3chap02.html#sec-fgm",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "8.2 Funções geradoras de momentos",
    "text": "8.2 Funções geradoras de momentos\n\nDefinição 8.5 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h<t<h\\), para algum \\(h>0\\).\n\n\n\nTeorema 8.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|<h\\), para algum \\(h>0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide Apêndice A), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 8.5, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|<h\\), para algum \\(h>0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 8.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|<h\\), para algum \\(h>0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha>0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t>\\beta\\), a quantidade \\(-(\\beta-t)>0\\), então \\(e^{-(\\beta-t)x}>x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t<\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 8.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.7 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com densidade conjunta dada por \\[f(x; a)=a^2e^{-ax_2}, \\quad 0<x_1<x_2<\\infty, \\quad a>0.\\] Encontre \\(\\mathbb{E}X^r_1\\), \\(\\mathbb{E}X^r_2\\), para \\(r=1,2,3,\\ldots\\), e \\(\\mathrm{Corr}[X_1,X_2]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nTemos que,\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_1&=\\int_0^\\infty\\int_0^{x_2}x_1^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\int_0^{x_2}x_1^r\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2e^{-ax_2}\\frac{x_1^{r+1}}{r+1}\\Biggr|_{0}^{x_2}\\,dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\frac{x_2^{r+1}}{r+1}\\,dx_2\\\\\n  &=\\frac{\\Gamma(r+1)}{a^r}=\\frac{r!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_2&=\\int_0^\\infty\\int_0^{x_2}x_2^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2^re^{-ax_2}\\int_0^{x_2}\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2x_2^{r+1}e^{-ax_2}\\,dx_2=\\frac{\\Gamma(r+2)}{a^r}=\\frac{(r+1)!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nO momento cruzado é calculado como\n\n\\[\\begin{align*}\n  \\mathbb{E}X_1X_2&=\\int_0^\\infty\\int_0^{x_2}x_1x_2a^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2e^{-ax_2}\\int_0^{x_2}x_1\\,dx_1dx_2\\\\\n  &=\\frac{a^2}{2}\\int_0^\\infty x_2^3e^{-ax_2}dx_2\n  =\\frac{\\Gamma(4)}{2a^2}\\int_0^\\infty \\frac{a^4}{\\Gamma(4)}x_2^3e^{-ax_2}dx_2=\\frac{3}{a^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, a correlação é dada por\n\n\\[\\begin{align*}\n  \\mathrm{Corr}[X_1,X_2]&=\\frac{\\mathbb{E}X_1X_2-\\mathbb{E}X_1\\mathbb{E}X_2}{\\sqrt{\\mathrm{Var}[X_1]\\mathrm{Var}[X_2]}}\n  =\\frac{\\frac{3}{a^2}-\\frac1a\\frac2a}{\\sqrt{\\frac{1}{a^2}\\frac{2}{a^2}}}=\\frac{1}{\\sqrt{2}}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t<1\\) ou \\(t<-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 8.9 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha>0\\) e \\(\\beta>0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.5, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta>r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta>2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|<\\frac{h}{|b|}\\), \\(h>0\\).\n\n\n\nExemplo 8.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b>a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 8.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h>0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|<h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 8.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988) [pp. 261], Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 8.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|<h\\), , para algum \\(h>0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 8.5 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 8.11 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 8.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.12 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 8.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 8.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t<\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 8.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 8.7, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t<\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 8.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 8.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\n  =\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n>1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 8.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n8.2.1 Momentos fatoriais\n\nDefinição 8.6 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 8.13 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.14 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|<\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da Seção A.2 do Apêndice A.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 8.15 Considere o cenário do Exemplo 8.13. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 8.13, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n8.2.2 Cumulantes\n\nDefinição 8.7 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|<h\\), \\(h>0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 8.16 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 8.7, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.17 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.6, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 8.13, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r>1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 8.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|<h\\), para algum \\(h>0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|<h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc."
  },
  {
    "objectID": "p3chap02.html#função-geradora-de-momentos-conjunta",
    "href": "p3chap02.html#função-geradora-de-momentos-conjunta",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "8.3 Função geradora de momentos conjunta",
    "text": "8.3 Função geradora de momentos conjunta\n\nDefinição 8.8 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i>0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 8.18 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1>0, x_2>0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.8, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.19 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*<\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2<\\frac14 \\text{ ou } |t|<\\frac12.\n\\end{align*}\\]\n\nPelo @-fgm_unicidade, temos que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\).\n\n\n\n\nExemplo 8.20 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Calcule a FGMC do vetor \\((Y,Z)\\), onde \\(Y=X_1+X_2\\) e \\(Z=X_1^2+X_2^2\\). As variáveis \\(Y\\) e \\(Z\\) são correlacionadas?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[e^{t_1Y+t_2Z}\\right]=\\mathbb{E}\\left[e^{t_1(X_1+X_2)+t_2\\left(X_1^2+X_2^2\\right)}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_1X_2+t_2X_1^2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right].\n\\end{align*}\\]\n\nPor outra parte, temos que\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]&=\\int_{-\\infty}^\\infty e^{t_1x_1+t_2x_1^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12x_1^2}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{t_1x_1+\\left(t_2-\\frac12\\right)x_1^2\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left(x_1^2-\\frac{2t_1}{1-2t_2}x_1\\right)\\right\\}\\,dx_1, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right)\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left[\\left(x_1-\\frac{t_1}{1-2t_2}\\right)^2-\\frac{t_1^2}{(1-2t_2)^2}\\right]\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}. \\quad \\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\left(\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}\\right)^2\\\\\n      &=\\frac{1}{1-2t_2}\\exp\\left\\{\\frac{t_1^2}{1-2t_2}\\right\\}, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right). \\quad\\checkmark\n\\end{align*}\\]\n\nObserve que, \\(M(t_1,0)=M(t_1)=e^{t_1^2}\\), para \\(t_1\\in\\mathbb{R}\\), i.e. \\(Y\\) é uma variável aleatória de uma população com distribuição normal com média \\(0\\) e variância \\(2\\). Por otro lado, \\(M(,t_2)=M(t_2)=\\frac{1}{1-2t_2}\\), para \\(-\\infty<t_2<\\frac12\\), i.e., \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade.\nPara verificar se \\(Y\\) e \\(Z\\) são correlacionadas, calcula-se a correlação entre as duas variáveis aleatórias. Assim,\n\n\\[\\begin{align*}\n      \\mathbb{E}YZ=\\frac{\\partial^2}{\\partial t_1\\partial t_2}M(t_1,t_2)\\Biggr|_{(t_1,t_2)=(0,0)}\n      &=\\frac{\\partial}{\\partial t_1}\\left[\\frac{\\partial}{\\partial t_2}M(t_1,t_2)\\right]\\Biggr|_{(t_1,t_2)=(0,0)}\\\\       &=\\frac{4t_1\\exp\\left\\{\\frac{t^2_1}{1-2t_2}\\right\\}(-4t_2+t_1^2+2)}{(1-2t_2)^4}\\Biggr|_{(t_1,t_2)=(0,0)}\\\\\n      &=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, \\(\\mathrm{Cov}[Y,Z]=\\mathbb{E}YZ-\\mathbb{E}Y\\mathbb{E}Z=0\\). Daí, as variáveis \\(Y\\) e \\(Z\\) são não-correlacionadas."
  },
  {
    "objectID": "p3chap02.html#exercícios-sugeridos",
    "href": "p3chap02.html#exercícios-sugeridos",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nSeja \\(X\\) uma variável aletória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\). Verifique que \\(\\mathbb{E}[X]=\\mathcal{M}'_X(1)=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}\\) e \\(\\mathrm{Var}[X]=\\mathcal{M}''_X(1)+\\mathcal{M}'_X(1)-\\left[\\mathcal{M}'_X(1)\\right]^2\\).\nSejam \\(X\\) e \\(Y\\) variáveis aleatórias com funções geradoras de probabilidade \\(\\mathcal{M}_X(t)\\) e \\(\\mathcal{M}_Y(t)\\), respectivamente. Mostre que \\(\\mathcal{M}_X(t)=\\mathcal{M}_Y(t)\\) se e somente se \\(\\mathbb{P}[X=k]=\\mathbb{P}[Y=k]\\).\nVerifique que \\(\\mathbb{E}[X^r]=\\sum_{j=0}^r{n\\brace k}\\mathbb{E}[(X)_j]\\), onde \\({n\\brace k}=\\frac{1}{k!}\\sum_{i=0}^k(-1)^{k-i}\\binom{k}{i}i^n=\\sum_{i=0}^k\\frac{(-1)^{k-i}i^n}{(k-i)!i!}\\) são chamados de números de Stirling tipo II.\nSeja \\(X\\) uma variável aleatória de uma população com distribuição hipergeométrica com parâmetros \\(N, K\\) e \\(n\\), i.e., \\(\\mathbb{P}[X=x]=\\frac{\\dbinom{K}{x}\\dbinom{N-K}{n-x}}{\\dbinom{N}{n}}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Mostre que \\(\\mathbb{E}[(X)_r]=\\frac{\\dbinom{K}{r}\\dbinom{n}{r}}{\\dbinom{N}{r}}r!\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\). Mostre que \\(\\mathbb{E}[X-\\lambda]^3=\\lambda\\) e \\(\\mathbb{E}[X-\\lambda]^4=\\lambda+3\\lambda^3\\).\nSeja \\(X\\) uma variável aleatória com função geradora de momentos \\(m(t)=\\exp\\{4(e^{t}-1)\\}\\). Qual o valor de \\(\\mathbb{P}[\\mu-2\\sigma<X<\\mu+2\\sigma]\\)?\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Irwin-Hall com parâmetro \\(n\\), i.e.\n\n\\[f(x;n)=\\frac{1}{(n-1)!}\\sum_{k=0}^n(-1)^k\\binom{n}{k}(x-k)_+^{n-1}\\mathbb{I}_{[0,n]}(x),\\] onde \\((x-k)_+^{n-1}=\\begin{cases}x-k, & x-k\\ge0;\\\\ 0, & x-k<0.\\end{cases}\\)\nUtilize a FGM para calcular a variância de \\(X\\).\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão e seja \\(Y=e^X\\). Calcule a FGM e o \\(r\\)-ésimo momento da distribuição de \\(Y\\).\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\) com média \\(\\mu\\) e variância \\(\\sigma^2\\). Suponha que \\(\\mathbb{E}[X^r]=\\mu'_r\\) e \\(\\mathbb{E}[(X-\\mu)^r]=\\mu_r\\) existem, para todo \\(r=1,2,\\ldots\\). Mostre que\n\n\\(\\mathbb{E}\\left[\\bar{X}_n^3\\right]=\\frac{1}{n^2}\\left(\\mu'_3+3(n-1)\\mu'_2\\,\\mu+(n-1)(n-2)\\mu^3\\right)\\);\n\\(\\mathbb{E}\\left[\\bar{X}_n^4\\right]=\\frac{1}{n^3}\\left(\\mu'_4+4(n-1)\\mu'_3\\,\\mu+6(n-1)(n-2)\\mu'_2\\,\\mu^2 +3(n-1)\\mu'^{\\,2}_2+(n-1)(n-2)(n-3)\\mu^4\\right)\\).\n\nSeja \\(U\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\([0,1]\\). Utilize a FGM para calcular a média e a variância de \\(Y=\\tan\\left(\\left(U-\\frac12\\right)\\pi\\right)\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão. Mostre que\n\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r=&\n  \\begin{cases}\n  2^{r/2}\\frac{\\Gamma\\left(\\frac{r+1}{2}\\right)}{\\Gamma\\left(\\frac12\\right)}, & r \\text{ par};\\\\\n  0, & r \\text{ ímpar}.\n  \\end{cases}\n\\end{align*}\\]\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição binomial com parâmetros \\(n\\) e \\(p\\). Mostre que\n\n\\(\\mu_{r+1}=p(1-p)\\left[\\frac{\\partial\\mu_r}{\\partial p}+nr\\mu_{r-1}\\right]\\);\n\\(\\mathbb{P}[X=x+1]=\\frac{n-x}{x+1}\\frac{p}{1-p}\\mathbb{P}[X=x]\\), para \\(x=0,1,2,\\ldots,n-1\\).\n\n\n\n\n\n\nBillingsley, Patrick. 1986. Probability and Measure. Second. John Wiley; Sons.\n\n\nCurtiss, J. H. 1942. \"A note on the theory moment generating functions\". The annals of the mathematical statistics 13 (4): 430–33.\n\n\nDudewicz, E., e S. Mishra. 1988. Modern mathematical statistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An introduction to probability theory and its applications. 2nd ed. Vol. II. Wiley.\n\n\nShohat, J. A., e J. D. Tamarkin. 1970. The problem of moments. Vol. I. American Mathematical Society."
  },
  {
    "objectID": "p4chap01.html#sec-prop_estimadores",
    "href": "p4chap01.html#sec-prop_estimadores",
    "title": "9  Estimação pontual",
    "section": "9.1 Propriedades dos estimadores",
    "text": "9.1 Propriedades dos estimadores\n\nDefinição 9.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "amostral/p3chap02.html#sec-momentos_amostrais",
    "href": "amostral/p3chap02.html#sec-momentos_amostrais",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "",
    "text": "Definição 8.2 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 8.3 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 8.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a Definição 8.1, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\n\n\n\n\n\nLembre que…\n\n\n\n\n\nSe \\(X\\) é uma variável aleatória, o \\(r\\)-ésimo momento de \\(X\\) define-se como \\(\\mathbb{E}[X^r]=\\mu_r'\\), se existe. Também pode-se definir o \\(r\\)-ésimo momento central como \\(\\mathbb{E}[(X-\\mu_X)^r]=\\mu_r\\), onde \\(\\mu_X=\\mathbb{E}[X]=\\mu_1'\\).\nPara \\(r=2\\), temos que \\(\\mathbb{E}[X-\\mu_X]^2=\\mathrm{Var}[X]\\).\n\n\n\n\nExemplo 8.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 8.5 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 8.4, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 8.5.\n\n\n\n\n\nExemplo 8.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 8.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 8.1, sabemos da Definição 8.2 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 8.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 8.4 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 8.7 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 8.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 8.4. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da ?sec-somas do ?sec-resutados_mat, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estatísticas e momentos amostrais</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap02.html#sec-fgm",
    "href": "amostral/p3chap02.html#sec-fgm",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "8.2 Funções geradoras de momentos",
    "text": "8.2 Funções geradoras de momentos\n\nDefinição 8.5 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h&lt;t&lt;h\\), para algum \\(h&gt;0\\).\n\n\n\nTeorema 8.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide ?sec-resutados_mat), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 8.5, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 8.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha&gt;0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t&gt;\\beta\\), a quantidade \\(-(\\beta-t)&gt;0\\), então \\(e^{-(\\beta-t)x}&gt;x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t&lt;\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 8.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.9 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com densidade conjunta dada por \\[f(x; a)=a^2e^{-ax_2}, \\quad 0&lt;x_1&lt;x_2&lt;\\infty, \\quad a&gt;0.\\] Encontre \\(\\mathbb{E}X^r_1\\), \\(\\mathbb{E}X^r_2\\), para \\(r=1,2,3,\\ldots\\), e \\(\\mathrm{Corr}[X_1,X_2]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nTemos que,\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_1&=\\int_0^\\infty\\int_0^{x_2}x_1^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\int_0^{x_2}x_1^r\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2e^{-ax_2}\\frac{x_1^{r+1}}{r+1}\\Biggr|_{0}^{x_2}\\,dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\frac{x_2^{r+1}}{r+1}\\,dx_2\\\\\n  &=\\frac{\\Gamma(r+1)}{a^r}=\\frac{r!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_2&=\\int_0^\\infty\\int_0^{x_2}x_2^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2^re^{-ax_2}\\int_0^{x_2}\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2x_2^{r+1}e^{-ax_2}\\,dx_2=\\frac{\\Gamma(r+2)}{a^r}=\\frac{(r+1)!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nO momento cruzado é calculado como\n\n\\[\\begin{align*}\n  \\mathbb{E}X_1X_2&=\\int_0^\\infty\\int_0^{x_2}x_1x_2a^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2e^{-ax_2}\\int_0^{x_2}x_1\\,dx_1dx_2\\\\\n  &=\\frac{a^2}{2}\\int_0^\\infty x_2^3e^{-ax_2}dx_2\n  =\\frac{\\Gamma(4)}{2a^2}\\int_0^\\infty \\frac{a^4}{\\Gamma(4)}x_2^3e^{-ax_2}dx_2=\\frac{3}{a^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, a correlação é dada por\n\n\\[\\begin{align*}\n  \\mathrm{Corr}[X_1,X_2]&=\\frac{\\mathbb{E}X_1X_2-\\mathbb{E}X_1\\mathbb{E}X_2}{\\sqrt{\\mathrm{Var}[X_1]\\mathrm{Var}[X_2]}}\n  =\\frac{\\frac{3}{a^2}-\\frac1a\\frac2a}{\\sqrt{\\frac{1}{a^2}\\frac{2}{a^2}}}=\\frac{1}{\\sqrt{2}}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t&lt;1\\) ou \\(t&lt;-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 8.11 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha&gt;0\\) e \\(\\beta&gt;0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.5, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta&gt;r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta&gt;2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|&lt;\\frac{h}{|b|}\\), \\(h&gt;0\\).\n\n\n\nExemplo 8.12 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b&gt;a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 8.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h&gt;0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|&lt;h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 8.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988) [pp. 261], Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 8.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|&lt;h\\), , para algum \\(h&gt;0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 8.5 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 8.13 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 8.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.14 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 8.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 8.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t&lt;\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 8.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 8.8, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t&lt;\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 8.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 8.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nExemplo 8.15 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n&gt;1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 8.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n\nTeorema 8.7 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\), então\n\n\n\\(\\bar{X}_n\\) segue uma distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\);\n\n\n\\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\);\n\n\n\\(\\bar{X}_n\\) e \\(S^2_n\\) são variáveis aleatórias independentes.\n\n\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\n\n\nVide Teorema 8.1 e Exemplo 8.13;\n\n\nVide Exemplo 8.15;\n\n\nNote que\n\n\\[\\begin{align*}\nS^2_n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\left[(X_1-\\bar{X}_n)+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=1}^n(X_i-\\bar{X}_n)-(X_1-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=2}^n(X_i-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\n\\end{align*}\\]\n\nDaí, \\(S^2_n\\) pode ser escrito em função de \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n, \\ldots, X_n-\\bar{X}_n\\). Dessa forma, se as variáveis aleatórias \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) são independentes de \\(\\bar{X}_n\\), obtem-se o resultado requerido.\nPara isso, calcula-se a densidade conjunta das variáveis aleatórias \\(\\bar{X}_n\\), \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) para verificar se a mesma pode ser escrita como produto das densidades de \\(\\bar{X}_n\\) e \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\). Dessa maneira, para usar o método do jacobiano, considere \\(y_1=\\bar{x}_n\\) e \\(y_i=x_i-\\bar{x}_n\\) para \\(i=2,3,\\ldots,n\\). Então,\n\n\\[\\begin{align*}\n    x_1&=y_1-\\sum_{i=2}^ny_i; & x_i&=y_i+y_1, \\quad i=2,3,4,\\ldots.\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n    \\frac{\\partial x_1}{\\partial y_1}&=1; & \\frac{\\partial x_2}{\\partial y_1}&=1; & \\frac{\\partial x_3}{\\partial y_1}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_1}&=1;\\\\\n    \\frac{\\partial x_1}{\\partial y_2}&=-1; & \\frac{\\partial x_2}{\\partial y_2}&=1; & \\frac{\\partial x_3}{\\partial y_2}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_2}&=0;\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=0;\\\\\n    &\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots & & &&\\,\\,\\,\\vdots\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=1.\n\\end{align*}\\]\n\nAssim sendo,\n\n\\[\\begin{align*}\n    |J|&=\\begin{vmatrix}\n     1 & 1 & 1 & \\cdots & 1\\\\\n    -1 & 1 & 0 & \\cdots & 0\\\\\n    -1 & 0 & 1 & \\cdots & 0\\\\\n    \\vdots &  &  & \\ddots & \\vdots \\\\\n    -1 & 0 & 0 & \\cdots & 1\\\\\n    \\end{vmatrix}=n.\n\\end{align*}\\]\n\nPor otro lado, sabemos que a função de densidade conjunta de \\(X_1,X_2,\\ldots,X_n\\) é dada por\n\\[f(x_1,x_2,\\ldots,x_n)=(2\\pi)^{-n/2}e^{-\\frac12\\sum_{i=1}^nx_i}, \\quad x_i\\in\\mathbb{R}.\\] Portanto,\n\n\\[\\begin{align*}\n    f(y_1,y_2,\\ldots,y_n)&=\\frac{n}{(2\\pi)^{-n/2}}e^{-\\frac12(y_1-\\sum_{i=2}^ny_i)^2}e^{-\\frac12\\sum_{i=2}^n(y_i+y_1)^2}\\\\\n    &=\\frac{1}{(2\\pi n^{-1})^{\\frac12}}e^{-\\frac12ny_1^2}\\frac{1}{(2\\pi )^{\\frac{n-1}{2}}n^{-1/2}}e^{\\left[-\\frac12\\sum_{i=2}^ny_i^2+(\\sum_{i=2}^ny_i)^2\\right]}.\n\\end{align*}\\]\n\nEntão, \\(Y_1\\) é independente de \\(Y_2,Y_3,\\ldots,Y_n. \\quad \\blacksquare\\)\n\n\n\n\n\n\n8.2.1 Momentos fatoriais\n\nDefinição 8.6 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 8.16 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.17 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|&lt;\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da ?sec-combinatorias do ?sec-resutados_mat.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 8.18 Considere o cenário do Exemplo 8.16. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 8.16, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n8.2.2 Cumulantes\n\nDefinição 8.7 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|&lt;h\\), \\(h&gt;0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 8.19 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 8.7, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.20 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.6, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 8.16, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r&gt;1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 8.5, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|&lt;h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc.",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estatísticas e momentos amostrais</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap02.html#função-geradora-de-momentos-conjunta",
    "href": "amostral/p3chap02.html#função-geradora-de-momentos-conjunta",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "8.3 Função geradora de momentos conjunta",
    "text": "8.3 Função geradora de momentos conjunta\n\nDefinição 8.8 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i&gt;0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 8.21 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1&gt;0, x_2&gt;0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 8.8, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 8.22 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*&lt;\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12.\n\\end{align*}\\]\n\nPelo @-fgm_unicidade, temos que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\).\n\n\n\n\nExemplo 8.23 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Calcule a FGMC do vetor \\((Y,Z)\\), onde \\(Y=X_1+X_2\\) e \\(Z=X_1^2+X_2^2\\). As variáveis \\(Y\\) e \\(Z\\) são correlacionadas?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[e^{t_1Y+t_2Z}\\right]=\\mathbb{E}\\left[e^{t_1(X_1+X_2)+t_2\\left(X_1^2+X_2^2\\right)}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_1X_2+t_2X_1^2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right].\n\\end{align*}\\]\n\nPor outra parte, temos que\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]&=\\int_{-\\infty}^\\infty e^{t_1x_1+t_2x_1^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12x_1^2}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{t_1x_1+\\left(t_2-\\frac12\\right)x_1^2\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left(x_1^2-\\frac{2t_1}{1-2t_2}x_1\\right)\\right\\}\\,dx_1, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right)\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left[\\left(x_1-\\frac{t_1}{1-2t_2}\\right)^2-\\frac{t_1^2}{(1-2t_2)^2}\\right]\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}. \\quad \\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\left(\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}\\right)^2\\\\\n      &=\\frac{1}{1-2t_2}\\exp\\left\\{\\frac{t_1^2}{1-2t_2}\\right\\}, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right). \\quad\\checkmark\n\\end{align*}\\]\n\nObserve que, \\(M(t_1,0)=M(t_1)=e^{t_1^2}\\), para \\(t_1\\in\\mathbb{R}\\), i.e. \\(Y\\) é uma variável aleatória de uma população com distribuição normal com média \\(0\\) e variância \\(2\\). Por otro lado, \\(M(,t_2)=M(t_2)=\\frac{1}{1-2t_2}\\), para \\(-\\infty&lt;t_2&lt;\\frac12\\), i.e., \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade.\nPara verificar se \\(Y\\) e \\(Z\\) são correlacionadas, calcula-se a correlação entre as duas variáveis aleatórias. Assim,\n\n\\[\\begin{align*}\n      \\mathbb{E}YZ=\\frac{\\partial^2}{\\partial t_1\\partial t_2}M(t_1,t_2)\\Biggr|_{(t_1,t_2)=(0,0)}\n      &=\\frac{\\partial}{\\partial t_1}\\left[\\frac{\\partial}{\\partial t_2}M(t_1,t_2)\\right]\\Biggr|_{(t_1,t_2)=(0,0)}\\\\       &=\\frac{4t_1\\exp\\left\\{\\frac{t^2_1}{1-2t_2}\\right\\}(-4t_2+t_1^2+2)}{(1-2t_2)^4}\\Biggr|_{(t_1,t_2)=(0,0)}\\\\\n      &=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, \\(\\mathrm{Cov}[Y,Z]=\\mathbb{E}YZ-\\mathbb{E}Y\\mathbb{E}Z=0\\). Daí, as variáveis \\(Y\\) e \\(Z\\) são não-correlacionadas.",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estatísticas e momentos amostrais</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap02.html#exercícios-sugeridos",
    "href": "amostral/p3chap02.html#exercícios-sugeridos",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nSeja \\(X\\) uma variável aletória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\). Verifique que \\(\\mathbb{E}[X]=\\mathcal{M}'_X(1)=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}\\) e \\(\\mathrm{Var}[X]=\\mathcal{M}''_X(1)+\\mathcal{M}'_X(1)-\\left[\\mathcal{M}'_X(1)\\right]^2\\).\nSejam \\(X\\) e \\(Y\\) variáveis aleatórias com funções geradoras de probabilidade \\(\\mathcal{M}_X(t)\\) e \\(\\mathcal{M}_Y(t)\\), respectivamente. Mostre que \\(\\mathcal{M}_X(t)=\\mathcal{M}_Y(t)\\) se e somente se \\(\\mathbb{P}[X=k]=\\mathbb{P}[Y=k]\\).\nVerifique que \\(\\mathbb{E}[X^r]=\\sum_{j=0}^r{n\\brace k}\\mathbb{E}[(X)_j]\\), onde \\({n\\brace k}=\\frac{1}{k!}\\sum_{i=0}^k(-1)^{k-i}\\binom{k}{i}i^n=\\sum_{i=0}^k\\frac{(-1)^{k-i}i^n}{(k-i)!i!}\\) são chamados de números de Stirling tipo II.\nSeja \\(X\\) uma variável aleatória de uma população com distribuição hipergeométrica com parâmetros \\(N, K\\) e \\(n\\), i.e., \\(\\mathbb{P}[X=x]=\\frac{\\dbinom{K}{x}\\dbinom{N-K}{n-x}}{\\dbinom{N}{n}}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Mostre que \\(\\mathbb{E}[(X)_r]=\\frac{\\dbinom{K}{r}\\dbinom{n}{r}}{\\dbinom{N}{r}}r!\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\). Mostre que \\(\\mathbb{E}[X-\\lambda]^3=\\lambda\\) e \\(\\mathbb{E}[X-\\lambda]^4=\\lambda+3\\lambda^3\\).\nSeja \\(X\\) uma variável aleatória com função geradora de momentos \\(m(t)=\\exp\\{4(e^{t}-1)\\}\\). Qual o valor de \\(\\mathbb{P}[\\mu-2\\sigma&lt;X&lt;\\mu+2\\sigma]\\)?\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Irwin-Hall com parâmetro \\(n\\), i.e.\n\n\\[f(x;n)=\\frac{1}{(n-1)!}\\sum_{k=0}^n(-1)^k\\binom{n}{k}(x-k)_+^{n-1}\\mathbb{I}_{[0,n]}(x),\\] onde \\((x-k)_+^{n-1}=\\begin{cases}x-k, & x-k\\ge0;\\\\ 0, & x-k&lt;0.\\end{cases}\\)\nUtilize a FGM para calcular a variância de \\(X\\).\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão e seja \\(Y=e^X\\). Calcule a FGM e o \\(r\\)-ésimo momento da distribuição de \\(Y\\).\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\) com média \\(\\mu\\) e variância \\(\\sigma^2\\). Suponha que \\(\\mathbb{E}[X^r]=\\mu'_r\\) e \\(\\mathbb{E}[(X-\\mu)^r]=\\mu_r\\) existem, para todo \\(r=1,2,\\ldots\\). Mostre que\n\n\\(\\mathbb{E}\\left[\\bar{X}_n^3\\right]=\\frac{1}{n^2}\\left(\\mu'_3+3(n-1)\\mu'_2\\,\\mu+(n-1)(n-2)\\mu^3\\right)\\);\n\\(\\mathbb{E}\\left[\\bar{X}_n^4\\right]=\\frac{1}{n^3}\\left(\\mu'_4+4(n-1)\\mu'_3\\,\\mu+6(n-1)(n-2)\\mu'_2\\,\\mu^2 +3(n-1)\\mu'^{\\,2}_2+(n-1)(n-2)(n-3)\\mu^4\\right)\\).\n\nSeja \\(U\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\([0,1]\\). Utilize a FGM para calcular a média e a variância de \\(Y=\\tan\\left(\\left(U-\\frac12\\right)\\pi\\right)\\).\nSeja \\(X\\) uma variável aleatória de uma população com distribuição normal padrão. Mostre que\n\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r=&\n  \\begin{cases}\n  2^{r/2}\\frac{\\Gamma\\left(\\frac{r+1}{2}\\right)}{\\Gamma\\left(\\frac12\\right)}, & r \\text{ par};\\\\\n  0, & r \\text{ ímpar}.\n  \\end{cases}\n\\end{align*}\\]\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição binomial com parâmetros \\(n\\) e \\(p\\). Mostre que\n\n\\(\\mu_{r+1}=p(1-p)\\left[\\frac{\\partial\\mu_r}{\\partial p}+nr\\mu_{r-1}\\right]\\);\n\\(\\mathbb{P}[X=x+1]=\\frac{n-x}{x+1}\\frac{p}{1-p}\\mathbb{P}[X=x]\\), para \\(x=0,1,2,\\ldots,n-1\\).\n\n\n\n\n\n\nBillingsley, Patrick. 1986. Probability and Measure. Second. John Wiley; Sons.\n\n\nCurtiss, J. H. 1942. “A note on the theory moment generating functions”. The annals of the mathematical statistics 13 (4): 430–33.\n\n\nDudewicz, E., e S. Mishra. 1988. Modern mathematical statistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An introduction to probability theory and its applications. 2nd ed. Vol. II. Wiley.\n\n\nShohat, J. A., e J. D. Tamarkin. 1970. The problem of moments. Vol. I. American Mathematical Society.",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estatísticas e momentos amostrais</span>"
    ]
  },
  {
    "objectID": "variaveis/p2chap01.html#sec-prop_estimadores",
    "href": "variaveis/p2chap01.html#sec-prop_estimadores",
    "title": "4  Estimação pontual",
    "section": "4.1 Propriedades dos estimadores",
    "text": "4.1 Propriedades dos estimadores\n\nDefinição 4.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente."
  },
  {
    "objectID": "variaveis/p2chap02.html#sec-prop_estimadores",
    "href": "variaveis/p2chap02.html#sec-prop_estimadores",
    "title": "5  Estimação pontual",
    "section": "",
    "text": "Definição 5.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente.",
    "crumbs": [
      "II. Variáveis aleatórias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "variaveis/p2chap03.html#sec-prop_estimadores",
    "href": "variaveis/p2chap03.html#sec-prop_estimadores",
    "title": "6  Estimação pontual",
    "section": "",
    "text": "Definição 6.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\(B(T)=\\mathbb{E}[T]-\\tau(\\theta)\\).\n\n\nSeja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), então\n\\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\);\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2.\n\\end{align*}\\] Daí, \\(\\bar{X}_n\\) e \\(S^2\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente.",
    "crumbs": [
      "II. Variáveis aleatórias",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap01.html",
    "href": "amostral/p3chap01.html",
    "title": "7  Conceitos preliminares",
    "section": "",
    "text": "Definição 7.1 (População alvo) Define-se população alvo, ou simplesmente população, a um conjunto de unidades experimentais.\n\n\nDefinição 7.2 (Amostra aleatória) A sequência \\(X_1,X_2,\\ldots,X_n\\) de variáveis aleatórias independentes e identicamente distribuídas define uma amostra aleatória de tamanho \\(n\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSe as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) configuram uma amostra aleatória de uma população com distribuição contínua, então a função de densidade conjunta é dada por \\[f(x_1,x_2,\\ldots,x_n)=f(x_1)f(x_2)\\cdots f(x_n)=\\prod_{i=1}^nf(x_i), \\tag{7.1}\\] onde \\(f(\\cdot)\\) representa a função de densidade marginal para cada variável aleatória \\(X_i\\), com \\(i=1,2,\\ldots,n\\). Nesse caso a Equação 7.1 representa a distribuição da amostra aleatória.\n\n\n\nExemplo 7.1 Seja \\(X\\) uma variável aleatória de uma população com densidade \\(f(\\cdot)\\). Seja \\(X_1,X_2\\) uma amostra aleatória de tamanho \\(2\\). Por definição, a distribuição conjunta de \\(X_1,X_2\\) é dada por \\[f(x_1,x_2)=f(x_1)f(x_2).\\]\n\n\nExemplo 7.2 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\), i.e.,\n\\[\\mathbb{P}[X=x]=p^x(1-p)^{1-x}\\mathbb{I}_{\\{0,1\\}}(x).\\] Daí,\n\n\\[\\begin{align*}\n    \\mathbb{P}[X_1=x_1,X_2=x_2]&=\\mathbb{P}[X_1=x_1]\\mathbb{P}[X_2=x_2]\\\\\n    &=p^{x_1}(1-p)^{1-x_1}\\mathbb{I}_{\\{0,1\\}}(x_1)p^{x_2}(1-p)^{1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_2)\\\\\n    &=p^{x_1+x_2}(1-p)^{2-x_1-x_2}\\mathbb{I}_{\\{0,1\\}}(x_1)\\mathbb{I}_{\\{0,1\\}}(x_2).\n  \\end{align*}\\]\n\nNesse caso, a distribuição conjunta da amostra está definida no pares \\((x_1,x_2)\\in\\{(0,0), (0,1), (1,0), (1,1)\\}\\).\nCabe ressaltar que, a distribuição conjunta da amostra não é igual à distribuição obtida considerando os elementos de uma distribuição tipo binomial. No caso da distribuição binomial, seja \\(Y=X_1+X_2\\) (com prob. \\(1\\)) então \\[\\mathbb{P}[Y=y]=\\binom{2}{y}p^y(1-p)^{2-y}\\mathbb{I}_{\\{0,1,2\\}}(y).\\] Dessa forma, a distribuição binomial é definida para uma única variável que representa o número de sucessos, e difere da distribuição conjunta da amostra porque a binomial não considera a ordem da coleta, por exemplo, para a distribuição conjunta da amostra, \\(\\mathbb{P}[X_1=0,X_2=1]\\) representa a probabilidade de coletar primeiro \\(0\\) e depois \\(1\\).",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conceitos preliminares</span>"
    ]
  },
  {
    "objectID": "pontual/p4chap01.html#sec-prop_estimadores",
    "href": "pontual/p4chap01.html#sec-prop_estimadores",
    "title": "10  Estimação pontual",
    "section": "",
    "text": "Definição 10.2 (Viés) Um estimador \\(T=t(X_1,X_2,\\ldots,X_n)\\) se diz não-viesado do parâmetro \\(\\tau(\\theta)\\) se e somente se \\(\\mathbb{E}[T]=\\tau(\\theta)\\), para todo \\(\\theta\\in\\Theta\\). O viés do estimador \\(T\\) é dado por \\[B(T)=\\mathbb{E}[T]-\\tau(\\theta)=\\mathbb{E}\\left[t(X_1,X_2,\\ldots,X_n)\\right]-\\tau(\\theta).\\]\n\n\nExemplo 10.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de densidade \\(f(\\cdot;\\mu,\\sigma^2)\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) os estimadores de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente. Calcule o viés dos estimadores.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(\\mathbb{E}[\\bar{X}_n]=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}[X_i]=\\mu\\). Daí, \\(\\bar{X}_n\\) é um estimador não-viesado de \\(\\mu\\).\nPara \\(S^2_n\\), temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S^2_n\\right]&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\sum_{i=1}^n\\mathbb{E}\\left(X_i-\\mu+\\mu-\\bar{X}_n\\right)^2\\\\\n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left\\{\\mathbb{E}\\left(X_i-\\mu\\right)^2+\\mathbb{E}\\left(\\mu-\\bar{X}_n\\right)^2-2\\mathbb{E}\\left(X_i-\\mu\\right)\\left(\\mu-\\bar{X}_n\\right)\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí, \\(\\bar{X}_n\\) e \\(S^2_n\\) são esimadores não-viesados de \\(\\mu\\) e \\(\\sigma^2\\), respectivamente.\n\n\n\n\nDefinição 10.3 (Erro Quadrático Médio) Seja \\(T=t(X_1,X_2,\\ldots,X_n)\\) um estimador do parâmetro \\(\\tau(\\theta)\\). O Erro Quadrático Médio (EQM) de \\(T\\) define-se como \\[\\mathrm{EQM(T)}=\\mathbb{E}\\left[T-\\tau(\\theta)\\right]^2=\\mathbb{E}\\left[t(X_1,X_2,\\ldots,X_n)-\\tau(\\theta)\\right]^2.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nNote que,\n\n\\[\\begin{align*}\n\\mathrm{EQM(T)}&=\\mathbb{E}\\left[T-\\tau(\\theta)\\right]^2=\\mathbb{E}\\left[T^2-2T \\tau(\\theta)+\\tau^2(\\theta)\\right]\n=\\mathbb{E}\\left[T^2\\right]-2\\tau(\\theta)\\mathbb{E}\\left[T\\right] +\\tau^2(\\theta)\\\\\n&=\\mathbb{E}\\left[T^2\\right]-\\left[\\mathbb{E}T\\right]^2+\\left[\\mathbb{E}T\\right]^2+2\\tau(\\theta)\\mathbb{E}\\left[T\\right] +\\tau^2(\\theta)\n=\\mathrm{var}\\left[T\\right]+\\left[\\mathbb{E}T-\\tau(\\theta)\\right]^2\\\\\n&=\\mathrm{var}\\left[T\\right]+B^2(T). \\quad\\checkmark\n\\end{align*}\\]\n\nDo resultado acima, pode-se afirmar que, se o estimador \\(T\\) é não-viesado, então \\(\\mathrm{EQM(T)}=\\mathrm{var}[T]\\).\n\n\n\nExemplo 10.3 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Calcule o EQM do estimador \\(T=M_2'=\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara o cálculo, podemos aproveitar o resultado do Exemplo 10.2, dessa forma\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[M_2'\\right]&=\\frac1n\\mathbb{E}\\left[\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right]\n=\\frac{n-1}{n}\\frac{1}{n-1}\\left[\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\right]\n=\\frac{n-1}{n}\\mathbb{E}\\left[S^2\\right]=\\frac{n-1}{n}\\sigma^2. \\quad\\checkmark\n\\end{align*}\\]\n\nO EQM é calculado como\n\n\\[\\begin{align*}\n\\mathrm{EQM}\\left(M_2'\\right)&=\\mathbb{E}\\left[\\frac{n-1}{n}S^2-\\sigma^2\\right]^2\n=\\mathrm{var}\\left[M_2'\\right]+\\left(\\frac{n-1}{n}\\sigma^2-\\sigma^2\\right)^2\n=\\mathrm{var}\\left[M_2'\\right]+\\frac{\\sigma^4}{n^2}\\\\\n&=\\frac{(n-1)^2}{n^2}\\mathrm{var}\\left[S^2\\right]+\\frac{\\sigma^4}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPelo Teorema 8.2, temos que\n\n\\[\\begin{align*}\n\\mathrm{EQM}\\left(M_2'\\right) &=\\frac{(n-1)^2}{n^2}\\mathrm{var}\\left[S^2\\right]+\\frac{\\sigma^4}{n^2}\n= \\frac{(n-1)^2}{n^2}\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)+\\frac{\\sigma^4}{n^2},  \\quad\\checkmark\n\\end{align*}\\]\n\npara \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\).\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nO EQM é comumente utilizado para comparar o desempenho de dois estimadores, por exemplo, se existir um estimador não-viesado \\(T^*\\), tal que, \\(\\mathrm{var}\\left[T^*\\right]\\le\\mathrm{var}\\left[T\\right]\\), então, o estimador \\(T^*\\) é dito um estimador não-viesado com variância minima.\n\n\n\nExemplo 10.4 Seja \\(X_1,X_2\\) e \\(X_3\\) uma amostra aleatória de uma população com função de densidade de probabilidade \\(f(x; \\theta, 1)\\), onde \\(\\theta\\) e \\(1\\) representam a média e a variância da população, respectivamente. Considere os seguintes estimadores: \\(T_1=\\bar{X}_3=\\frac13\\sum_{i=1}^3X_i\\) e \\(T_2=\\frac12X_1+\\frac14X_2+\\frac14X_3\\). Calcule o viés e o \\(\\mathrm{EQM}\\) dos estimadores.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Corolário 8.1, temos que \\(\\mathbb{E}T_1=\\mathbb{E}[\\bar{X}_3]=\\theta\\) e \\(\\mathrm{var}\\left[\\bar{X}_3\\right]=\\frac1n\\). Por outra parte, \\(\\mathbb{E}T_2=\\left(\\frac12+\\frac14+\\frac14\\right)\\theta=\\theta\\) e \\(\\mathrm{var}\\left[T_2\\right]=\\frac{6}{16}\\). Comparando os dois estimadores, observa-se que o estimador \\(T_1=\\bar{X}_3\\) é um estimador não-viesado de menor variância.\n\n\n\n\nExemplo 10.5 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com distribuição uniforme no intervalo \\((0,\\theta)\\). Considere os seguintes estimadores para \\(\\theta\\): \\(T_1=\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(T_2=Y_n=\\max\\{X_1,X_2,\\ldots, X_n\\}\\). Calcule o viés e o \\(\\mathrm{EQM}\\) dos estimadores.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se \\(X\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,\\theta)\\), então \\(\\mathbb{E}[X]=\\frac12\\theta\\) e \\(\\mathrm{var}[X]=\\frac{\\theta^2}{12}\\). Dessa forma, pelo Corolário 8.1, temos que \\(\\mathbb{E}T_1=\\frac12\\theta\\) e \\(\\mathrm{var}\\left[T_1\\right]=\\frac{\\theta^2}{12n}\\). De fato, o estimador \\(T_1\\) é um estimador viesado para \\(\\theta\\).\nO \\(\\mathrm{EQM}\\) do estimador \\(T_1\\) é dado por \\[\\mathrm{EQM}(T_1)=\\frac{\\theta^2}{12n}+\\left(\\frac12\\theta-\\theta\\right)^2=\\frac{1+3n}{12n}\\theta^2. \\quad\\checkmark\\]\nPor outra parte, pelo Teorema 9.2, \\[f_{Y_n}(y)=\\frac{n!}{(n-1)!(n-n)!}\\left[F(y)\\right]^{n-1}[1-F(y)]^{n-n}f(y)=n\\left[F(y)\\right]^{n-1}f(y)=n\\left(\\frac{y}{\\theta}\\right)^{n-1}\\frac1\\theta,\\] para \\(0&lt;y&lt;\\theta\\).\nDaí,\n\n\\[\\begin{align*}\n\\mathbb{E}Y_n &=\\int_0^\\theta x\\frac{nx^{n-1}}{\\theta^n}\\,dx=\\frac{n}{\\theta^n}\\left.\\frac{x^{n+1}}{n+1}\\right|_0^\\theta\n=\\frac{n}{\\theta^n}\\frac{\\theta^{n+1}}{n+1}=\\frac{n}{n+1}\\theta;  \\\\\n\\mathbb{E}Y^2_n &=\\int_0^\\theta x^2\\frac{nx^{n-1}}{\\theta^n}\\,dx=\\frac{n}{\\theta^n}\\left.\\frac{x^{n+2}}{n+2}\\right|_0^\\theta\n=\\frac{n}{\\theta^n}\\frac{\\theta^{n+2}}{n+2}=\\frac{n}{n+2}\\theta^2.\n\\end{align*}\\]\n\nAssim, \\(\\mathrm{var}[Y_n]=\\frac{n}{n+2}\\theta^2-\\left(\\frac{n}{n+1}\\theta\\right)^2=\\frac{n(n+1)^2-n^2(n+2)}{(n+1)^2(n+2)}\\theta^2=\\frac{n}{(n+1)^2(n+2)}\\theta^2\\). Dessa forma,\n\n\\[\\begin{align*}\n\\mathrm{EQM}(T_2) &= \\frac{n}{(n+1)^2(n+2)}\\theta^2+\\left(\\frac{n}{n+1}\\theta-\\theta\\right)^2\n=\\frac{n}{(n+1)^2(n+2)}\\theta^2+\\frac{1}{(n+1)^2}\\theta^2\\\\\n&=\\frac{n+n+2}{(n+1)^2(n+2)}\\theta^2=\\frac{2}{(n+1)(n+2)}\\theta^2. \\quad\\checkmark\n\\end{align*}\\]\n\nObserva-se que, o estimador \\(T_2\\) é assintoticamente não-viesado e o \\(\\mathrm{EQM}(T_2)\\longrightarrow0\\), quando \\(n\\to\\infty\\).\n\n\n\n\nExemplo 10.6 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com densidade \\(f(\\cdot; \\mu, \\sigma^2)\\).\n\nMostre que \\(\\sum_{i=1}^na_iX_i\\) é um estimador não-viesado de \\(\\mu\\), para qualquer conjunto de constantes \\(a_1,a_2,\\ldots,a_n\\), tal que \\(\\sum_{i=1}^na_i=1\\);\nSe \\(\\sum_{i=1}^na_i=1\\), mostre que \\(\\mathrm{var}\\left[\\sum_{i=1}^na_iX_i\\right]\\) tem valor mínimo quando \\(a_i=\\frac1n\\), com \\(i=1,2,\\ldots,n\\).\n\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\n\n\\(\\mathbb{E}\\left[\\sum_{i=1}^na_iX_i\\right]=\\sum_{i=1}^na_i\\mathbb{E}\\left[X_i\\right]=\\sum_{i=1}^na_i\\mu=\\mu\\sum_{i=1}^na_i=\\mu\\);\n\\(\\mathrm{var}\\left[\\sum_{i=1}^na_iX_i\\right]=\\sum_{i=1}^na^2_i\\mathrm{var}\\left[X_i\\right]=\\sum_{i=1}^na^2_i\\sigma^2=\\sigma^2\\sum_{i=1}^na^2_i\\). O termo \\(\\sum_{i=1}^na^2_i\\) pode ser reescrito como\n\n\n\\[\\begin{align*}\n\\sum_{i=1}^na^2_i &= \\sum_{i=1}^n\\left(a_i-\\frac1n+\\frac1n\\right)^2\n= \\sum_{i=1}^n\\left(a_i-\\frac1n\\right)^2+\\sum_{i=1}^n\\frac{1}{n^2}+2\\sum_{i=1}^n\\left(a_i-\\frac1n\\right)\\frac1n\\\\\n&= \\sum_{i=1}^n\\left(a_i-\\frac1n\\right)^2+\\frac{1}{n}+\\frac2n\\sum_{i=1}^n\\left(a_i-\\frac1n\\right)= \\sum_{i=1}^n\\left(a_i-\\frac1n\\right)^2+\\frac{1}{n}. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí, \\(\\mathrm{var}\\left[\\sum_{i=1}^na_iX_i\\right]=\\sigma^2\\sum_{i=1}^na^2_i=\\sigma^2\\left[\\sum_{i=1}^n\\left(a_i-\\frac1n\\right)^2+\\frac{1}{n}\\right]=\\sigma^2\\sum_{i=1}^n\\left(a_i-\\frac1n\\right)^2+\\frac{\\sigma^2}{n}\\). Para avaliar os valores de \\(a_i\\) que minimizam a variância, considere o seguinte cenário\n\\[\\underset{c\\in\\mathbb{R}}{\\arg\\min}\\,\\mathcal{h}(c)=\\underset{c\\in\\mathbb{R}}{\\arg\\min}\\sum_{i=1}^n\\left(a_i-c\\right)^2.\\] Assim, \\(\\mathcal{h}'(c)=\\sum_{i=1}^n2\\left(a_i-c\\right)(-1)\\), então para \\(\\mathcal{h}'(c)=0\\), \\(c=\\frac1n\\) um ponto crítico. Sendo a segunda derivada positiva, o valor \\(c=\\frac1n\\) é um mínimo. Dessa forma, a variância de \\(\\sum_{i=1}^na_iX_i\\) será mínima quando \\(a_i=\\frac1n\\), para \\(i=1,2,3,\\ldots,n. \\quad\\checkmark\\)\n\n\n\n\nDefinição 10.4 (Consistência em Erro Quadrático Médio) Seja \\(T_1,T_2,\\ldots,T_k, T_{k+1}, \\ldots\\) uma sequência de estimadores de \\(\\tau(\\theta)\\), tal que \\(T_k=t(X_1,X_2,\\ldots,X_n)\\). A sequência \\(\\{T_k\\}\\) se diz consistente em Erro Quadrático Médio, com respeito a \\(\\tau(\\theta)\\), see \\[\\lim_{k\\to\\infty}\\mathbb{E}\\left[T_k-\\tau(\\theta)\\right]^2=0,\\] para todo \\(\\theta\\in\\Theta\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nA consistência em \\(\\mathrm{EQM}\\) implica que tanto a variância, quanto o viés, se aproximam de zero quando \\(n\\to\\infty\\).\n\n\n\n\nDefinição 10.5 (Consistência) Seja \\(T_1,T_2,\\ldots,T_k, T_{k+1}, \\ldots\\) uma sequência de estimadores de \\(\\tau(\\theta)\\), tal que \\(T_k=t(X_1,X_2,\\ldots,X_n)\\). A sequência \\(\\{T_k\\}\\) se diz consistente, com respeito a \\(\\tau(\\theta)\\), see \\[ T_k \\underset{k\\to\\infty}{\\xrightarrow{p}} \\tau(\\theta),\\] para todo \\(\\theta\\in\\Theta\\).\n\n\n\n\n\n\n\nNotação\n\n\n\nPara facilitar a escrita da notação, daqui em diante será utilizada a notação \\(T_k \\overset{p}{\\longrightarrow} \\tau(\\theta)\\) para denotar a convergência em probabilidade.\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe um estimador é consistente em \\(\\mathrm{EQM}\\), então, o estimador é consistente. Tal afirmação pode ser verificada com a seguinte relação:\n\\[\\mathbb{P}\\left(|T_n-\\tau(\\theta)|&lt;\\epsilon\\right)=\\mathbb{P}\\left(|T_n-\\tau(\\theta)|^2&lt;\\epsilon^2\\right)\\ge1-\\frac{\\mathbb{E}\\left[T_n-\\tau(\\theta)\\right]^2}{\\epsilon^2}.\\]",
    "crumbs": [
      "IV. Estimação",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap03.html",
    "href": "amostral/p3chap03.html",
    "title": "9  Estatísticas de ordem",
    "section": "",
    "text": "Definição 9.1 (Estatísticas de ordem) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\). As variáveis aleatórias ordenadas em ordem crescente, i.e. \\(X_{(1)},X_{(2)},\\ldots,X_{(n)}\\), tal que \\(X_{(1)}\\le X_{(2)}\\le\\ldots\\le X_{(n)}\\) são chamadas de estatísticas de ordem da amostra \\(X_1,X_2,\\ldots,X_n\\).\n\n\n\n\n\n\n\nNotação\n\n\n\nNa literatura é comum encontrar várias formas para denotar as estatísticas de ordem de uma amostra aleatória, na Definição 9.1, e.g., foi apresentada a seguinte notação: \\(X_{(1)},X_{(2)},\\ldots,X_{(n)}\\). Outras notações encontradas são as seguintes: \\(X_{1:n},X_{2:n},\\ldots,X_{n:n}\\) ou simplesmente \\(Y_1,Y_2,\\ldots, Y_n\\), onde \\(Y_1\\le Y_2\\le \\cdots\\le Y_n\\), i.e., \\(Y_1=\\min\\{X_1,X_2,\\ldots,X_n\\}\\) e \\(Y_n=\\max\\{X_1,X_2,\\ldots,X_n\\}\\). Ao longo do capítulo, adotaremos a notação \\(Y_1,Y_2, \\ldots, Y_n\\) para representar as estatísticas de ordem da amostra \\(X_1,X_2,\\ldots,X_n\\), por ser esta mais simples.\n\n\n\n\nTeorema 9.1 Sejam \\(Y_1,Y_2,\\ldots, Y_n\\) as estatísticas de ordem de uma amostra aleatória \\(X_1,X_2,\\ldots,X_n\\) com distribuição \\(F\\). A distribuição acumulada marginal de \\(Y_\\alpha\\), onde \\(\\alpha=1,2,\\ldots, n\\), é dada por\n\\[F_{Y_\\alpha}(y)=\\sum_{j=\\alpha}^n\\binom{n}{j}\\left[F(y)\\right]^j\\left[1-F(y)\\right]^{n-j}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara \\(y\\) fixo, seja \\(Z_i=\\mathbb{I}_{(-\\infty,y]}(X_i)\\), então \\(\\sum_{i=1}^nZ_i\\) representa o número de eventos \\(X_i\\le y\\). Daí, \\(\\sum_{i=1}^nZ_i\\) segue uma distribuição binomial com parâmetros \\(n\\) e \\(F(y)\\).\nDessa forma,\n\\[F_{Y_\\alpha}(y)=\\mathbb{P}[Y_\\alpha\\le y]=\\mathbb{P}\\left[\\sum_{i=1}^nZ_i\\ge \\alpha\\right] =\\sum_{j=\\alpha}^n\\binom{n}{j}\\left[F(y)\\right]^j\\left[1-F(y)\\right]^{n-j}. \\quad\\blacksquare\\]\n\n\n\n\n\nCorolário 9.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição \\(F\\) e sejam \\(Y_1,Y_2,\\ldots, Y_n\\) as estatísticas de ordem associadas à amostra. Então\n\nDistribuição do Máximo: \\(F_{Y_n}[y]=\\left[F(y)\\right]^n\\);\nDistribuição do Mínimo: \\(F_{Y_1}[y]=1-\\left[1-F(y)\\right]^n\\);\n\n\n\n\nExemplo 9.1  \nSejam \\(Y_1\\le Y_2\\le \\cdots\\le Y_{10}\\) as estatísticas de ordem associadas a uma amostra aleatória de uma população com densidade uniforme no intervalo \\((0,1)\\).\n\n\nQuál o valor de \\(\\mathbb{P}[Y_{10}&lt;0.9]\\)?\n\n\nQuál o valor de \\(k\\) para que \\(\\mathbb{P}[Y_1&gt;k]=0.5\\)?\n\n\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\n\n\n\\(F_{Y_{10}}(y)=\\mathbb{P}[Y_{10}&lt;0.9]=\\left[F(y)\\right]^{10}=y^{10}=0.9^{10}=0.34867844;  \\quad\\checkmark\\)\n\n\nSabemos que, \\(F_{Y_{1}}(y)=\\mathbb{P}[Y_1&lt;y]=1-[1-F(y)]^{10}=1-[1-y]^{10}\\). Daí, \\(\\mathbb{P}[Y_1&gt;k]=1-\\mathbb{P}[Y_1\\le k]=[1-k]^{10}\\).\nEntão, o exercício se reduz a encontrar as raízes do seguinte polinômio:\n\n\\[\\begin{align*}\n  k^{10}-10k^9+45k^8-120k^7+210k^6-252k^5+210k^4-120k^3+45k^2-10k+0.5=0;\n\\end{align*}\\]\n\nDessa forma, as únicas raizes reais são \\(k=0.066967\\) e \\(k=1.93303\\), sendo esses os possíveis valores de \\(k. \\quad\\blacksquare\\)\n\n\n\n\n\n\n\nTeorema 9.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\) e função de densidade de probabilidade \\(f\\). Sejam \\(Y_1,Y_2,\\ldots, Y_n\\) as estatísticas de ordem da amostra aleatória. Então,\n\n\n\\(f_{Y_\\alpha}(y)=\\frac{n!}{(\\alpha-1)!(n-\\alpha)!}[F(y)]^{\\alpha-1}[1-F(y)]^{n-\\alpha}f(y)\\);\n\n\n\\(f_{Y_\\alpha Y_\\beta}(x,y)=\\frac{n!}{(\\alpha-1)!(\\beta-\\alpha-1)!(n-\\beta)!}[f(x)]^{\\alpha-1}[F(y)-F(x)]^{\\beta-\\alpha-1}[1-F(y)]^{n-\\beta}f(x)f(y)\\mathbb{I}_{(x,\\infty)}(y)\\);\n\n\n\\(f_{Y_1,Y_2,\\ldots, Y_n}(y_1,y_2,\\ldots,y_n)=n!f(y_1)f(y_2)\\ldots f(y_n)\\), para \\(y_1&lt;y_2&lt;\\cdots&lt;y_n\\).\n\n\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\n\n\nPelo ?exm-binom_beta, sabemos que\n\n\\[\\begin{align*}\n    F_{Y_\\alpha}(y)=\\sum_{j=\\alpha}^n\\binom{n}{j}[F(y)]^j[1-F(y)]^{n-j}=\\alpha\\binom{n}{\\alpha}\\int_0^{F(y)}t^{\\alpha-1}(1-t)^{n-\\alpha}\\,dt.\n\\end{align*}\\]\n\nDaí, \\(\\frac{\\partial}{\\partial y}F_{Y\\alpha}(y)=\\alpha\\binom{n}{\\alpha}[F(y)]^{\\alpha-1}[1-F(y)]^{n-\\alpha}f(y)\\).\n\nOutra forma de provar\n\nSabemos que,\n\n\\[\\begin{align*}\n    f_{Y_\\alpha}(y)=\\lim_{h\\to0}\\frac{F_{Y_\\alpha}(y+h)-F_{Y_\\alpha}(y)}{h}=\\lim_{h\\to0}\\frac{\\mathbb{P}[y\\le Y_\\alpha\\le y+h]}{h}.\n\\end{align*}\\]\n\nPor outra parte, observe que o evento \\([y\\le Y_\\alpha\\le y+h]\\) é equivalente ao evento \\(A(h):\\) “\\(\\alpha-1\\) observações menores que \\(y\\), uma observação em \\([y,y+h]\\) e \\(n-\\alpha\\) observações menores que \\(y+h\\)”.\nAssim,\n\n\\[\\begin{align*}\n    \\mathbb{P}[A(h)]=\\frac{n!}{(\\alpha-1)!1!(n-\\alpha)!}[F(y)]^{\\alpha-1}[F(y+h)-F(y)][1-F(y+h)]^{n-\\alpha}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n    f_{Y_\\alpha}(y)&=\\lim_{h\\to0}\\frac{n!}{(\\alpha-1)!1!(n-\\alpha)!}\\frac{[F(y)]^{\\alpha-1}[F(y+h)-F(y)][1-F(y+h)]^{n-\\alpha}}{h}\\\\\n    &=\\frac{n!}{(\\alpha-1)!1!(n-\\alpha)!}[F(y)]^{\\alpha-1}[1-F(y+h)]^{n-\\alpha}f(y)\\\\\n    &=\\alpha\\binom{n}{\\alpha}[F(y)]^{\\alpha-1}[1-F(y+h)]^{n-\\alpha}f(y). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n  f_{Y_\\alpha Y_\\beta}(x,y)&=\\lim_{h\\to0}\\lim_{t\\to0}\\frac{F_{Y_\\alpha Y_\\beta}(x+h,y+t)-F_{Y_\\alpha Y_\\beta}(x,y+t)-F_{Y_\\alpha Y_\\beta}(x+h,y)+F_{Y_\\alpha Y_\\beta}(x,y))}{ht}\\\\\n  &=\\lim_{h\\to0}\\lim_{t\\to0}\\frac{\\mathbb{P}[x\\le Y_\\alpha\\le x+h;y\\le Y_\\beta\\le y+t]}{ht}\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  &\\mathbf{Intervalo} & & \\mathbf{Probabilidade}\\\\\n  I_1 & = (-\\infty,x] & & F(x)\\\\\n  I_2 & = (x,x+h] & & F(x+h)-F(x)\\\\\n  I_3 & = (x+h,y] & & F(y)-F(x+h)\\\\\n  I_4 & = (y,y+t] & & F(y+t)-F(y)\\\\\n  I_5 & = (y+t,\\infty) & & 1-F(y+t)\\\\\n\\end{align*}\\]\n\nDessa forma, define-se o evento\n\n\\(A(h,t):\\) “\\(\\alpha-1\\) observações pertencem ao intervalo \\(I_1\\), \\(1\\) observação pertence ao intervalo \\(I_2\\), \\(1\\) observação pertence ao intervalo \\(I_4\\), \\(n-\\beta\\) observações pertencem ao intervalo \\(I_5\\) e \\(\\beta-\\alpha-1\\) observações pertencem ao intervalo \\(I_3\\)”.\n\n\n\n\nDaí,\n\n\n\\[\\begin{align*}\n  f_{Y_\\alpha Y_\\beta}(x,y)&=\\lim_{h\\to0}\\lim_{t\\to0}\\frac{\\mathbb{P}[A(h,t)]}{ht}\\\\\n  &=\\lim_{h\\to0}\\lim_{t\\to0}\\frac{n!}{(\\alpha-1)!1!(\\beta-\\alpha-1)!1!(n-\\beta)!}[F(x)]^{\\alpha-1}\\\\\n  &\\hspace{3cm}\\times[F(x+h)-F(x)][F(y)-F(x+h)]^{\\beta-\\alpha-1}[F(y+h)-F(y)][1-F(y+t)]^{n-\\beta}\\\\ \\\\\n  &=\\frac{n!}{(\\alpha-1)!(\\beta-\\alpha-1)!(n-\\beta)!}[F(x)]^{\\alpha-1}[F(y)-F(x+h)]^{\\beta-\\alpha-1}[1-F(y+t)]^{n-\\beta}f(x)f(y),\n\\end{align*}\\]\npara \\(x&lt;y\\). \\(\\quad\\checkmark\\)\n\n\n\n\n\nEm geral, temos que\n\n\\[\\begin{align*}\nf_{Y_1,Y_2,\\ldots, Y_n}(y_1,y_2,\\ldots,y_n)&=\\lim_{h_1\\to0}\\lim_{h_2\\to0}\\cdots\\lim_{h_n\\to0}\n\\frac{\\mathbb{P}\\left[\\cap_{i=1}^n[y_i\\le Y_i\\le y_i+h_i]\\right]}{\\prod_{i=1}^nh_i}\\\\\n&=\\lim_{h_1\\to0}\\lim_{h_2\\to0}\\cdots\\lim_{h_n\\to0}\n\\frac{\\mathbb{P}\\left[\\text{um } X_i\\in[y_1, y_1+h_1], \\text{um } X_i\\in[y_2, y_2+h_2],\\ldots, \\text{um } X_i\\in[y_n, y_n+h_n]\\right]}{\\prod_{i=1}^nh_i}\\\\\n&=n!f(y_1)f(y_2)\\cdots f(y_n)=n!\\prod_{i=1}^nf(y_i), \\quad y_1&lt;y_2&lt;\\cdots&lt;y_n. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\n\n\nExemplo 9.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com densidade \\(f\\) e função de distribuição acumulada \\(F\\). Sejam \\(Y_1\\le Y_2\\le \\cdots\\le Y_n\\) as estatísticas de ordem asssociadas à amostra. Sejam \\(R=Y_n-Y_1\\) e \\(T=\\frac{Y_1+Y_n}{2}\\). Encontre a distribuição de \\(T\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nConsidere \\(r=y_n-y_1\\) e \\(t=\\frac{y_1+y_n}{2}\\), então \\(y_n=\\frac12r+t\\) e \\(y_1=t-\\frac12r\\). Daí,\n\n\\[\\begin{align*}\n  \\frac{\\partial y_n}{\\partial r}&=\\frac12 & \\frac{\\partial y_1}{\\partial r}&=-\\frac12\\\\\n  \\frac{\\partial y_n}{\\partial t}&=1 & \\frac{\\partial y_1}{\\partial t}&=1.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  f_{R,T}(r,t)=\\frac{n!}{(n-2)!}\\left[F\\left(t+\\frac12r\\right)-F\\left(t-\\frac12r\\right)\\right]^{n-2}f\\left(t-\\frac12r\\right)f\\left(t+\\frac12r\\right), \\quad r&gt;0,\n\\end{align*}\\]\n\nentão,\n\n\\[\\begin{align*}\n  f_{T}(t)=\\int_{-\\infty}^\\infty f_{R,T}(r,t)\\,dt=n(n-1)\\int_{-\\infty}^\\infty\\left[F\\left(t+\\frac12r\\right)-F\\left(t-\\frac12r\\right)\\right]^{n-2}f\\left(t-\\frac12r\\right)f\\left(t+\\frac12r\\right)\\,dt.  \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 9.3  \nSejam \\(Y_1\\le Y_2\\le Y_3\\le Y_4\\) as estatísticas de ordem associadas a uma amostra aleatória \\(X_1,X_2,X_3,X_4\\), de uma população com densidade \\(f(x)=2x\\mathbb{I}_{(0,1)}(x)\\).\n\n\nCalcule a densidade conjunta de \\((Y_3, Y_4)\\);\n\n\nCalcule a densidade condicional de \\(Y_3\\) dado \\(Y_4=y_4\\);\n\n\nCalcule \\(\\mathbb{E}[Y_3|Y_4=y_4]\\).\n\n\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\n\n\nPelo Teorema 9.1, sabemos que \\(f_{Y_1, Y_2, Y_3, Y_4}(y_1,y_2,y_3,y_4)=4!(2y_1)(2y_2)(2y_3)(2y_4)\\), para \\(0&lt;y_1\\le y_2\\le y_3\\le y_4&lt;1\\). Dessa forma,\n\n\\[\\begin{align*}\n  f_{Y_3,Y_4}(y_3, y_4)&=\\int_{y_2=0}^{y_3}\\int_{y_1=0}^{y_2}4!(2y_1)(2y_2)(2y_3)(2y_4)\\,dy_1dy_2\n  =48y_3^5y_4, \\quad 0&lt;y_3\\le y_4&lt;1;  \\quad\\checkmark\n\\end{align*}\\]\n\nComo exercício, use Item 2 do Teorema 9.1 para obter o mesmo resultado.\n\n\n\n\nSabemos que, \\(f_{Y_3|Y_4}(y_3|y_4)=\\dfrac{f_{Y_3,Y_4}(y_3,y_4)}{f_{Y_4}(y_4)}=\\dfrac{48y_3^5y_4}{8y_4^7}=\\dfrac{6y_3^5}{y_4^6}\\) para \\(0&lt;y_3\\le y_4\\); \\(\\quad\\checkmark\\)\n\n\nUsando o resultado do item anterior, temos que \\(\\mathbb{E}[Y_3|Y_4=y_4]=\\int_{0}^{y_4}\\frac{6y_3^6}{y_4^6}\\,dy_3=\\frac67y_4\\). para \\(0&lt;y_4&lt;1. \\quad\\checkmark\\)\n\n\n\n\n\n\nExemplo 9.4 Sejam \\(Y_1\\le Y_2\\le Y_3\\) as estatísticas de ordem associadas a uma amostra aleatória de tamanho \\(3\\), de uma população com densidade \\(f(x)=2x\\mathbb{I}_{(0,1)}(x)\\). Mostre que \\(Z_1=\\frac{Y_1}{Y_2}\\), \\(Z_2=\\frac{Y_2}{Y_3}\\) e \\(Z_3=Y_3\\) são mutuamente independentes.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n  f(x_1, x_2, x_3)&=\\prod_{i=1}^32x_i\\mathbb{I}_{(0,1)}(x_i)=8x_1x_2x_3\\prod_{i=1}^3\\mathbb{I}_{(0,1)}(x_i).\n\\end{align*}\\]\n\nConsidere, \\(z_1=\\frac{y_1}{y_2}\\), \\(y_2=\\frac{y_2}{y_3}\\) e \\(z_3=y_3\\), então \\(y_1=z_1z_2z_3\\), \\(y_2=z_2z_3\\) e \\(y_3=z_3\\). Daí,\n\n\\[\\begin{align*}\n  \\frac{\\partial y_1}{\\partial z_1}&=z_2z_3 & \\frac{\\partial y_2}{\\partial z_1}&=0 & \\frac{\\partial y_3}{\\partial z_1}&=0\\\\\n  \\frac{\\partial y_1}{\\partial z_2}&=z_1z_3 & \\frac{\\partial y_2}{\\partial z_2}&=z_3 & \\frac{\\partial y_3}{\\partial z_2}&=0\\\\\n  \\frac{\\partial y_1}{\\partial z_3}&=z_1z_2 & \\frac{\\partial y_2}{\\partial z_3}&=z_2 & \\frac{\\partial y_3}{\\partial z_3}&=1.\n\\end{align*}\\]\n\nPor outro lado, sabemos que \\(f(y_1,y_2,y_3)=3!(2y_1)(2y_2)(2y_3)\\) e o jacobiano é dado por \\(z_2z_3^2\\). Dessa forma, temos que\n\n\\[\\begin{align*}\n  f(z_1,z_2,z_3)&=3! (2z_1z_2z_3) (2z_2z_3) (2z_3) (z_2z_3^2)=48z_1z_2^2z_3^5=(2z_1)(4z_2^3)(6z_3^5),\n\\end{align*}\\]\n\npara \\(0&lt;z_i&lt;1, \\quad i=1,2,3\\). Agora é só verificar que \\(f(z_1)=2z_1\\), \\(f(z_2)=4z_2^3\\) e \\(f(z_3)=6z_3^5\\). Por exemplo,\n\n\\[\\begin{align*}\n  f(z_3)&=\\int_0^1\\int_0^1(2z_1)(4z_2^3)(6z_3^5)\\,dz_1dz_2=6z_3^5\\int_0^1\\int_0^1(2z_1)(4z_2^3)\\,dz_1dz_2\\\\\n  &=6z_3^5\\int_0^1 4z_2^3\\,dz_2= 6z_3^5. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 9.5  \nSejam \\(Y_1\\le Y_2\\le Y_3\\) as estatísticas de ordem associadas a uma amostra aleatória de tamanho \\(3\\), de uma população com densidade \\(f(x)=2x\\mathbb{I}_{(0,1)}(x)\\).\n\n\nCalcule a probabilidade do mínimo ser maior que a mediana da população da amostra;\n\n\nQual a correlação entre \\(Y_2\\) e \\(Y_3\\)?\n\n\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\n\n\nSabemos que, \\(F(x)=\\mathbb{P}[X\\le x]=x^2\\). A mediana é o valor \\(m\\), tal que \\(\\int_0^m2x\\,dx=\\frac12\\). Daí, \\(m=\\frac{\\sqrt2}{2}\\).\nPelo Corolário 9.1, temos que \\(\\mathbb{P}[Y_1&gt;m]=[1-F(m)]^3=[1-m^2]^3=\\frac18. \\quad\\checkmark\\)\n\n\nSabemos que, \\(\\textrm{Corr}[Y_2,Y_3]=\\frac{\\textrm{Cov}[Y_2,Y_3]}{\\sqrt{\\textrm{Var}[Y_2]\\textrm{Var}[Y_3]}}=\\frac{\\mathbb{E}[Y_2Y_3]-\\mathbb{E}Y_2\\mathbb{E}Y_3}{\\sqrt{\\textrm{Var}[Y_2]\\textrm{Var}[Y_3]}}\\).\nPelo Teorema 9.1, temos que, \\(f_{Y_2,Y_3}(y_2,y_3)=24y_2^3y_3\\), para \\(0&lt;y_2\\le y_3&lt;1\\). Desse resultado, também temos que, \\(f_{Y_2}(x)=12(x^3-x^5)\\), para \\(0&lt;x&lt;1\\) e \\(f_{Y_3}(y)=6y^5\\), para \\(0&lt;y&lt;1\\). Assim, \\(\\mathbb{E}[Y_2Y_3]=\\int_0^1\\int_0^{y_3}y_2y_324y_2^3y_3\\,dy_2dy_3=\\frac35\\), \\(\\mathbb{E}Y_2=\\frac{24}{35}\\), \\(\\mathrm{Var}[Y_2]=\\frac12-\\left(\\frac{24}{35}\\right)^2=\\frac{73}{2450}\\), \\(\\mathbb{E}Y_3=\\frac67\\) e \\(\\mathrm{Var}[Y_3]=\\frac68-\\left(\\frac67\\right)^2=\\frac{6}{392}\\).\nDessa forma, \\[\\textrm{Corr}[Y_2,Y_3]=\\frac{\\mathbb{E}[Y_2Y_3]-\\mathbb{E}Y_2\\mathbb{E}Y_3}{\\sqrt{\\textrm{Var}[Y_2]\\textrm{Var}[Y_3]}}=\\frac{\\frac35-\\frac{24}{35}\\frac67}{\\sqrt{\\frac{73}{2450}\\frac{6}{392}}}=0.573382179 \\quad\\blacksquare\\]",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estatísticas de ordem</span>"
    ]
  },
  {
    "objectID": "variaveis/p2chap01.html",
    "href": "variaveis/p2chap01.html",
    "title": "4  Função de distribuição",
    "section": "",
    "text": "Exemplo 4.1 Seja \\(X\\) uma variável aleatória de uma população com distribuição \\(Binomial(n,p)\\) e seja \\(Y\\) uma variável aleatória de uma população com distribuição \\(Beta(k,n-k+1)\\). Mostre que, \\(F_Y(p)=1-F_X(k-1)\\), ou seja,\n\\[\\mathbb{P}[X\\ge k]=\\sum_{j=k}^n\\binom{n}{j}p^j(1-p)^{n-j}=\\frac{1}{B(k,n-k+1)}\\int_0^pu^{k-1}(1-u)^{n-k}\\,du,\\] onde \\(B(a,b)=\\int_0^1x^{a-1}(1-x)^{b-1}\\,dx=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\) representa a função beta.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPrimeiramente, observe que\n\n\\[\\begin{align*}\n  \\frac{1}{B(k,n-k+1)}&=\\frac{\\Gamma(n+1)}{\\Gamma(n-k+1)\\Gamma(k)}=\\frac{n!}{(n-k)!(k-1)!}=k\\binom{n}{k}.\n\\end{align*}\\]\n\nPor integração por partes, considere \\(w=(1-u)^{n-k}\\) e \\(dv=u^{k-1}\\,du\\), então\n\n\\[\\begin{align*}\n  k\\binom{n}{k}\\int_0^pu^{k-1}(1-u)^{n-k}\\,du&=k\\binom{n}{k}\\left\\{\\frac1ku^k(1-u)^{n-k}\\Biggr|_0^p+\\frac{n-k}{k}\\int_0^pu^k(1-u)^{n-k-1}\\,du\\right\\}\\\\\n  &=\\binom{n}{k}p^k(1-p)^{n-k}+(n-k)\\binom{n}{k}\\int_0^pu^k(1-u)^{n-k-1}\\,du\\\\\n  &=\\binom{n}{k}p^k(1-p)^{n-k}+\\frac{n!}{(n-k-1)!k!}\\int_0^pu^k(1-u)^{n-k-1}\\,du\\\\\n  &=\\binom{n}{k}p^k(1-p)^{n-k}+(k+1)\\binom{n}{k+1}\\int_0^pu^k(1-u)^{n-k-1}\\,du\\\\\n  &=\\binom{n}{k}p^k(1-p)^{n-k}+\\binom{n}{k+1}p^{k+1}(1-p)^{n-k}\\,du+(k+2)\\binom{n}{k+2}\\int_0^pu^{k+1}(1-u)^{n-k-2}\\,du\\\\\n  &\\,\\,\\,\\vdots\\\\\n  &=\\binom{n}{k}p^k(1-p)^{n-k}+\\binom{n}{k+1}p^{k+1}(1-p)^{n-k}\\,du+\\cdots+p^n\\\\\n  &=\\sum_{j=k}^n\\binom{n}{j}p^j(1-p)^{n-j}. \\quad \\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "II. Variáveis aleatórias",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Função de distribuição</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap03.html#testando",
    "href": "amostral/p3chap03.html#testando",
    "title": "9  Estatísticas de ordem",
    "section": "9.1 testando",
    "text": "9.1 testando"
  },
  {
    "objectID": "pontual/p4chap01.html",
    "href": "pontual/p4chap01.html",
    "title": "10  Estimação pontual",
    "section": "",
    "text": "10.1 Propriedades dos estimadores",
    "crumbs": [
      "IV. Estimação",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "variaveis/p2chap02.html",
    "href": "variaveis/p2chap02.html",
    "title": "5  Estimação pontual",
    "section": "",
    "text": "5.1 Propriedades dos estimadores",
    "crumbs": [
      "II. Variáveis aleatórias",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "variaveis/p2chap03.html",
    "href": "variaveis/p2chap03.html",
    "title": "6  Estimação pontual",
    "section": "",
    "text": "6.1 Propriedades dos estimadores",
    "crumbs": [
      "II. Variáveis aleatórias",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "amostral/p3chap02.html",
    "href": "amostral/p3chap02.html",
    "title": "8  Estatísticas e momentos amostrais",
    "section": "",
    "text": "8.1 Momentos amostrais",
    "crumbs": [
      "III. Amostragem e distribuições amostrais",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estatísticas e momentos amostrais</span>"
    ]
  },
  {
    "objectID": "prob/p1chap02.html",
    "href": "prob/p1chap02.html",
    "title": "2  Estimação pontual",
    "section": "",
    "text": "2.1 Propriedades dos estimadores",
    "crumbs": [
      "I. Noções de probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "pontual/p4chap01.html#sec-met_momentos",
    "href": "pontual/p4chap01.html#sec-met_momentos",
    "title": "10  Estimação pontual",
    "section": "10.2 Métodos dos momentos",
    "text": "10.2 Métodos dos momentos\nSeja \\(X_1, X_2, \\ldots, X_n\\) uma amostra aleatória de uma população com distribuição que envolve o parâmetro \\(\\theta=(\\theta_1, \\theta_2,\\ldots, \\theta_k)\\). Seja \\(M_j'=\\frac1n\\sum_{i=1}^nX_i^j\\) o \\(j\\)-ésimo momento amostral em torno de zero. O método dos momentos consiste em determinar as estatísticas que convergem em probabilidade para cada componente \\(\\theta_j\\), com \\(j=1,2,\\ldots, k\\), do parâmetro \\(\\theta=(\\theta_1, \\theta_2,\\ldots, \\theta_k)\\), a partir do sistema de equações:\n\n\\[\\begin{align*}\nM_1' &\\overset{p}{\\longrightarrow} \\mu_1'\\\\\nM_2' &\\overset{p}{\\longrightarrow} \\mu_2'\\\\\n&\\hspace{0.5cm}\\vdots \\\\\nM_k' &\\overset{p}{\\longrightarrow} \\mu_k'.\n\\end{align*}\\]\n\nO método dos momentos baseia-se na lei dos grandes números e pode-se formalizar com o seguinte resultado.\n\n\nTeorema 10.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetros \\(\\theta\\). Se \\(\\mu_{2r}=\\mathbb{E}X^{2r}\\) existe, então\n\\[\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^r \\overset{p}{\\longrightarrow} \\mu_r.\n\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSabemos que, \\(\\mu_r=\\mathbb{E}\\left[X-\\mu\\right]^r\\), onde \\(\\mu=\\mathbb{E}X\\), daí\n\n\\[\\begin{align*}\n\\mu_r &= \\mathbb{E}\\left[X-\\mu\\right]^r = \\mathbb{E}\\left(\\sum_{j=0}^r\\binom{r}{j}X^j(-\\mu)^{r-j}\\right)\n=\\sum_{j=0}^r\\binom{r}{j}(-\\mu)^{r-j}\\mathbb{E}X^j=\\sum_{j=0}^r\\binom{r}{j}\\mu_j'(-\\mu)^{r-j}.\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n\\frac1n\\sum_{j=1}^n\\left(X_i-\\bar{X}_n\\right)^r &= \\frac1n\\sum_{i=1}^n\\sum_{j=0}^r\\binom{r}{j}X_i^j(-\\bar{X}_n)^{r-j}\n=\\sum_{j=0}^r\\left[\\frac1n\\sum_{i=1}^n\\binom{r}{j}X_i^j(-\\bar{X}_n)^{r-j}\\right]\\\\\n&=\\sum_{j=0}^r\\binom{r}{j}(-\\bar{X}_n)^{r-j}\\frac1n\\sum_{i=1}^nX_i^j=\\sum_{j=0}^r\\binom{r}{j}(-\\bar{X}_n)^{r-j}M_j'.\n\\end{align*}\\]\n\nAssumindo que o momento \\(\\mu_{2r}\\) existe, então os momentos \\(\\mu_s\\) e \\(\\mu_s'\\) existem para \\(s\\le 2r\\). Daí,\n\\[M_r=\\sum_{j=0}^r\\binom{r}{j}M_j'(-\\bar{X}_n)^{r-j} \\overset{p}{\\longrightarrow} \\sum_{j=0}^r\\binom{r}{j}\\mu_j'(-\\mu)^{r-j}=\\mu_r.\n\\]\n\nLembre que, \\(\\mu_r'=\\mathbb{E}[X^r]\\) e \\(\\mu_r=\\mathbb{E}[(X-\\mu_X)^r]\\). Os momentos amostrais foram definidos na Seção 8.1.\n\n\n\n\n\nExemplo 10.7 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Encontre os estimadores para \\(\\mu\\) e \\(\\sigma^2\\) pelo método dos momentos.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo método dos momentos, temos que\n\n\\[\\begin{align*}\n\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i &\\overset{p}{\\longrightarrow} \\mu;\\\\\n\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2 &\\overset{p}{\\longrightarrow} \\sigma^2.\n\\end{align*}\\]\n\n\n\n\n\nExemplo 10.8 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com função de densidade dada por \\[f(x;\\theta)=\\frac{\\theta_2^{\\theta_1}}{\\Gamma(\\theta_1)}x^{\\theta_1-1}e^{-\\theta_2 x}\\mathbb{I}_{(0,\\infty)}(x).\\] Encontre os estimadores para \\(\\theta_1\\) e \\(\\theta_2\\) pelo método dos momentos.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA função de densidade de probabilidade acima representa a densidade gamma com parâmetros \\(\\theta_1\\) e \\(\\theta_2\\). Dessa forma, se \\(X\\) é uma variável aleatória de uma população com distribuição gamma\\((\\theta_1, \\theta_2)\\), então \\(\\mathbb{E}X=\\frac{\\theta_1}{\\theta_2}\\) e \\(\\mathrm{var}[X]=\\frac{\\theta_1}{\\theta_2^2}\\). Dessa forma, pelo método dos momentos, temos que\n\n\\[\\begin{align*}\n\\bar{X}_n &\\overset{p}{\\longrightarrow} \\frac{\\theta_1}{\\theta_2};\\\\\n\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2 &\\overset{p}{\\longrightarrow} \\frac{\\theta_1}{\\theta_2^2}.\n\\end{align*}\\]\n\nDaí, \\(\\dfrac{\\dfrac{\\bar{X}_n}{\\theta_2}}{\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2 } \\overset{p}{\\longrightarrow} 1\\) ou \\(\\dfrac{\\bar{X}_n}{\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2 } \\overset{p}{\\longrightarrow} \\theta_2.  \\quad\\checkmark\\)\nPor outra parte, \\(\\dfrac{1}{\\bar{X}_n} \\overset{p}{\\longrightarrow} \\dfrac{\\theta_2}{\\theta_1}\\) ou \\(\\left(\\dfrac{\\theta_1}{\\bar{X}_n}\\right)^2 \\overset{p}{\\longrightarrow} \\theta_2^2\\), então\n\n\\[\\begin{align*}\n\\dfrac{\\dfrac{\\theta_1}{\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2}}{\\left(\\dfrac{\\theta_1}{\\bar{X}_n}\\right)^2}\n= \\dfrac{\\bar{X}_n^2}{\\theta_1\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2} &\\overset{p}{\\longrightarrow} 1,\n\\end{align*}\\]\n\ndaí, \\(\\dfrac{\\bar{X}_n^2}{\\frac1n\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2} \\overset{p}{\\longrightarrow} \\theta_1. \\quad\\checkmark\\)\n\n\n\n\nExemplo 10.9 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com distribuição uniforme no intervalo \\((-\\theta,\\theta)\\). Encontre um estimador para \\(\\theta\\) pelo método dos momentos.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSendo \\(X\\) uma variável aleatória com distribuição uniforme em \\((-\\theta, \\theta)\\), temos que, \\(\\mathbb{E}X=\\mu=0\\) e \\(\\mathrm{var}[X]=\\dfrac{4\\theta^2}{12}=\\dfrac13\\theta^2\\). Dessa forma, \\(\\bar{X}_n \\overset{p}{\\longrightarrow} \\mu=0\\), o que significa que o estimador \\(\\bar{X}_n\\) não tem informação sobre \\(\\theta\\). Nesse caso, será necessário verificar o segundo momento:\n\\[\\frac1n\\sum_{i=1}^nX_i^2 \\overset{p}{\\longrightarrow} \\frac13\\theta^2.\\] Daí, o estimador de momentos para \\(\\theta\\) é dado por\n\\[\\sqrt{\\frac3n\\sum_{i=1}^nX_i^2} \\overset{p}{\\longrightarrow} \\theta. \\quad\\checkmark\\]\n\n\n\n\nExemplo 10.10 Seja \\(X_1,X_2,\\ldots, X_n\\) uma amostra aleatória de uma população com função de densidade de probabilidade \\(f(x;\\theta)=\\theta e^{-\\theta x}\\mathbb{I}_{(0,\\infty)}(x)\\), com \\(\\theta&gt;0\\). Determine um estimador para a mediana da distribuição da amostra pelo método dos momentos.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPrimeiramente, será necessário encontrar o formato analítico da mediana. Para uma variável aleatória \\(X\\), de uma população com função de densidade de probabilidade \\(f(x;\\theta)=\\theta e^{-\\theta x}\\mathbb{I}_{(0,\\infty)}(x)\\), com \\(\\theta&gt;0\\), a função de distribuição acumulada é dada por:\n\n\\[\\begin{align*}\n\\mathbb{P}[X\\le x]&=\\int_0^x\\theta e^{-\\theta y}\\,dy=\\left.e^{-\\theta y}\\right|_x^0\n=1-e^{-\\theta x}.\n\\end{align*}\\]\n\nDaí, a mediana é calculada como \\(1-e^{-\\theta x_{0.5}}=\\frac12\\) ou \\(x_{0.5}=\\dfrac{\\log2}{\\theta}\\).\nPor outro lado, sabemos que \\(\\mathbb{E}X=\\dfrac1\\theta\\), então\n\\[\\bar{X}_n \\log2 \\overset{p}{\\longrightarrow} \\frac{\\log2}{\\theta}. \\quad\\checkmark\\]",
    "crumbs": [
      "IV. Estimação",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "pontual/p4chap01.html#sec-met_mv",
    "href": "pontual/p4chap01.html#sec-met_mv",
    "title": "10  Estimação pontual",
    "section": "10.3 Métodos de máxima verossimilhança",
    "text": "10.3 Métodos de máxima verossimilhança",
    "crumbs": [
      "IV. Estimação",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Estimação pontual</span>"
    ]
  },
  {
    "objectID": "p1chap01.html",
    "href": "p1chap01.html",
    "title": "1  Estrutura básica",
    "section": "",
    "text": "1.1 Opções para renderização\nInfelizmente, algumas funcionalidades do manim estão em constante mudança e podem ser alteradas conforme a atualização das versões. A seguir, são apresentadas algumas opções mais estáveis, disponíveis na versão 0.18.1:\nPara mais opções, consulte a Lista de flags de manim ou digite no terminal o comando manim render --help.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estrutura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-momentos_amostrais",
    "href": "p1chap01.html#sec-momentos_amostrais",
    "title": "1  Estructura básica",
    "section": "1.2 Momentos amostrais",
    "text": "1.2 Momentos amostrais\n\nDefinição 1.2 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 1.3 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 1.3 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a Definição 1.1, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\nExemplo 1.4 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.5 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 1.6 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 1.5, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 1.5.\n\n\n\n\n\nExemplo 1.7 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 1.1, sabemos da Definição 1.2 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 1.4 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 1.8 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 1.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 1.4. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-fgm",
    "href": "p1chap01.html#sec-fgm",
    "title": "1  Estructura básica",
    "section": "1.4 Funções geradoras de momentos",
    "text": "1.4 Funções geradoras de momentos\n\nDefinição 1.2 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h&lt;t&lt;h\\), para algum \\(h&gt;0\\).\n\n\n\nTeorema 1.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide Apêndice A), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 1.2, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 1.2, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha&gt;0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t&gt;\\beta\\), a quantidade \\(-(\\beta-t)&gt;0\\), então \\(e^{-(\\beta-t)x}&gt;x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t&lt;\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 1.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.9 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com densidade conjunta dada por \\[f(x; a)=a^2e^{-ax_2}, \\quad 0&lt;x_1&lt;x_2&lt;\\infty, \\quad a&gt;0.\\] Encontre \\(\\mathbb{E}X^r_1\\), \\(\\mathbb{E}X^r_2\\), para \\(r=1,2,3,\\ldots\\), e \\(\\mathrm{Corr}[X_1,X_2]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nTemos que,\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_1&=\\int_0^\\infty\\int_0^{x_2}x_1^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\int_0^{x_2}x_1^r\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2e^{-ax_2}\\frac{x_1^{r+1}}{r+1}\\Biggr|_{0}^{x_2}\\,dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\frac{x_2^{r+1}}{r+1}\\,dx_2\\\\\n  &=\\frac{\\Gamma(r+1)}{a^r}=\\frac{r!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_2&=\\int_0^\\infty\\int_0^{x_2}x_2^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2^re^{-ax_2}\\int_0^{x_2}\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2x_2^{r+1}e^{-ax_2}\\,dx_2=\\frac{\\Gamma(r+2)}{a^r}=\\frac{(r+1)!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nO momento cruzado é calculado como\n\n\\[\\begin{align*}\n  \\mathbb{E}X_1X_2&=\\int_0^\\infty\\int_0^{x_2}x_1x_2a^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2e^{-ax_2}\\int_0^{x_2}x_1\\,dx_1dx_2\\\\\n  &=\\frac{a^2}{2}\\int_0^\\infty x_2^3e^{-ax_2}dx_2\n  =\\frac{\\Gamma(4)}{2a^2}\\int_0^\\infty \\frac{a^4}{\\Gamma(4)}x_2^3e^{-ax_2}dx_2=\\frac{3}{a^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, a correlação é dada por\n\n\\[\\begin{align*}\n  \\mathrm{Corr}[X_1,X_2]&=\\frac{\\mathbb{E}X_1X_2-\\mathbb{E}X_1\\mathbb{E}X_2}{\\sqrt{\\mathrm{Var}[X_1]\\mathrm{Var}[X_2]}}\n  =\\frac{\\frac{3}{a^2}-\\frac1a\\frac2a}{\\sqrt{\\frac{1}{a^2}\\frac{2}{a^2}}}=\\frac{1}{\\sqrt{2}}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t&lt;1\\) ou \\(t&lt;-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 1.11 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha&gt;0\\) e \\(\\beta&gt;0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 1.2, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta&gt;r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta&gt;2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|&lt;\\frac{h}{|b|}\\), \\(h&gt;0\\).\n\n\n\nExemplo 1.12 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b&gt;a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 1.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h&gt;0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|&lt;h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 1.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988) [pp. 261], Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 1.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|&lt;h\\), , para algum \\(h&gt;0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 1.2 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 1.13 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 1.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.14 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 1.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 1.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t&lt;\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 1.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 1.8, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t&lt;\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 1.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 1.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nExemplo 1.15 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n&gt;1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 1.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n\nTeorema 1.7 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\), então\n\n\n\\(\\bar{X}_n\\) segue uma distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\);\n\n\n\\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\);\n\n\n\\(\\bar{X}_n\\) e \\(S^2_n\\) são variáveis aleatórias independentes.\n\n\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\n\n\nVide Teorema 1.1 e Exemplo 1.13;\n\n\nVide Exemplo 1.15;\n\n\nNote que\n\n\\[\\begin{align*}\nS^2_n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\left[(X_1-\\bar{X}_n)+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=1}^n(X_i-\\bar{X}_n)-(X_1-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=2}^n(X_i-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\n\\end{align*}\\]\n\nDaí, \\(S^2_n\\) pode ser escrito em função de \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n, \\ldots, X_n-\\bar{X}_n\\). Dessa forma, se as variáveis aleatórias \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) são independentes de \\(\\bar{X}_n\\), obtem-se o resultado requerido.\nPara isso, calcula-se a densidade conjunta das variáveis aleatórias \\(\\bar{X}_n\\), \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) para verificar se a mesma pode ser escrita como produto das densidades de \\(\\bar{X}_n\\) e \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\). Dessa maneira, para usar o método do jacobiano, considere \\(y_1=\\bar{x}_n\\) e \\(y_i=x_i-\\bar{x}_n\\) para \\(i=2,3,\\ldots,n\\). Então,\n\n\\[\\begin{align*}\n    x_1&=y_1-\\sum_{i=2}^ny_i; & x_i&=y_i+y_1, \\quad i=2,3,4,\\ldots.\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n    \\frac{\\partial x_1}{\\partial y_1}&=1; & \\frac{\\partial x_2}{\\partial y_1}&=1; & \\frac{\\partial x_3}{\\partial y_1}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_1}&=1;\\\\\n    \\frac{\\partial x_1}{\\partial y_2}&=-1; & \\frac{\\partial x_2}{\\partial y_2}&=1; & \\frac{\\partial x_3}{\\partial y_2}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_2}&=0;\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=0;\\\\\n    &\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots & & &&\\,\\,\\,\\vdots\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=1.\n\\end{align*}\\]\n\nAssim sendo,\n\n\\[\\begin{align*}\n    |J|&=\\begin{vmatrix}\n     1 & 1 & 1 & \\cdots & 1\\\\\n    -1 & 1 & 0 & \\cdots & 0\\\\\n    -1 & 0 & 1 & \\cdots & 0\\\\\n    \\vdots &  &  & \\ddots & \\vdots \\\\\n    -1 & 0 & 0 & \\cdots & 1\\\\\n    \\end{vmatrix}=n.\n\\end{align*}\\]\n\nPor otro lado, sabemos que a função de densidade conjunta de \\(X_1,X_2,\\ldots,X_n\\) é dada por\n\\[f(x_1,x_2,\\ldots,x_n)=(2\\pi)^{-n/2}e^{-\\frac12\\sum_{i=1}^nx_i}, \\quad x_i\\in\\mathbb{R}.\\] Portanto,\n\n\\[\\begin{align*}\n    f(y_1,y_2,\\ldots,y_n)&=\\frac{n}{(2\\pi)^{-n/2}}e^{-\\frac12(y_1-\\sum_{i=2}^ny_i)^2}e^{-\\frac12\\sum_{i=2}^n(y_i+y_1)^2}\\\\\n    &=\\frac{1}{(2\\pi n^{-1})^{\\frac12}}e^{-\\frac12ny_1^2}\\frac{1}{(2\\pi )^{\\frac{n-1}{2}}n^{-1/2}}e^{\\left[-\\frac12\\sum_{i=2}^ny_i^2+(\\sum_{i=2}^ny_i)^2\\right]}.\n\\end{align*}\\]\n\nEntão, \\(Y_1\\) é independente de \\(Y_2,Y_3,\\ldots,Y_n. \\quad \\blacksquare\\)\n\n\n\n\n\n\n1.4.1 Momentos fatoriais\n\nDefinição 1.3 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 1.16 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.17 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|&lt;\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da Seção A.2 do Apêndice A.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 1.18 Considere o cenário do Exemplo 1.16. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 1.16, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n1.4.2 Cumulantes\n\nDefinição 1.4 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|&lt;h\\), \\(h&gt;0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 1.19 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 1.4, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.20 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 1.3, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 1.16, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r&gt;1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 1.2, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|&lt;h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#função-geradora-de-momentos-conjunta",
    "href": "p1chap01.html#função-geradora-de-momentos-conjunta",
    "title": "1  Estructura básica",
    "section": "1.5 Função geradora de momentos conjunta",
    "text": "1.5 Função geradora de momentos conjunta\n\nDefinição 1.5 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i&gt;0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 1.21 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1&gt;0, x_2&gt;0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 1.5, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.22 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*&lt;\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12.\n\\end{align*}\\]\n\nPelo @-fgm_unicidade, temos que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\).\n\n\n\n\nExemplo 1.23 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Calcule a FGMC do vetor \\((Y,Z)\\), onde \\(Y=X_1+X_2\\) e \\(Z=X_1^2+X_2^2\\). As variáveis \\(Y\\) e \\(Z\\) são correlacionadas?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[e^{t_1Y+t_2Z}\\right]=\\mathbb{E}\\left[e^{t_1(X_1+X_2)+t_2\\left(X_1^2+X_2^2\\right)}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_1X_2+t_2X_1^2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right].\n\\end{align*}\\]\n\nPor outra parte, temos que\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]&=\\int_{-\\infty}^\\infty e^{t_1x_1+t_2x_1^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12x_1^2}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{t_1x_1+\\left(t_2-\\frac12\\right)x_1^2\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left(x_1^2-\\frac{2t_1}{1-2t_2}x_1\\right)\\right\\}\\,dx_1, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right)\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left[\\left(x_1-\\frac{t_1}{1-2t_2}\\right)^2-\\frac{t_1^2}{(1-2t_2)^2}\\right]\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}. \\quad \\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\left(\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}\\right)^2\\\\\n      &=\\frac{1}{1-2t_2}\\exp\\left\\{\\frac{t_1^2}{1-2t_2}\\right\\}, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right). \\quad\\checkmark\n\\end{align*}\\]\n\nObserve que, \\(M(t_1,0)=M(t_1)=e^{t_1^2}\\), para \\(t_1\\in\\mathbb{R}\\), i.e. \\(Y\\) é uma variável aleatória de uma população com distribuição normal com média \\(0\\) e variância \\(2\\). Por otro lado, \\(M(,t_2)=M(t_2)=\\frac{1}{1-2t_2}\\), para \\(-\\infty&lt;t_2&lt;\\frac12\\), i.e., \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade.\nPara verificar se \\(Y\\) e \\(Z\\) são correlacionadas, calcula-se a correlação entre as duas variáveis aleatórias. Assim,\n\n\\[\\begin{align*}\n      \\mathbb{E}YZ=\\frac{\\partial^2}{\\partial t_1\\partial t_2}M(t_1,t_2)\\Biggr|_{(t_1,t_2)=(0,0)}\n      &=\\frac{\\partial}{\\partial t_1}\\left[\\frac{\\partial}{\\partial t_2}M(t_1,t_2)\\right]\\Biggr|_{(t_1,t_2)=(0,0)}\\\\       &=\\frac{4t_1\\exp\\left\\{\\frac{t^2_1}{1-2t_2}\\right\\}(-4t_2+t_1^2+2)}{(1-2t_2)^4}\\Biggr|_{(t_1,t_2)=(0,0)}\\\\\n      &=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, \\(\\mathrm{Cov}[Y,Z]=\\mathbb{E}YZ-\\mathbb{E}Y\\mathbb{E}Z=0\\). Daí, as variáveis \\(Y\\) e \\(Z\\) são não-correlacionadas.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#exercícios-sugeridos",
    "href": "p1chap01.html#exercícios-sugeridos",
    "title": "1  Estrutura básica",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nO Yin e Yang é um conceito criado pela sábia filosofia chinesa e representa o equilíbrio dinâmico da flutuação cíclica incessante que sustenta o ritmo fundamental do universo. Use os conhecimentos adquiridos para desenhar o simbolo Yin-Yang em preto e branco;\nO espelho de Vênus e o escudo de Marte são representações icônicas dos gêneros feminino e masculino, respectivamente. Desenhe os simbolos, um ao lado do outro, em cores rosa e azul, respectivamente;\nO símbolo que caracteriza internacionalmente o movimento de “paz e amor” foi criado pelo artista britânico Gerald Herbert Holtom (1914-1985), exclusivamente para a chamada “Campanha do Desarmamento” (Campaign for Nuclear Disarmament – CND), em 1958. O símbolo é formado pelas letras N e D (Nuclear Disarmament, em inglês). Desenhe o simbolo em cor preta, modificando a cor de fundo da cena para branco.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estrutura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-rederizar",
    "href": "p1chap01.html#sec-rederizar",
    "title": "1  Estrutura básica",
    "section": "",
    "text": "-p (Preview): Abre automaticamente seu arquivo ao final da renderização, seja um vídeo ou uma imagem;\n-q (Quality): Para modificar a qualidade do vídeo pode usar o flag -q, com as seguintes opções:\n\nl: Resolução baixa, i.e., 854 x 480 a 15 FPS;\nm: Resolução média, i.e., 1280 x 720 a 30FPS;\nh: Resolução alta, i.e., 1920 x 1080 a 60FPS. Essa é a opção padrão no processo de renderização;\nk: Resolução em 4k, i.e., 3840 x 2160 a 60FPS.\n\nEssas opções devem ser indicadas após o flag q. Por exemplo, ao executar manim -pql script01.py MinhaCena, o resultado será um vídeo no formato MP4 com baixa resolução e abrirá automaticamente para visualização.\n-s: Renderiza e salva apenas o último quadro de uma cena como uma imagem PNG;\n-o: Para especificar o nome do arquivo das cenas renderizadas. Por exemplo: manim -pql script01.py MinhaCena -o ArquivoCenas;\n-a: Renderiza todas essas cenas do script;\n-n START END: Funciona apenas na renderização de vídeo. Inicia a renderização da animação START até a animação END, onde START e END são os números de algumas animações. Se END não for especificado, renderiza todas as cenas após START;\n-t: O vídeo rederizado será exportado no formato .mov em um canal alfa, caso você renderize uma imagem ele exportará um PNG com fundo transparente;\n--format [png|gif|mp4|webm|mov]: Modifica o formato do arquivo. O formato .mov é o de melhor qualidade. Por exemplo: manim -pql script01.py MinhaCena --format=gif;\n--fps=SOME_NUMBER: Permite modificar a taxa de quadros no processo de renderização. Por exemplo: manim -pql script01.py MinhaCena --fps=15 --format=gif.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estrutura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-mobjects",
    "href": "p1chap01.html#sec-mobjects",
    "title": "1  Estructura básica",
    "section": "1.2 Mobjects",
    "text": "1.2 Mobjects\n\nDefinição 1.1 (Momento amostral em torno de zero) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno de zero, define-se como \\[M_r'=\\frac1n\\sum_{i=1}^nX_i^r.\\]\n\n\nDefinição 1.2 (Momento amostral em torno da média amostral) Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O \\(r\\)-ésimo momento amostral em torno da média amostral, define-se como \\[M_r=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^r.\\]\n\n\nExemplo 1.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a ?def-estatistica, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\nExemplo 1.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 1.5 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 1.4, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 1.5.\n\n\n\n\n\nExemplo 1.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 1.1, sabemos da Definição 1.1 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 1.3 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 1.7 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 1.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 1.3. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-objetos",
    "href": "p1chap01.html#sec-objetos",
    "title": "1  Estrutura básica",
    "section": "1.2 Objetos em manim",
    "text": "1.2 Objetos em manim\nOs objetos que podem ser exibidos na tela são chamados de Mathematical objects ou, simplesmente Mobjects, e existem vários tipos. Alguns desses tipos são apresentados a seguir:\n\nmobject: Esta é a classe que gera todos os objetos que podem ser exibidos;\nImageMobject: São imagens em bitmaps, como PNGs, JPGs, etc. Nessa versão, o manim não possui suporte para GIFs;\nSVGMobject: São imagens em SVG (Scalable Vector Graphics), i.e., em formato de arquivo vetorial compatível com a web;\nVMobject: São Mobject vetorizados, em geral, curvas de Bézier. Os Mobject vetorizados são mais flexíveis, já que você mesmo pode definir seus próprios VMobjects ou usar os disponíveis por padrão.\n\nAo longo deste tutorial, serão apresentados e utilizados diversos tipos de Mobjects em diferentes exemplos práticos. Cada um desses objetos desempenha um papel importante na criação de animações e gráficos dinâmicos, sendo uma ferramenta fundamental para aqueles que desejam criar conteúdos visuais de maneira eficiente e flexível. Durante o processo, você terá a oportunidade de explorar as funcionalidades e características de cada tipo de Mobject, familiarizando-se com as diferentes opções disponíveis para a construção de sua próxima animação.\nOs Mobjects são componentes visuais utilizados para compor as animações e vídeos interativos. Eles podem ser formas geométricas, textos, gráficos ou qualquer outro elemento visual que se deseje animar. A versatilidade desses objetos permite uma vasta gama de possibilidades na criação de conteúdo audiovisual. Por meio deste tutorial, iremos demonstrar como utilizar esses objetos em diversas situações, oferecendo uma visão prática e clara de como aplicá-los para alcançar resultados profissionais.\nEste tutorial não apenas oferecerá uma introdução básica ao uso de Mobjects, mas também fornecerá dicas e truques para otimizar suas animações, aumentando a qualidade e a fluidez do resultado final. Ao final, você estará capacitado a aplicar os conhecimentos adquiridos para criar animações personalizadas e impactantes, utilizando as diversas opções que os Mobjects oferecem.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estrutura básica</span>"
    ]
  },
  {
    "objectID": "p1chap02.html",
    "href": "p1chap02.html",
    "title": "2  Objetos em manim",
    "section": "",
    "text": "2.1 Tipo geometry\nNo tipo geometry encontram-se vários Mobjetos geométricos úteis para criar as nossas animações. Temos por exemplo:",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  },
  {
    "objectID": "p1chap02.html#sec-fgm",
    "href": "p1chap02.html#sec-fgm",
    "title": "2  Objetos em manim",
    "section": "2.2 Funções geradoras de momentos",
    "text": "2.2 Funções geradoras de momentos\n\nDefinição 2.2 (Função geradora de momentos) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos (FGM) de \\(F_X\\), define-se como \\[M_X(t)=\\mathbb{E}\\left[e^{tX}\\right],\\] se existe para todo \\(t\\), tal que \\(-h&lt;t&lt;h\\), para algum \\(h&gt;0\\).\n\n\n\nTeorema 2.3 Seja \\(M_X(t)\\) a FGM de uma distribuição \\(F_X\\). Se existe \\(M_X(t)\\) para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\), então \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existe e\n\\[\\mathbb{E}X^r=M_X^{(r)}(0)=\\frac{\\partial^r}{\\partial t^r}M_X(t)\\Biggr|_{t=0}.\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPela expansão em série de Taylor da função exponencial (vide Apêndice A), temos que \\[e^x=1+x+\\frac{1}{2!}x^2+\\frac{1}{3!}x^3+\\frac{1}{4!}x^4+\\cdots.\\]\nPor outra parte, pela Definição 2.2, temos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), então\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\mathbb{E}\\left[\\sum_{j=0}^\\infty\\frac{1}{j!}(tX)^j\\right]\n  =\\sum_{j=0}^\\infty\\frac{t^j}{j!}\\mathbb{E}\\left[X^j\\right].\n\\end{align*}\\]\n\nPor hipóteses, \\(\\mathbb{E}X^r\\), \\(r=1,2,\\ldots\\), existem para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\).\nDiferenciando em ambos os lados da igualdade e avaliando em \\(t=0\\), temos que\n\n\\[\\begin{align*}\n  M'_X(0)&=\\left(0+\\mathbb{E}X+\\mathbb{E}X^2t+\\cdots+\\mathbb{E}\\left[X^r\\right]\\frac{t^{r-1}}{(r-1)!}\\right)\\Biggr|_{t=0}=\\mathbb{E}X. \\quad\\checkmark\n\\end{align*}\\]\n\nEm geral,\n\n\\[\\begin{align*}\n  M_X^{(r)}(0)&=\\frac{\\partial^r}{\\partial t}M_x(t)\\Biggr|_{t=0}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial t}\\frac{\\partial^{r-1}}{\\partial t^{r-1}}e^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}\\left[X^re^{tX}\\right]\\Biggr|_{t=0}=\\mathbb{E}X^r. \\quad\\blacksquare\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.8 Seja \\(X\\) uma variável aleatória de uma população com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\), i.e. \\[f(x; \\alpha, \\beta)=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\mathbb{I}_{(0,\\infty)}(x).\\] Utilize a FGM para calcular a variância de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela definição Definição 2.2, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\), para todo \\(t\\), tal que \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Dessa forma,\n\n\\[\\begin{align*}\n  M_X(t)&=\\int_0^\\infty e^{tx}\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx\n  =\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx.\n\\end{align*}\\]\n\nPara avaliar a existência da integral, consideremos os seguintes casos:\n\nSe \\(t=\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lim_{a\\to\\infty}\\int_0^a x^{\\alpha-1}\\,dx=\\frac{\\beta^\\alpha}{\\alpha\\Gamma(\\alpha)}\\lim_{a\\to\\infty} a^{\\alpha}.\n\\end{align*}\\]\n\nPara \\(\\alpha&gt;0\\), \\(\\lim_{a\\to\\infty} a^{\\alpha}\\) é infinito. Dessa forma, \\(M_X(t)\\) não está definida.\n\nSe \\(t&gt;\\beta\\), a quantidade \\(-(\\beta-t)&gt;0\\), então \\(e^{-(\\beta-t)x}&gt;x^{\\alpha-1}\\). Assim, a integral é divergente e, por conseguinte, a FGM não existe.\nSe \\(t&lt;\\beta\\),\n\n\n\\[\\begin{align*}\n  M_X(t)&=\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\int_0^\\infty x^{\\alpha-1}e^{-(\\beta-t)x}\\,dx\n  =\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\int_0^\\infty u^{\\alpha-1}e^{-u}\\,du, \\quad u=(\\beta-t)x\\\\\n  &=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\beta-t)^\\alpha}\\Gamma(\\alpha)\n  =\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}=\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}. \\quad\\checkmark\n\\end{align*}\\]\n\nPara calcular a variância, utilizamos o resultado do Teorema 2.3. Daí\n\n\\[\\begin{align*}\n  M'_X(0)&=\\frac{\\alpha}{\\beta}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha-1}\\Biggr|_{t=0}=\\frac{\\alpha}{\\beta}\\\\\n  M''_X(0)&=\\frac{\\alpha(\\alpha+1)}{(\\beta-x)^2}\\left(1-\\frac{t}{\\beta}\\right)^{-\\alpha}\\Biggr|_{t=0}\n  =\\frac{\\alpha(\\alpha+1)}{\\beta^2}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathrm{Var}[X]&=\\mathbb{E}X^2-\\left(\\mathbb{E}X\\right)^2=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\left(\\frac{\\alpha}{\\beta}\\right)^2=\\frac{\\alpha}{\\beta^2}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.9 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com densidade conjunta dada por \\[f(x; a)=a^2e^{-ax_2}, \\quad 0&lt;x_1&lt;x_2&lt;\\infty, \\quad a&gt;0.\\] Encontre \\(\\mathbb{E}X^r_1\\), \\(\\mathbb{E}X^r_2\\), para \\(r=1,2,3,\\ldots\\), e \\(\\mathrm{Corr}[X_1,X_2]\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nTemos que,\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_1&=\\int_0^\\infty\\int_0^{x_2}x_1^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\int_0^{x_2}x_1^r\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2e^{-ax_2}\\frac{x_1^{r+1}}{r+1}\\Biggr|_{0}^{x_2}\\,dx_2\n  =\\int_0^\\infty a^2e^{-ax_2}\\frac{x_2^{r+1}}{r+1}\\,dx_2\\\\\n  &=\\frac{\\Gamma(r+1)}{a^r}=\\frac{r!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r_2&=\\int_0^\\infty\\int_0^{x_2}x_2^ra^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2^re^{-ax_2}\\int_0^{x_2}\\,dx_1dx_2\\\\\n  &=\\int_0^\\infty a^2x_2^{r+1}e^{-ax_2}\\,dx_2=\\frac{\\Gamma(r+2)}{a^r}=\\frac{(r+1)!}{a^r}, \\quad r=1,2,\\ldots. \\quad\\checkmark\n\\end{align*}\\]\n\nO momento cruzado é calculado como\n\n\\[\\begin{align*}\n  \\mathbb{E}X_1X_2&=\\int_0^\\infty\\int_0^{x_2}x_1x_2a^2e^{-ax_2}\\,dx_1dx_2\n  =\\int_0^\\infty a^2x_2e^{-ax_2}\\int_0^{x_2}x_1\\,dx_1dx_2\\\\\n  &=\\frac{a^2}{2}\\int_0^\\infty x_2^3e^{-ax_2}dx_2\n  =\\frac{\\Gamma(4)}{2a^2}\\int_0^\\infty \\frac{a^4}{\\Gamma(4)}x_2^3e^{-ax_2}dx_2=\\frac{3}{a^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, a correlação é dada por\n\n\\[\\begin{align*}\n  \\mathrm{Corr}[X_1,X_2]&=\\frac{\\mathbb{E}X_1X_2-\\mathbb{E}X_1\\mathbb{E}X_2}{\\sqrt{\\mathrm{Var}[X_1]\\mathrm{Var}[X_2]}}\n  =\\frac{\\frac{3}{a^2}-\\frac1a\\frac2a}{\\sqrt{\\frac{1}{a^2}\\frac{2}{a^2}}}=\\frac{1}{\\sqrt{2}}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.10 Seja \\(X\\) uma variável aleatória de uma população com distribuição geométrica com parâmetro \\(p\\), i.e.. \\(\\mathbb{P}[X=x]=p(1-p)^x\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Apresente uma forma analítica para calcula o \\(r\\)-ésimo momento da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx}p(1-p)^x=p\\sum_{x=0}^\\infty \\left(e^{t}(1-p)\\right)^x=\\frac{p}{1-(1-p)e^t}.\n\\end{align*}\\]\n\nA última soma pode ser tratada como uma série geométrica. Nesse caso, a soma converge quando \\((1-p)e^t&lt;1\\) ou \\(t&lt;-\\log(1-p)\\).\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{p(1-p)e^t}{(1-(1-p)e^t)^2}\\Biggr|_{t=0}=\\frac{1-p}{p};\\\\ \\\\\n  \\mathbb{E}\\left[X^2\\right]&=\\frac{p(1-p)e^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}\\Biggr|_{t=0}=\\frac{(1-p)(2-p)}{p^2}.\n\\end{align*}\\]\n\nA variância é dada por \\(\\mathrm{Var}[X]=\\mathbb{E}X^2-(\\mathbb{E}X)^2=\\dfrac{(1-p)(2-p)}{p^2}-\\dfrac{(1-p)^2}{p^2}=\\dfrac{1-p}{p^2}\\).\nO cálculo das derivadas de ordem superior tornam trabalhoso o uso dessa estratégia. Uma forma analítica para o \\(r\\)-ésimo momento pode ser obtida como\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X^r\\right]&=\\sum_{x=0}^\\infty x^r(1-q)q^x=\\sum_{x=0}^\\infty x^{r-1}(1-q)qxq^{x-1}=(1-q)q\\sum_{x=0}^\\infty x^{r-1}xq^{x-1}\\\\\n  &=(1-q)q\\sum_{x=0}^\\infty x^{r-1}\\frac{\\partial}{\\partial q}q^{x}=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\sum_{x=0}^\\infty x^{r-1}q^{x}\\right]=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\sum_{x=0}^\\infty x^{r-1}(1-q)q^{x}\\right]\\\\\n  &=(1-q)q\\frac{\\partial}{\\partial q}\\left[\\frac{1}{1-q}\\mathbb{E}X^{r-1}\\right],\n\\end{align*}\\]\n\nonde \\(q=1-p\\) e \\(r=1, 2, \\ldots\\). \\(\\quad\\checkmark\\)\n\n\n\n\nExemplo 2.11 Seja \\(X\\) uma variável aleatória de uma população com distribuição Pareto com parâmetros \\(\\alpha&gt;0\\) e \\(\\beta&gt;0\\), i.e. \\[f(x;\\alpha,\\beta)=\\beta\\frac{\\alpha^\\beta}{x^{\\beta+1}}\\mathbb{I}_{[\\alpha,\\infty)}(x).\\] Utilize a FGM da distribuição de \\(X\\) para calcular a sua variância.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.2, temos que\n\n\\[\\begin{align*}\n  M_X(t)&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty\\frac{e^{tx}}{x^{\\beta+1}}\\,dx.\n\\end{align*}\\]\n\nNote que, quando \\(x\\to\\infty\\), \\(\\frac{e^{tx}}{x^{\\beta+1}}\\to\\infty\\) fazendo com que a FGM não exista.\nNo entanto, pode-se observar que para alguns valores específicos de \\(\\beta\\), alguns momentos da distribuição de \\(X\\) podem existir. Por exemplo, considere-se o caso \\(\\beta&gt;r\\), daí\n\n\\[\\begin{align*}\n  \\mathbb{E}X^r&=\\beta\\alpha^\\beta\\int_\\alpha^\\infty \\frac{x^r}{x^{\\beta+1}}\\,dx\n  =\\beta\\alpha^\\beta\\int_\\alpha^\\infty x^{r-\\beta-1}\\,dx\\\\\n  &=-\\beta\\alpha^\\beta\\frac{x^{-\\beta+r}}{\\beta-r}\\Biggr|_\\alpha^\\infty=\\frac{\\beta\\alpha^r}{\\beta-r}. \\quad\\checkmark\n\\end{align*}\\]\n\nEm particular, se \\(\\beta&gt;2\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}X&=\\frac{\\beta\\alpha}{\\beta-1} & &\\mathrm{ e } &  \\mathbb{E}X^2&=\\frac{\\beta\\alpha^2}{\\beta-2}.\n\\end{align*}\\]\n\nA variância de \\(X\\) é dada por\n\\[\\mathrm{Var}[X]=\\frac{\\beta\\alpha^2}{\\beta-2}-\\left(\\frac{\\beta\\alpha}{\\beta-1}\\right)^2=\\frac{\\alpha\\beta^2}{(\\beta-1)^2(\\beta-2)}. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(M_Y(t)=\\mathbb{E}\\left[e^{at}e^{btX}\\right]=e^{at}\\mathbb{E}\\left[e^{btX}\\right]=e^{at}M_X(bt)\\), com \\(|t|&lt;\\frac{h}{|b|}\\), \\(h&gt;0\\).\n\n\n\nExemplo 2.12 Seja \\(X\\) uma variável aleatória de uma população com distribuição uniforme no intervalo \\((0,1)\\), i.e. \\(f(x)=\\mathbb{I}_{(0,1)}(x)\\). Seja \\(y=(b-a)X+a\\), onde \\(a,b\\) são constantes, tal que \\(b&gt;a\\). Encontre a FGM da distribuição de \\(Y\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM da distribuição de \\(X\\) é dada por\n\\[M_X(t)=\\mathbb{E}[e^{tX}]=\\int_0^1e^{tx}\\,dx=\\frac{e^t-1}{t}, \\quad t\\ne0.\\] Então, a FGM da distribuição de \\(Y\\) é dada por\n\\[M_Y(t)=e^{at}\\frac{e^{(b-a)t}-1}{(b-a)t}=\\frac{e^{bt}-e^{at}}{(b-a)t}, \\quad t\\ne0. \\quad\\checkmark\\]\n\n\\(Y\\) é uma variável aleatória de uma população com distribuição uniforme no intervalo \\((a,b)\\). O \\(r\\)-ésimo momento da distribuição de \\(Y\\) é dado por \\[\\mathbb{E}Y^r=\\int_a^b\\frac{y^r}{b-a}\\,dy=\\frac{b^{r+1}-a^{r+1}}{(n+1)(b-a)}.\\]\n\n\n\n\n\n\nTeorema 2.4 Sejam \\(X\\) e \\(Y\\) duas variáveis aleatórias com FGM \\(M_{X}(t)\\) e \\(M_{Y}(t)\\), respectivamente. Se existe \\(h&gt;0\\), tal que \\(M_{X}(t)=M_{Y}(t)\\), para todo \\(|t|&lt;h\\), então \\(F_X(u)=F_Y(u)\\) para todo \\(u\\in\\mathbb{R}\\).\n\n\n\nA prova do Teorema 2.4 requer alguns conceitos que extrapolam o nivel do conteúdo. Para os detalhes sugere-se, e.g., Billingsley (1986) e Curtiss (1942).\n\n\n\n\n\n\n\nProblema dos momentos\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição \\(F(x)\\). Suponha que todos os momentos de \\(F(x)\\) existem, i.e. \\(\\mathbb{E}X\\), \\(\\mathbb{E}X^2\\), \\(\\mathbb{E}X^3\\ldots\\) existem. A distribuição de \\(X\\) pode ser determinada de forma única pelos seus momentos?\nMuitas vezes refere-se a esse questionamento como Problema dos momentos de Hausdorff, em honor ao matemático alemão Felix Hausdorff (1868-1942). Em \\(1921\\), Hausdorff mostrou as condições necessárias e suficientes para determinar de forma única a distribuição de uma variável aleatória a partir dos seus momentos populacionais em torno de zero. Na literatura, muitos autores discutem amplamente os detalhes desse problema e as propostas para sua solução. Sugere-se a leitura, e.g., Dudewicz e Mishra (1988) [pp. 261], Shohat e Tamarkin (1970) e Feller (1971, pp. 224).\n\n\n\n\nTeorema 2.5 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com função de distribuição \\(F\\). Seja \\(Y=\\sum_{i=1}^na_iX_i\\), com \\(a_1,a_2,\\ldots,a_n\\) constantes. A FGM da distribuição de \\(Y\\) é dada por \\[M_{Y}(t)=\\prod_{i=1}^nM_{X_1}(a_it),\\] para todo \\(|t|&lt;h\\), , para algum \\(h&gt;0\\).\n\n\n\n\n\n\n\n\nProva\n\n\n\n\n\nO resultado é consequência da Definição 2.2 e do fato que as variáveis aleatórias \\(X_1,X_2,\\ldots,X_n\\) são independentes e identicamente distribuídas. Assim,\n\n\\[\\begin{align*}\n  M_Y(t)&=\\mathbb{E}\\left[e^{tY}\\right]=\\mathbb{E}\\left[e^{t\\sum_{i=1}^na_iX_i}\\right]\n  =\\mathbb{E}\\left[\\prod_{i=1}^ne^{a_itX_i}\\right]\n  =\\prod_{i=1}^n\\mathbb{E}\\left[e^{a_itX_i}\\right]=\\prod_{i=1}^nM_{X_1}(a_it).\\quad\\blacksquare\n\\end{align*}\\]\n\n\nSe \\(a_1=a_2=\\cdots=a_n\\), então \\(M_Y(t)=\\left[M_{X_1}(a_it)\\right]^n\\).\n\n\n\n\n\nExemplo 2.13 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Mostre que \\(\\bar{X}_n\\) tem distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando o resultado do Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_{\\bar{X}_n}(t)&=\\mathbb{E}\\left[e^{t\\frac1n\\sum_{i=1}^nX_i}\\right]\n  =\\left[M_{X_1}\\left(\\frac{t}{n}\\right)\\right]^n\n  =\\left[\\exp\\left\\{\\frac{\\mu}{n}t+\\frac12\\left(\\frac{\\sigma t}{n}\\right)^2\\right\\}\\right]^n\n  =\\exp\\left\\{\\mu t+\\frac{1}{2n}\\left(\\sigma t\\right)^2\\right\\}, t\\in\\mathbb{R}.\\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.14 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGM da distribuição de \\(Y=\\sum_{i=1}^nX_i\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGM é dada por\n\n\\[\\begin{align*}\n  M_X(t)&=\\mathbb{E}\\left[e^{tX}\\right]=\\sum_{x=0}^\\infty e^{tx} \\frac{e^{-\\lambda}\\lambda^x}{\\lambda!}=e^{-\\lambda}\\sum_{x=0}^\\infty  \\frac{(\\lambda e^t)^x}{\\lambda!}=e^{-\\lambda}e^{\\lambda e^{t}}=e^{\\lambda(e^t-1)}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí, pelo Teorema 2.5, temos que\n\n\\[\\begin{align*}\n  M_Y(t)&=\\left(e^{\\lambda(e^t-1)}\\right)^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\n\nDessa forma, \\(Y\\) é uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(n\\lambda\\).\n\n\n\n\n\n\nTeorema 2.6 Seja \\(X_1,X_2,\\ldots,X_k\\) uma sequência de variáveis aleatórias independentes com distribuição normal com média \\(\\mu_i\\) e variância \\(\\sigma^2_i\\), para \\(i=1,2,\\ldots, k\\). Então \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu_i}{\\sigma_i}\\right)^2,\\] segue uma distribuição \\(\\chi^2_{(k)}\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nSeja \\(Z_i=\\frac{X_i-\\mu_i}{\\sigma_i}\\). Dessa forma, a sequência \\(Z_1,Z_2,\\ldots,Z_n\\) configura uma amostra aleatória de uma população com distribuição normal padrão. Daí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tZ^2_1}\\right]&=\\int_{-\\infty}^\\infty e^{tz^2}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac12z^2}\\,dz\n  = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\\\\\n  &= \\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{1-2t}}{\\sqrt{1-2t}}\\int_{-\\infty}^\\infty e^{-\\frac12(1-2t)z^2}\\,dz\n  = \\frac{1}{\\sqrt{1-2t}}, \\quad t&lt;\\frac12.\n\\end{align*}\\]\n\nPelo Teorema 2.5, a FGM da distribuição de \\(U\\) é dada por\n\n\\[\\begin{align*}\n  M_U(t)&=\\left(\\frac{1}{\\sqrt{1-2t}}\\right)^k, \\quad t&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeguindo o resultado do Exemplo 2.8, se \\(\\alpha=\\frac{k}{2}\\) e \\(\\beta=\\frac12\\), então \\[M_X(t)=\\left(1-2t\\right)^{-\\frac{k}{2}}, \\quad t&lt;\\frac12.\\] Sendo a FGM de uma variável aleatória com distribuição \\(\\chi^2_{(k)}\\). Dessa forma, pelo Teorema 2.4, a distribuição da variável aleatória \\(U\\) é \\(\\chi^2_{(k)}\\).\n\n\n\n\n\nCorolário 2.2 Seja \\(X_1,X_2,\\ldots,X_k\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Então, \\[U=\\sum_{i=1}^k\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\] é uma variável aleatória de uma população com distribição \\(\\chi^2_{(k)}\\).\n\n\n\nExemplo 2.15 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\). Mostre que \\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), daí\n\n\\[\\begin{align*}\n  (n-1)S^2_n&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1}+\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+\\sum_{i=1}^{n-1}(\\bar{X}_{n-1}-\\bar{X}_n)^2+2(\\bar{X}_{n-1}-\\bar{X}_n)\\underbrace{\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})}_{0}+(X_n-\\bar{X}_n)^2\\\\\n  &=\\sum_{i=1}^{n-1}(X_i-\\bar{X}_{n-1})^2+(n-1)(\\bar{X}_{n-1}-\\bar{X}_n)^2+(X_n-\\bar{X}_n)^2\\\\\n  &=(n-2)S^2_{n-1}+(n-1)\\left(\\frac{n}{n(n-1)}\\sum_{i=1}^{n-1}X_i-\\frac{n-1}{n(n-1)}\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n^2(n-1)}\\left(nX_n-\\sum_{i=1}^{n}X_i\\right)^2+(X_n-\\bar{X}_n)^2\\\\\n    &=(n-2)S^2_{n-1}+\\frac{1}{n-1}\\left(X_n-\\bar{X}_n\\right)^2+(X_n-\\bar{X}_n)^2\n    =(n-2)S^2_{n-1}+\\frac{n}{n-1}\\left(X_n-\\bar{X}_n\\right)^2\\\\\n  &=(n-2)S^2_{n-1}+\\frac{n-1}{n}\\left(X_n-\\bar{X}_{n-1}\\right)^2, \\quad n&gt;1.\\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, aplicando indução matemática, temos que\n\nPara \\(n=2\\), temos que \\(S^2_2=\\frac{1}{2}\\left(X_2-X_1\\right)^2\\). Sendo \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\), então \\(X_2-X_1\\) segue uma distribuição normal com média \\(0\\) e variância \\(2\\sigma^2\\). Daí, \\(\\frac{\\left(X_2-X_1\\right)^2}{2\\sigma^2}=\\frac{S_2^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\);\nSuponha que, para \\(n=k\\), \\(\\frac{(k-1)S^2_k}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k-1)}\\);\nProvemos para \\(n=k+1\\),\n\n\n\\[\\begin{align*}\n  \\frac{kS^2_{k+1}}{\\sigma^2}&=\\frac{(k-1)S^2_{k}}{\\sigma^2}+\\frac{1}{\\sigma^2}\\frac{k}{k+1}\\left(X_{k+1}-\\bar{X}_{k}\\right)^2.\n\\end{align*}\\]\n\nObserve que, \\(X_{k+1}-\\bar{X}_{k}\\) segue uma distribuição normal com média \\(0\\) e variância \\(\\left(\\frac{k+1}{k}\\right)\\sigma^2\\). Daí, \\(\\frac{k}{k+1}\\frac{(X_{k+1}-\\bar{X}_{k})^2}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(1)}\\). Pelo Teorema 2.6, \\(\\frac{kS^2_{k+1}}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(k)}\\). \\(\\quad\\checkmark\\)\n\n\n\n\n\nTeorema 2.7 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(\\sigma^2\\). Sejam \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) e \\(S^2_n=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\\), então\n\n\n\\(\\bar{X}_n\\) segue uma distribuição normal com média \\(\\mu\\) e variância \\(\\frac{\\sigma^2}{n}\\);\n\n\n\\(\\frac{(n-1)S^2_n}{\\sigma^2}\\) segue uma distribuição \\(\\chi^2_{(n-1)}\\);\n\n\n\\(\\bar{X}_n\\) e \\(S^2_n\\) são variáveis aleatórias independentes.\n\n\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\n\n\nVide Teorema 2.1 e Exemplo 2.13;\n\n\nVide Exemplo 2.15;\n\n\nNote que\n\n\\[\\begin{align*}\nS^2_n&=\\frac{1}{n-1}\\sum_{i=1}^n\\left(X_i-\\bar{X}_n\\right)^2\n=\\frac{1}{n-1}\\left[(X_1-\\bar{X}_n)+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=1}^n(X_i-\\bar{X}_n)-(X_1-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{\\left[\\sum_{i=2}^n(X_i-\\bar{X}_n)\\right]^2+\\sum_{i=2}^n\\left(X_i-\\bar{X}_n\\right)^2\\right\\}\n\\end{align*}\\]\n\nDaí, \\(S^2_n\\) pode ser escrito em função de \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n, \\ldots, X_n-\\bar{X}_n\\). Dessa forma, se as variáveis aleatórias \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) são independentes de \\(\\bar{X}_n\\), obtem-se o resultado requerido.\nPara isso, calcula-se a densidade conjunta das variáveis aleatórias \\(\\bar{X}_n\\), \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\) para verificar se a mesma pode ser escrita como produto das densidades de \\(\\bar{X}_n\\) e \\(X_2-\\bar{X}_n, X_3-\\bar{X}_n,\\ldots,X_n-\\bar{X}_n\\). Dessa maneira, para usar o método do jacobiano, considere \\(y_1=\\bar{x}_n\\) e \\(y_i=x_i-\\bar{x}_n\\) para \\(i=2,3,\\ldots,n\\). Então,\n\n\\[\\begin{align*}\n    x_1&=y_1-\\sum_{i=2}^ny_i; & x_i&=y_i+y_1, \\quad i=2,3,4,\\ldots.\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\n    \\frac{\\partial x_1}{\\partial y_1}&=1; & \\frac{\\partial x_2}{\\partial y_1}&=1; & \\frac{\\partial x_3}{\\partial y_1}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_1}&=1;\\\\\n    \\frac{\\partial x_1}{\\partial y_2}&=-1; & \\frac{\\partial x_2}{\\partial y_2}&=1; & \\frac{\\partial x_3}{\\partial y_2}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_2}&=0;\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=1; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=0;\\\\\n    &\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots &&\\,\\,\\,\\vdots & & &&\\,\\,\\,\\vdots\\\\\n    \\frac{\\partial x_1}{\\partial y_3}&=-1; & \\frac{\\partial x_2}{\\partial y_3}&=0; & \\frac{\\partial x_3}{\\partial y_3}&=0; & \\cdots & & \\frac{\\partial x_n}{\\partial y_3}&=1.\n\\end{align*}\\]\n\nAssim sendo,\n\n\\[\\begin{align*}\n    |J|&=\\begin{vmatrix}\n     1 & 1 & 1 & \\cdots & 1\\\\\n    -1 & 1 & 0 & \\cdots & 0\\\\\n    -1 & 0 & 1 & \\cdots & 0\\\\\n    \\vdots &  &  & \\ddots & \\vdots \\\\\n    -1 & 0 & 0 & \\cdots & 1\\\\\n    \\end{vmatrix}=n.\n\\end{align*}\\]\n\nPor otro lado, sabemos que a função de densidade conjunta de \\(X_1,X_2,\\ldots,X_n\\) é dada por\n\\[f(x_1,x_2,\\ldots,x_n)=(2\\pi)^{-n/2}e^{-\\frac12\\sum_{i=1}^nx_i}, \\quad x_i\\in\\mathbb{R}.\\] Portanto,\n\n\\[\\begin{align*}\n    f(y_1,y_2,\\ldots,y_n)&=\\frac{n}{(2\\pi)^{-n/2}}e^{-\\frac12(y_1-\\sum_{i=2}^ny_i)^2}e^{-\\frac12\\sum_{i=2}^n(y_i+y_1)^2}\\\\\n    &=\\frac{1}{(2\\pi n^{-1})^{\\frac12}}e^{-\\frac12ny_1^2}\\frac{1}{(2\\pi )^{\\frac{n-1}{2}}n^{-1/2}}e^{\\left[-\\frac12\\sum_{i=2}^ny_i^2+(\\sum_{i=2}^ny_i)^2\\right]}.\n\\end{align*}\\]\n\nEntão, \\(Y_1\\) é independente de \\(Y_2,Y_3,\\ldots,Y_n. \\quad \\blacksquare\\)\n\n\n\n\n\n\n2.2.1 Momentos fatoriais\n\nDefinição 2.3 (Função geradora de momentos fatoriais) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de momentos fatoriais (FGMF) de \\(F_X\\), denotada por \\(\\mathcal{M}_X(t)\\), se existe, define-se como\n\\[\\mathcal{M}_X(t)=M_X(\\log t)=\\mathbb{E}\\left[t^{X}\\right],\\]\npara todo \\(t\\in\\mathbb{R}\\).\nSe \\(\\mathcal{M}_X(t)\\) existe em uma vizinhança de \\(t = 1\\), o \\(r\\)-ésimo momento fatorial é dado por \\[\\mathbb{E}\\left[(X)_r\\right]=\\mathbb{E}\\left[X(X-1)(X-2)\\cdots (X-r+1)\\right]=\\Biggr.\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}.\\]\n\n\n\n\n\n\n\nDica\n\n\n\nSeja \\(Y=a+bX\\), onde \\(X\\) é uma variável aleatória, \\(a\\) e \\(b\\) constantes, então \\(\\mathcal{M}_Y(t)=t^a\\mathcal{M}_X(t^b)\\).\n\n\n\nExemplo 2.16 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{n}{x}p^x(1-p)^{n-x}\\mathbb{I}_{\\{0,1,2,\\ldots,n\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^n t^j \\binom{n}{j}p^j(1-p)^{n-j}=\\sum_{j=0}^n \\binom{n}{j}(tp)^j(1-p)^{n-j}=[(1-p)+tp]^n, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n[(1-p)+tp]^{n-1}p\\Biggr|_{t=1}=np;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=\\frac{\\partial^2}{\\partial t^2}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)p^2[(1-p)+tp]^{n-2}\\Biggr|_{t=1}=n(n-1)p^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=n(n-1)\\cdots(n-r+1)p^r=\\frac{n!}{(n-r)!}p^r=(n)_r\\,p^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nSeja \\(X\\) uma variável aleatória de uma população com distribuição Poisson com parâmetro \\(\\lambda\\), i.e. \\(\\mathbb{P}[X=x]=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Utilize a FGMF para calcular o \\(r\\)-ésimo momento fatorial da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\frac{e^{-\\lambda}\\lambda^j}{\\lambda!}=e^{-\\lambda}\\sum_{j=0}^\\infty  \\frac{(t\\lambda)^j}{\\lambda!}=e^{-\\lambda}e^{t\\lambda}=e^{(t-1)\\lambda}, \\quad t\\in\\mathbb{R}.\n\\end{align*}\\]\n\nDessa forma,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[X\\right]&=e^{(t-1)\\lambda}\\,\\lambda\\Biggr|_{t=1}=\\lambda;\\\\ \\\\\n  \\mathbb{E}\\left[(X)_2\\right]&=e^{(t-1)\\lambda}\\,\\lambda^2\\Biggr|_{t=1}=\\lambda^2;\\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{E}\\left[(X)_r\\right]&=\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=1}=e^{(t-1)\\lambda}\\,\\lambda^r\\Biggr|_{t=1}=\\lambda^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.17 Seja \\(X\\) uma variável aleatória de uma população com distribuição Binomial negativa com parâmetros \\(n\\) e \\(p\\), i.e., \\(\\mathbb{P}[X=x]=\\binom{x+n-1}{x}p^n(1-p)^{x}\\mathbb{I}_{\\{0,1,2,\\ldots\\}}(x)\\). Calcule a FGMF da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nA FGMF é dada por\n\n\\[\\begin{align*}\n  \\mathcal{M}_X(t)&=\\mathbb{E}\\left[t^{X}\\right]=\\sum_{j=0}^\\infty t^j \\binom{j+n-1}{j}p^n(1-p)^{j}=\\sum_{j=0}^\\infty \\binom{j+n-1}{j}p^n[t(1-p)]^{j}\\\\\n  &=p^n\\sum_{j=0}^\\infty \\binom{j+n-1}{j}[t(1-p)]^{j}=p^n[1-(1-p)t]^{-n}=\\left[\\frac{p}{1-(1-p)t}\\right]^n,  \\quad |t|&lt;\\frac{1}{1-p}. \\quad\\checkmark\n\\end{align*}\\]\nA última soma é resultado de aplicar os resultados da Seção A.2 do Apêndice A.\n\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nSe \\(X\\) é uma variável aleatória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\), então \\(\\mathcal{M}_X(t)\\) é chamada de função geradora de probabilidades (FGP). Nesse caso, \\(\\mathcal{M}_X(t)\\) existe para todo \\(|t|\\le1\\).\nA \\(r\\)-ésima probabilidade de massa de \\(X\\) calcula-se como \\[\\mathbb{P}\\left[X=r\\right]=\\frac{1}{k!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}.\\]\n\n\n\nExemplo 2.18 Considere o cenário do Exemplo 2.16. Utilize a FGP da distribuição de \\(X\\) para calcular a \\(r\\)-ésima probabilidade de massa.\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPelo Exemplo 2.16, sabemos que a FGP é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\). Daí,\n\n\\[\\begin{align*}\n  \\mathbb{P}\\left[X=0\\right]&=[(1-p)+tp]^n\\Biggr|_{t=0}=(1-p)^n; \\\\ \\\\\n  \\mathbb{P}\\left[X=1\\right]&=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=0}=np(1-p)^{n-1}; \\\\ \\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  \\mathbb{P}\\left[X=r\\right]&=\\frac{1}{r!}\\frac{\\partial^r}{\\partial t^r}\\mathcal{M}_X(t)\\Biggr|_{t=0}=\\frac{n(n-1)\\cdots(n-r+1)}{r!}p^r(1-p)^{n-r}\\\\\n  &=\\frac{n!}{(n-r)!r!}p^r(1-p)^{n-r}=\\binom{n}{r}p^r(1-p)^{n-r}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\n\n2.2.2 Cumulantes\n\nDefinição 2.4 (Função geradora de cumulantes) Seja \\(X\\) uma variável aleatória de uma população com função de distribuição \\(F_X\\). A função geradora de cumulantes (FGC) de \\(F_X\\), define-se como\n\\[K_X(t)=\\log M_X(t)=\\sum_{j=1}^\\infty\\kappa_n\\frac{t^j}{j!},\\]\nonde \\(M_X(t)\\) representa a FGM de \\(F_X\\), \\(|t|&lt;h\\), \\(h&gt;0\\). As constantes \\(\\kappa_1,\\kappa_2,\\kappa_3,\\ldots\\) são chamados de cumulantes da distribuição \\(F_X\\).\n\n\n\n\n\n\n\nDica\n\n\n\nOs cumulantes são funções dos momentos e podem ser obtidos diferenciando a função \\(K_X(t)\\)\n\\[\\kappa_j=K'_X(0)=\\frac{\\partial^j}{\\partial t^j}K_X(t)\\Biggr|_{t=0},\\] para \\(j=1,2,3,\\ldots\\).\n\n\n\nExemplo 2.19 Sejam \\(X_1,X_2\\) variáveis aleatórias independentes. Mostre que \\[K_{X+Y}(t)=K_X(t)+K_Y(t).\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nAplicando a Definição 2.4, temos que\n\n\\[\\begin{align*}\n  K_{X+Y}(t)&=\\log\\mathbb{E}[e^{t(X+Y)}]=\\log\\mathbb{E}[e^{tX}e^{tY}]\n  =\\log\\left(\\mathbb{E}[e^{tX}]\\mathbb{E}[e^{tY}]\\right)\\\\\n  &=\\log\\mathbb{E}[e^{tX}]+\\log\\mathbb{E}[e^{tY}]=K_X(t)+K_Y(t). \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.20 Seja \\(X\\) uma variável aleatória de uma população com distribuição Bernoulli com parâmetro \\(p\\). Utilize a FGC para calcular o \\(r\\)-ésimo cumulante da distribuição de \\(X\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.3, sabemos que \\(\\mathcal{M}_X(t)=M_X(\\log t)\\). Pelo Exemplo 2.16, sabemos que a FGMF de uma variável aleatória com distribuição binomial é dada por \\(\\mathcal{M}_X(t)=[(1-p)+tp]^n\\), com \\(t\\in\\mathbb{R}\\). Assim, a FGM de uma variável aleatória com distribuição Bernoulli é dada por \\(M_X(t)=(1-p)+e^tp\\), com \\(t\\in\\mathbb{R}\\). Daí, \\(K_{X}(t)=\\log M_X(t)=\\log[(1-p)+e^tp]\\).\nDessa forma,\n\n\\[\\begin{align*}\n  K'_{X}(0)&=\\kappa_1=\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}=p\\\\\n  K''_{X}(0)&=\\kappa_2=e^tp\\frac{(1-p)+e^tp-e^tp}{((1-p)+e^tp)^2}\\Biggr|_{t=0}=p(1-p)\\\\\n  K'''_{X}(0)&=\\kappa_3=\\frac{[(1-p)+e^tp]^2p(1-p)e^t-2p(1-p)e^t(e^tp+p(1-p)e^t)}{((1-p)+e^tp)^4}\\Biggr|_{t=0}=p(1-p)(1-2p)\\\\\n  &\\,\\,\\,\\,\\vdots \\\\ \\\\\n  K^{(r)}_X(0)&=\\kappa_r=\\frac{\\partial^r}{\\partial t^r}K_X(t)\\Biggr|_{t=0}.\n\\end{align*}\\]\n\nPara calcular a \\(r\\)-ésima derivada de \\(K_X(t)\\), note que\n\n\\[\\begin{align*}\n  \\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial}{\\partial p}\\left[\\frac{\\partial^r}{\\partial t^r}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial p}\\log[(1-p)+e^tp]\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outra parte,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}&=\\frac{\\partial^{r+1}}{\\partial t^{n+1}}K_X(t)\\Biggr|_{t=0}\n  =\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{\\partial}{\\partial t}K_X(t)\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]. \\quad\\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n  \\kappa_{r+1}-p(1-p)\\frac{\\partial\\kappa_r}{\\partial p}&=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp}{(1-p)+e^tp}-p(1-p)\\frac{e^t-1}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]\\\\\n  &=\\frac{\\partial^r}{\\partial t^r}\\left[\\frac{e^tp^2+p(1-p)}{(1-p)+e^tp}\\Biggr|_{t=0}\\right]=\\frac{\\partial^r}{\\partial t^r}p=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, para \\(r&gt;1\\) \\[\\kappa_{r+1}=p(1-p)\\frac{\\partial}{\\partial p}\\kappa_r. \\quad\\checkmark\\]\n\n\n\n\n\n\n\n\n\nObservação\n\n\n\nDa Definição 2.2, sabemos que \\(M_X(t)=\\mathbb{E}\\left[e^{tX}\\right]\\) existe para todo \\(|t|&lt;h\\), para algum \\(h&gt;0\\). Se \\(\\mathbb{E}X^r\\) existe para todo \\(r=1,2,\\ldots\\), então\n\\[M_X(t)=\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r, \\quad |t|&lt;h.\\] Dessa forma,\n\n\\[\\begin{align*}\n  K_X(t)&=\\log M_X(t)=\\log\\left[\\sum_{r=0}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\n  =\\log\\left[1+\\sum_{r=1}^\\infty \\frac{t^r}{r!}\\mathbb{E}X^r\\right]\\\\\n  &=\\mathbb{E}[X]\\, t+\\frac{1}{2!}\\left[\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right]t^2\n  +\\frac{1}{3!}\\left[\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X\\right]t^3+\\cdots.\n\\end{align*}\\]\n\nDaí, \\(\\kappa_1=\\mathbb{E}X\\), \\(\\kappa_2=\\mathbb{E}X^2-(\\mathbb{E}X)^2\\), \\(\\kappa_3=\\mathbb{E}X^3-\\mathbb{E}X\\mathbb{E}X^2-2\\left(\\mathbb{E}X^2-(\\mathbb{E}X)^2\\right)\\mathbb{E}X=\\mathbb{E}\\left[(X-\\mathbb{E}X)^3\\right]\\), \\(\\kappa_4=\\mathbb{E}\\left[(X-\\mathbb{E}X)^4\\right]-3\\kappa_2^2\\), etc.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  },
  {
    "objectID": "p1chap02.html#função-geradora-de-momentos-conjunta",
    "href": "p1chap02.html#função-geradora-de-momentos-conjunta",
    "title": "2  Objetos em manim",
    "section": "2.3 Função geradora de momentos conjunta",
    "text": "2.3 Função geradora de momentos conjunta\n\nDefinição 2.5 Seja \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)^\\intercal\\) uma vetor aleatório de tamanho \\(n\\times1\\). A função geradora de momentos conjunta (FGMC) define-se, como\n\\[M_{\\mathbf{X}}(\\mathbf{t})=\\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]=\\mathbb{E}\\left[e^{\\sum_{i=1}^nt_iX_i}\\right],\\] se existe para todos os vetores reais \\(\\mathbf{t}=(t_1,t_2,\\ldots,t_n)\\) que pertencem a um retângulo rechado \\(H\\), tal que\n\\[H=[-h_1,h_1]\\times[-h_2,h_2]\\times\\cdots\\times[-h_n,h_n]\\subset\\mathbb{R}^n,\\] com \\(h_i&gt;0\\) para todo \\(i=1,2,3,\\ldots, n\\).\n\n\nExemplo 2.21 Seja \\(\\mathbf{X}=(X_1,X_2)\\) um vetor aleatório de uma população com função de densidade conjunta dada por\n\\[f(x_1,x_2)=e^{-(x_1+x_2)}, \\quad x_1&gt;0, x_2&gt;0.\\]\nCalcule a FGMC da distribuição de \\(\\mathbf{X}=(X_1,X_2)\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPela Definição 2.5, sabemos que\n\n\\[\\begin{align*}\n  M_{\\mathbf{X}}(\\mathbf{t}) &= \\mathbb{E}\\left[e^{\\mathbf{t}^{\\intercal}\\mathbf{X}}\\right]\n  = \\mathbb{E}[e^{t_1X_1+t_2X_2}] = \\int_{0}^\\infty \\int_{0}^\\infty  e^{t_1x_1+t_2x_2}e^{-(x_1+x_2)}\\,dx_1dx_2 \\\\\n  &= \\int_{0}^\\infty \\int_{0}^\\infty e^{(t_1-1)x_1}e^{(t_2-1)x_2}\\,dx_1dx_2\n  = \\int_{0}^\\infty e^{(t_1-1)x_1}\\,dx_1 \\int_{0}^\\infty e^{(t_2-1)x_2}\\,dx_2\\\\\n  &= \\frac{1}{(1-t_1)(1-t_2)}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.22 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Encontre as distribuições de \\(2X_1X_2\\) e \\(X_2^2-X_1^2\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\(X_1\\) e \\(X_2\\) são variáveis aleatórias independentes e identicamente distribuidas com distribuição \\(N(0,1)\\). Dessa forma, \\(2X_1X_2\\) tem distribuição \\(2\\chi_{(1)}^2\\), ou seja \\(Gama\\left(\\frac12,\\frac14\\right)\\).\nSeja \\(U=2X_1X_2\\), então\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[e^{tU}\\right]&=\\mathbb{E}\\left[e^{2tX_1X_2}\\right]=\n      \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{2tx_1x_2}e^{-\\frac12(x_1^2+x_2^2)}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-\\frac12x_2^2}\\int_{-\\infty}^\\infty e^{-\\frac12x_1^2}e^{2tx_1x_2}\\,dx_1dx_2\\\\\n      &=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12x_2^2\\right\\}\\exp\\left\\{2t^2x_2^2\\right\\}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(x_1-2tx_2\\right)^2\\right\\}\\,dx_1dx_2\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12\\left(1-4t^2\\right)x_ 2^2\\right\\}\\,dx_2\\\\\n      &=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12. \\quad\\checkmark\n\\end{align*}\\]\n\nSeja \\(t^*=t^2\\), então \\(\\mathbb{E}\\left[e^{t^*U}\\right]=\\left(1-4t^*\\right)^{-\\frac12}\\), com \\(t^*&lt;\\frac{1}{4}\\), i.e., a função geradora de momentos de uma \\(Gama(\\frac12,\\frac14)\\).\nConsidere \\(V=X_2^2-X_1^2\\), então\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[e^{tV}\\right]&=\\mathbb{E}\\left[e^{t\\left(X_2^2-X_1^2\\right)}\\right]=\\mathbb{E}\\left[e^{tX_2^2}\\right]\\mathbb{E}\\left[e^{- tX_1^2}\\right]=\\left(1-2t\\right)^{-\\frac12}\\left(1+2t\\right)^{-\\frac12}\\\\\n      &=\\left[\\left(1-2t\\right)\\left(1+2t\\right)\\right]^{-\\frac12}=\\left(1-4t^2\\right)^{-\\frac12}, \\quad t^2&lt;\\frac14 \\text{ ou } |t|&lt;\\frac12.\n\\end{align*}\\]\n\nPelo @-fgm_unicidade, temos que \\(U\\) e \\(V\\) seguem a mesma distribuição \\(2\\chi_{(1)}^2\\) ou \\(gama(\\frac12,\\frac14)\\quad\\checkmark\\).\n\n\n\n\nExemplo 2.23 Seja \\(X_1,X_2\\) uma amostra aleatória de uma população com distribuição normal padrão. Calcule a FGMC do vetor \\((Y,Z)\\), onde \\(Y=X_1+X_2\\) e \\(Z=X_1^2+X_2^2\\). As variáveis \\(Y\\) e \\(Z\\) são correlacionadas?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[e^{t_1Y+t_2Z}\\right]=\\mathbb{E}\\left[e^{t_1(X_1+X_2)+t_2\\left(X_1^2+X_2^2\\right)}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_1X_2+t_2X_1^2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right].\n\\end{align*}\\]\n\nPor outra parte, temos que\n\n\\[\\begin{align*}\n      \\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]&=\\int_{-\\infty}^\\infty e^{t_1x_1+t_2x_1^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac12x_1^2}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{t_1x_1+\\left(t_2-\\frac12\\right)x_1^2\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left(x_1^2-\\frac{2t_1}{1-2t_2}x_1\\right)\\right\\}\\,dx_1, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right)\\\\\n      &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp\\left\\{-\\frac12(1-2t_2)\\left[\\left(x_1-\\frac{t_1}{1-2t_2}\\right)^2-\\frac{t_1^2}{(1-2t_2)^2}\\right]\\right\\}\\,dx_1\\\\\n      &=\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}. \\quad \\checkmark\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n      M(t_1,t_2)&=\\mathbb{E}\\left[\\exp\\left\\{t_1X_1+t_2X_1^2\\right\\}\\right]\\mathbb{E}\\left[\\exp\\left\\{t_1X_2+t_2X_2^2\\right\\}\\right]\\\\\n      &=\\left(\\frac{1}{\\sqrt{1-2t_2}}\\exp\\left\\{\\frac12\\frac{t_1^2}{1-2t_2}\\right\\}\\right)^2\\\\\n      &=\\frac{1}{1-2t_2}\\exp\\left\\{\\frac{t_1^2}{1-2t_2}\\right\\}, \\quad t_1\\in\\mathbb{R}, t_2\\in\\left(-\\infty,\\frac12\\right). \\quad\\checkmark\n\\end{align*}\\]\n\nObserve que, \\(M(t_1,0)=M(t_1)=e^{t_1^2}\\), para \\(t_1\\in\\mathbb{R}\\), i.e. \\(Y\\) é uma variável aleatória de uma população com distribuição normal com média \\(0\\) e variância \\(2\\). Por otro lado, \\(M(,t_2)=M(t_2)=\\frac{1}{1-2t_2}\\), para \\(-\\infty&lt;t_2&lt;\\frac12\\), i.e., \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade.\nPara verificar se \\(Y\\) e \\(Z\\) são correlacionadas, calcula-se a correlação entre as duas variáveis aleatórias. Assim,\n\n\\[\\begin{align*}\n      \\mathbb{E}YZ=\\frac{\\partial^2}{\\partial t_1\\partial t_2}M(t_1,t_2)\\Biggr|_{(t_1,t_2)=(0,0)}\n      &=\\frac{\\partial}{\\partial t_1}\\left[\\frac{\\partial}{\\partial t_2}M(t_1,t_2)\\right]\\Biggr|_{(t_1,t_2)=(0,0)}\\\\       &=\\frac{4t_1\\exp\\left\\{\\frac{t^2_1}{1-2t_2}\\right\\}(-4t_2+t_1^2+2)}{(1-2t_2)^4}\\Biggr|_{(t_1,t_2)=(0,0)}\\\\\n      &=0. \\quad\\checkmark\n\\end{align*}\\]\n\nDessa forma, \\(\\mathrm{Cov}[Y,Z]=\\mathbb{E}YZ-\\mathbb{E}Y\\mathbb{E}Z=0\\). Daí, as variáveis \\(Y\\) e \\(Z\\) são não-correlacionadas.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  },
  {
    "objectID": "p1chap02.html#exercícios-sugeridos",
    "href": "p1chap02.html#exercícios-sugeridos",
    "title": "2  Objetos em manim",
    "section": "Exercícios sugeridos",
    "text": "Exercícios sugeridos\n\nSeja \\(X\\) uma variável aletória discreta com valores em \\(\\{0,1,2,\\ldots\\}\\). Verifique que \\(\\mathbb{E}[X]=\\mathcal{M}'_X(1)=\\frac{\\partial}{\\partial t}\\mathcal{M}_X(t)\\Biggr|_{t=1}\\) e \\(\\mathrm{Var}[X]=\\mathcal{M}''_X(1)+\\mathcal{M}'_X(1)-\\left[\\mathcal{M}'_X(1)\\right]^2\\).\nSejam \\(X\\) e \\(Y\\) variáveis aleatórias com funções geradoras de probabilidade \\(\\mathcal{M}_X(t)\\) e \\(\\mathcal{M}_Y(t)\\), respectivamente. Mostre que \\(\\mathcal{M}_X(t)=\\mathcal{M}_Y(t)\\) se e somente se \\(\\mathbb{P}[X=k]=\\mathbb{P}[Y=k]\\).\nVerifique que \\(\\mathbb{E}[X^r]=\\sum_{j=0}^r{n\\brace k}\\mathbb{E}[(X)_j]\\), onde \\({n\\brace k}=\\frac{1}{k!}\\sum_{i=0}^k(-1)^{k-i}\\binom{k}{i}i^n=\\sum_{i=0}^k\\frac{(-1)^{k-i}i^n}{(k-i)!i!}\\) são chamados de números de Stirling tipo II.\n\n\n\n\n\nBillingsley, Patrick. 1986. Probability and Measure. Second. John Wiley; Sons.\n\n\nCurtiss, J. H. 1942. “A note on the theory moment generating functions”. The annals of the mathematical statistics 13 (4): 430–33.\n\n\nDudewicz, E., e S. Mishra. 1988. Modern mathematical statistics. 1st ed. John Wiley & Sons.\n\n\nFeller, W. 1971. An introduction to probability theory and its applications. 2nd ed. Vol. II. Wiley.\n\n\nShohat, J. A., e J. D. Tamarkin. 1970. The problem of moments. Vol. I. American Mathematical Society.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  },
  {
    "objectID": "p1chap02.html#sec-retangle",
    "href": "p1chap02.html#sec-retangle",
    "title": "2  Objetos em manim",
    "section": "",
    "text": "arc: Permite a construção de Mobjetos curvados;\n\n\nExemplo 2.1 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a ?def-estatistica, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\nExemplo 2.2 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.3 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 2.4 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 2.3, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 2.5.\n\n\n\n\n\nExemplo 2.5 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 2.1, sabemos da ?def-momento_amostral0 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 2.6 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 2.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 2.1. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-animations",
    "href": "p1chap01.html#sec-animations",
    "title": "1  Estructura básica",
    "section": "1.3 Animações",
    "text": "1.3 Animações\nNo tipo geometry encontram-se vários Mobjetos geométricos úteis para criar as nossas animações.\n\narc: Permite a construção de Mobjetos curvados;\n\n\nExemplo 1.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a ?def-estatistica, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\nExemplo 1.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 1.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 1.5 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 1.4, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 1.5.\n\n\n\n\n\nExemplo 1.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 1.1, sabemos da ?def-momento_amostral0 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 1.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 1.7 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 1.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 1.1. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-add",
    "href": "p1chap01.html#sec-add",
    "title": "1  Estructura básica",
    "section": "1.4 Método Scene.add()",
    "text": "1.4 Método Scene.add()\nA primeira maneira é utilizando o método Scene.add(). Este método permite que você adicione um Mobject instantaneamente à cena, sem qualquer animação ou duração associada. Ou seja, o objeto aparecerá imediatamente no quadro, sem transições visuais ou efeitos de movimento. Essa abordagem é útil quando se deseja simplesmente inserir um elemento na tela de forma estática e direta, sem a necessidade de animações elaboradas.\n\nExemplo 1.2 Criação de três quadrados coloridos usando o método Scene.add().\n\nfrom manim import *\n\nclass CenaFixa(Scene):\n    def construct(self):\n        \n        texto = Text(\"Quadrados coloridos\")\n        texto.to_edge(UP)\n\n        q1 = Square(\n            color=BLUE_E,\n            fill_color=BLUE_E,\n            fill_opacity=1\n        )\n        \n        q2 = q1.copy()\n        q2.set_color(RED)\n        q2.next_to(q1, RIGHT)\n\n        q3 = q1.copy()\n        q3.set_color(YELLOW_E)\n        q3.next_to(q1, LEFT)\n        \n        self.add(texto, q1, q2, q3)\n\n\n\n\n\n\n\n\nResultado\n\n\n\n\n\n\n\n\n\n\n\nFigura 1.1: Quadrados coloridos usando o método Scene.add().\n\n\n\n\n\n\nLinha 7. O método mobject.to_edge() move o objeto verticalmente ou horizontalmente até a borda. No caso do exemplo, até a borda superior;\nLinha 9. Instanciando um objeto da classe Square() e atribuído à variável q1. Em palavras mais simples, criamos um quadrado de cor azul;\nLinhas 15-17. Criamos uma cópia do objeto q1. modificamos a cor com o método mobject.set_color() e posicionamos do lado direito do objeto;\nLinha 23. Adicionamos os objetos à cena usando o método Scene.add().\n\n\n\n\n\n\nDica\n\n\n\nNote que, no exemplo Exemplo 1.2, foram utilizados alguns atributos definidos especificamente para os Mobjects. Esses atributos são propriedades fundamentais que controlam como os objetos se comportam e são exibidos na cena.\nNo decorrer deste tutorial, serão apresentados e explorados os principais atributos que podem ser utilizados para modificar os Mobjects. Através de exemplos práticos, você aprenderá como aplicar esses atributos de forma eficaz, a fim de atingir os efeitos desejados em suas animações. Isso inclui não apenas a manipulação básica dos atributos, mas também o uso de combinações avançadas que podem resultar em transições e efeitos visuais sofisticados.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-adicionando",
    "href": "p1chap01.html#sec-adicionando",
    "title": "1  Estrutura básica",
    "section": "1.3 Adicionando elementos à cena",
    "text": "1.3 Adicionando elementos à cena\nExistem duas maneiras principais de adicionar um objeto à cena no processo de criação de animações: Scene.add() e Scene.play().\n\n1.3.1 Método Scene.add()\nA primeira maneira é utilizando o método Scene.add(). Este método permite que você adicione um Mobject instantaneamente à cena, sem qualquer animação ou duração associada. Ou seja, o objeto aparecerá imediatamente no quadro, sem transições visuais ou efeitos de movimento. Essa abordagem é útil quando se deseja simplesmente inserir um elemento na tela de forma estática e direta, sem a necessidade de animações elaboradas.\n\nExemplo 1.2 Criação de três quadrados coloridos usando o método Scene.add().\n\nfrom manim import *\n\nclass CenaFixa(Scene):\n    def construct(self):\n        \n        texto = Text(\"Quadrados coloridos\")\n        texto.to_edge(UP)\n\n        q1 = Square(\n            color=BLUE_E,\n            fill_color=BLUE_E,\n            fill_opacity=1\n        )\n        \n        q2 = q1.copy()\n        q2.set_color(RED)\n        q2.next_to(q1, RIGHT)\n\n        q3 = q1.copy()\n        q3.set_color(YELLOW_E)\n        q3.next_to(q1, LEFT)\n        \n        self.add(texto, q1, q2, q3)\n\n\n\n\n\n\n\nResultado\n\n\n\n\n\n\n\n\n\n\n\nFigura 1.1: Quadrados coloridos usando o método Scene.add().\n\n\n\n\n\n\nLinha 7. O método mobject.to_edge() move o objeto verticalmente ou horizontalmente até a borda. No caso do exemplo, até a borda superior;\nLinha 9. Instanciando um objeto da classe Square() e atribuído à variável q1. Em palavras mais simples, criamos um quadrado de cor azul;\nLinhas 15-17. Criamos uma cópia do objeto q1. modificamos a cor com o método mobject.set_color() e posicionamos do lado direito do objeto;\nLinha 23. Adicionamos os objetos à cena usando o método Scene.add().\n\n\n\n\n\n\n\nDica\n\n\n\nNote que, no exemplo Exemplo 1.2, foram utilizados alguns atributos definidos especificamente para os Mobjects. Esses atributos são propriedades fundamentais que controlam como os objetos se comportam e são exibidos na cena.\nNo decorrer deste tutorial, serão apresentados e explorados os principais atributos que podem ser utilizados para modificar os Mobjects. Através de exemplos práticos, você aprenderá como aplicar esses atributos de forma eficaz, a fim de atingir os efeitos desejados em suas animações. Isso inclui não apenas a manipulação básica dos atributos, mas também o uso de combinações avançadas que podem resultar em transições e efeitos visuais sofisticados.\n\n\n\n\n1.3.2 Método Scene.play()\nA segunda maneira, que oferece mais flexibilidade e controle, é através do método Scene.play(). Este método, ao contrário do Scene.add(), permite que você adicione um Mobject à cena de forma animada, utilizando transições, efeitos e um tempo de duração definidos. Com ele, é possível criar movimentos suaves, alterações de posição, escala, rotação e outros tipos de transformações visuais que enriquecem a apresentação gráfica. Esse método será discutido em maior detalhe mais adiante no tutorial, onde exploraremos suas funcionalidades e o impacto que ele pode ter no dinamismo de uma animação.\nEnquanto o Scene.add() é prático para situações em que a simplicidade e a rapidez são os principais objetivos, o Scene.play() oferece uma gama muito maior de possibilidades, permitindo a criação de animações fluidas e envolventes. Ambos os métodos têm seus usos específicos, dependendo da necessidade do projeto em questão. No decorrer deste tutorial, veremos exemplos práticos que ilustram como e quando utilizar cada um desses métodos, garantindo que você tenha o domínio necessário para criar animações eficientes e atraentes.\n\nExemplo 1.3 Criação de três quadrados coloridos usando o método Scene.play().\n\nfrom manim import *\n\nclass CenaAnimada(Scene):\n    def construct(self):\n        \n        texto = Text(\"Quadrados coloridos\")\n        \n        q1 = Square(\n            color=BLUE_E,\n            fill_color=BLUE_E,\n            fill_opacity=1\n        )\n        q2 = q1.copy()\n        q2.set_color(RED)\n\n        q3 = q1.copy()\n        q3.set_color(YELLOW_E)\n\n        self.play(FadeIn(texto))\n        self.play(texto.animate.to_edge(UP))\n        self.wait(0.5)\n        self.play(FadeIn(q1), FadeIn(q2), FadeIn(q3))\n        self.play(q2.animate.next_to(q1, RIGHT), q3.animate.next_to(q1, LEFT))\n        self.wait()\n\n\n\n\n\n\n\nResultado\n\n\n\n\n\n\n\n\n\n\n\nFigura 1.2: Quadrados coloridos usando o método Scene.play().\n\n\n\n\n\n\nLinhas 19-20. Utilizamos uma transição relativamente lenta e suave para mostrar o texto e solicitamos que o posicione na borda superior;\nLinha 21. Adicionamos uma pausa de 0.5 segundos. A duração padrão do método Scene.wait() é 1 segundo;\nLinha 22-23. Mostramos os quadrados com uma transição lenta e posicionamos os objetos na cena. Nesse caso, os quadrados vermelho e amarelo são posicionados ao lado direito e esquerdo do quadrado azul, respectivamente.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estrutura básica</span>"
    ]
  },
  {
    "objectID": "p1chap01.html#sec-play",
    "href": "p1chap01.html#sec-play",
    "title": "1  Estructura básica",
    "section": "1.4 Método Scene.play()",
    "text": "1.4 Método Scene.play()\nA segunda maneira, que oferece mais flexibilidade e controle, é através do método Scene.play(). Este método, ao contrário do Scene.add(), permite que você adicione um Mobject à cena de forma animada, utilizando transições, efeitos e um tempo de duração definidos. Com ele, é possível criar movimentos suaves, alterações de posição, escala, rotação e outros tipos de transformações visuais que enriquecem a apresentação gráfica. Esse método será discutido em maior detalhe mais adiante no tutorial, onde exploraremos suas funcionalidades e o impacto que ele pode ter no dinamismo de uma animação.\nEnquanto o Scene.add() é prático para situações em que a simplicidade e a rapidez são os principais objetivos, o Scene.play() oferece uma gama muito maior de possibilidades, permitindo a criação de animações fluidas e envolventes. Ambos os métodos têm seus usos específicos, dependendo da necessidade do projeto em questão. No decorrer deste tutorial, veremos exemplos práticos que ilustram como e quando utilizar cada um desses métodos, garantindo que você tenha o domínio necessário para criar animações eficientes e atraentes.\n\nExemplo 1.3 Criação de três quadrados coloridos usando o método Scene.play().\n\nfrom manim import *\n\nclass CenaAnimada(Scene):\n    def construct(self):\n        \n        texto = Text(\"Quadrados coloridos\")\n        \n        q1 = Square(\n            color=BLUE_E,\n            fill_color=BLUE_E,\n            fill_opacity=1\n        )\n        q2 = q1.copy()\n        q2.set_color(RED)\n\n        q3 = q1.copy()\n        q3.set_color(YELLOW_E)\n\n        self.play(FadeIn(texto))\n        self.play(texto.animate.to_edge(UP))\n        self.wait(0.5)\n        self.play(FadeIn(q1), FadeIn(q2), FadeIn(q3))\n        self.play(q2.animate.next_to(q1, RIGHT), q3.animate.next_to(q1, LEFT))\n        self.wait()\n\n\n\n\n\n\n\n\nResultado\n\n\n\n\n\n\n\n\n\n\n\nFigura 1.2: Quadrados coloridos usando o método Scene.play().\n\n\n\n\n\n\nLinhas 19-20. Utilizamos uma transição relativamente lenta e suave para mostrar o texto e solicitamos que o posicione na borda superior;\nLinha 21. Adicionamos uma pausa de 0.5 segundos. A duração padrão do método Scene.wait() é 1 segundo;\nLinha 22-23. Mostrados os quadrados com uma transição lenta e posicionamos os objetos na cena.",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Estructura básica</span>"
    ]
  },
  {
    "objectID": "p1chap02.html#sec-geometry",
    "href": "p1chap02.html#sec-geometry",
    "title": "2  Objetos em manim",
    "section": "",
    "text": "2.1.1 Classe Retangle\nclass Rectangle(color=ManimColor('#FFFFFF'), height=2.0, width=4.0, grid_xstep=None, grid_ystep=None, **kwargs)\nParâmetros\n\ncolor: Define a cor do retângulo. O padrão da cor é branco;\nheight (float): A altura vertical do retângulo. O padrão da altura são 2 unidades;\nwidth (float): A largura horizontal do retângulo. O padrão da largura são 4 unidades;\ngrid_xstep (float | None): Espaço entre linhas de grade verticais;\ngrid_ystep (float | None): Espaço entre linhas de grade horizontais;\nkwargs: Argumentos adicionais a serem passados à classe Polygon.\n\n\nExemplo 2.1 Criação de três quadrados coloridos usando o método Scene.add().\n\nfrom manim import *\n\nclass Retangulos(Scene):\n    def construct(self):\n        retangulo1 = Rectangle(color=ORANGE,\n                               fill_color=BLUE,\n                               fill_opacity=0.3\n                            )\n        \n        retangulo2 = Rectangle(color=TEAL,\n                          width=1.0, \n                          height=4.0,\n                          grid_xstep=0.5, \n                          grid_ystep=0.5)\n        \n        retangulo3 = Rectangle(color=PURPLE,\n                          width=2.0, \n                          height=2.0, \n                          grid_xstep=1.0, \n                          grid_ystep=1.0)\n        \n        retangulo3.set_stroke(width=10)\n        retangulo3.grid_lines.set_stroke(width=1)\n\n        retangulos = Group(retangulo1, retangulo2, retangulo3).arrange(buff=1)\n        self.add(retangulos)\n\n\n\n\n\n\n\nResultado\n\n\n\n\n\n\n\n\n\n\n\nFigura 2.1: Diferentes retângulos criados em manim.\n\n\n\n\n\n\nLinhas 6-7. O parâmetro fill_color permite definir a cor de preenchimento do objeto e fill_opacity define a transparência da cor de preechimento. Caso não seja definida uma cor de preechimento, o padrão é a mesma cor do objeto. O parâmetro fill_opacity é definido com um valor entre 0 e 1. Quanto mais próximo de 1, mais sólida a cor;\nLinhas 22-23. O método mobject.set_stroke() permite alterar as propriedades de espessura das linhas que definem o objeto. No exemplo, o método é utilizado para modificar a espessura das linhas do retângulo e da grade interior;\nLinha 25. Atribuimos à variável retangulos um grupo de objetos e aplicamos o método mobject.arrange() para posicioná-los na cena.\n\n\nExemplo 2.2 Em particular, quando \\(r=1\\), \\(M_1'=\\frac1n\\sum_{i=1}^nX_i=\\bar{X}_n\\), i.e., o primeiro momento amostral em torno de zero é a média amostral. Por outra parte, quando \\(r=2\\), \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\).\n\n\n\n\n\n\n\nObservação\n\n\n\nSeguindo a ?def-estatistica, podemos concluir que os momentos amostrais são exemplos de Estatísticas e podem ser utilizados para estimar os parâmetros da população.\n\n\n\nExemplo 2.3 Calcule \\(\\mathbb{E}\\left[(M'_1-\\mu)^3\\right]\\) e \\(\\mathbb{E}\\left[(M'_1-\\mu)^4\\right]\\), onde \\(M'_1=\\bar{X}_n\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nPara calcular os momentos centrais de terceira e quarta ordem de \\(M'_1=\\bar{X}_n\\), temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^3\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^3\\right]\n  =\\frac{1}{n^3}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^3]=\\frac{\\mu_3}{n^2}. \\quad\\checkmark\n\\end{align*}\\]\n\nPor outro lado, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[(M'_1-\\mu)^4\\right]&=\\mathbb{E}\\left[(\\bar{X}_n-\\mu)^4\\right]\n  =\\frac{1}{n^4}\\sum_{i=1}^n\\mathbb{E}[(X_i-\\mu)^4]+\\frac{6}{n^4}+\\underset{i\\ne j}{\\sum_{i=1}^n\\sum_{j=1}^n}\\mathbb{E}\\left[(X_i-\\mu)^2(X_j-\\mu)^2\\right]\\\\\n  &=\\frac{\\mu_4}{n^3}+3\\frac{(n-1)\\mu_2^2}{n^3}. \\quad\\checkmark\n\\end{align*}\\]\n\n\n\n\n\nExemplo 2.4 Seja \\(X\\) uma variável aleatória de uma população com distribuição qui-quadrado com \\(k\\) graus de liberdade. Encontre uma expressão para o \\(r\\)-ésimo momento para a distribuição de \\(X\\). O \\(r\\)-ésimo momento existe para todos os valores de \\(r\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, se a distribuição de \\(X\\) é qui-quadrado, então \\[f(x)=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\mathbb{I}_{(0,\\infty)}(x).\\] Dessa forma, temos que\n\n\\[\\begin{align*}\n  \\mathbb{E}[X^r]&=\\int_0^\\infty x^r\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}x^{k/2-1}e^{-\\frac12x}\\,dx\n  =\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\int_0^\\infty x^rx^{k/2-1}e^{-\\frac12x}\\,dx\\\\\n  &=\\frac{1}{\\Gamma\\left(\\frac{k}{2}\\right)}\\left(\\frac12\\right)^{k/2}\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\left(\\frac12\\right)^{k/2+r}}\\,dx, \\quad r&gt;-\\frac{k}{2}\\\\\n  &=\\frac{\\Gamma\\left(\\frac{k}{2}+r\\right)}{\\Gamma\\left(\\frac{k}{2}\\right)}2^r. \\quad\\checkmark\n\\end{align*}\\]\n\n\nA última integral é resultado de \\(\\int_0^\\infty\\frac{1}{\\Gamma(\\alpha)}\\beta^\\alpha x^{\\alpha-1}e^{-\\beta x}\\,dx=1\\) ou \\(\\int_0^\\infty x^{\\alpha-1}e^{-\\beta x}\\,dx=\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\). Essa identidade não é mais do que a densidade de uma variável aleatória com distribuição gama com parâmetros \\(\\alpha\\) e \\(\\beta\\).\n\n\n\n\n\nExemplo 2.5 Seja \\(X_1,X_2, \\ldots, X_n\\) uma sequência de variáveis aleatórias independentes função de distribuição \\(F_i(x_i)\\), com \\(i=1,2,3,,\\ldots,k\\). Considere a variável aleatória \\(Z=-2\\sum_{i=1}^k\\log\\left(1-F_i(X_i)\\right)\\). Qual o valor da curtose de \\(Z\\)?\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSeja \\(U_i=F_i(X_i)\\) e considere \\(k=1\\), então \\(Z=-2\\log(1-U_1)\\). Dessa forma, temos que \\[F_Z(z)=\\mathbb{P}[Z\\le z]=\\mathbb{P}\\left[-2\\log(1-U_1)\\le z\\right]=\\mathbb{P}\\left[U_1\\le 1-e^{-\\frac12z}\\right]=1-e^{-\\frac12z}.\\] Do Exemplo 2.4, sabemos que \\(Z\\) é uma variável aleatória de uma população com distribuição qui-quadrado com \\(2\\) graus de liberdade, i.e., \\(\\chi^2_{(2)}\\). Portanto, \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\).\nPor outra parte, sabemos que a curtose é definida como \\(C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^2\\right)^2}=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}\\). Desse modo,\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z]&=k\\\\\n  \\mathbb{E}\\left[Z^2\\right]&=k(k+2)\\\\\n  \\mathbb{E}\\left[Z^3\\right]&=2k(k+2)\\left(\\frac{k}{2}+2\\right)=k(k+2)(k+4)\\\\\n  \\mathbb{E}\\left[Z^4\\right]&=2k(k+2)(k+4)\\left(\\frac{k}{2}+3\\right)=k(k+2)(k+4)(k+6).\n\\end{align*}\\]\n\nSeguindo os resultados acima, temos que \\(\\mathrm{var}[Z]=\\mathbb{E}Z^2-\\left[\\mathbb{E}Z\\right]^2=k(k+2)-k^2=2k\\). Ainda,\n\n\\[\\begin{align*}\n  \\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4&=\\mathbb{E}\\left[Z-k\\right]^4\n  =\\mathbb{E}Z^4-4k\\mathbb{E}Z^3+6k^2\\mathbb{E}Z^2-4k^3\\mathbb{E}Z+k^4\\\\\n  &=k(k+2)(k+4)(k+6)-4k^2(k+2)(k+4)+6k^2k(k+2)-3k^4\\\\\n  &=-3k(k^2-4)(k+4)+3k^3(k+4)=12k(k+4).\n\\end{align*}\\]\n\nAssim, \\[C=\\dfrac{\\mathbb{E}\\left[Z-\\mathbb{E}Z\\right]^4}{\\left(\\mathrm{var}[Z]\\right)^2}=\\dfrac{12k(k+4)}{4k^2}=3+\\frac{12}{k}.  \\quad\\checkmark\\]\n\nPara mostrar que \\(Z=-2\\sum_{i=1}^k\\log\\left(1-U_i\\right)\\) segue uma distribuição \\(\\chi^2_{(2k)}\\), o leitor pode aplicar o Teorema 2.5.\n\n\n\n\n\nExemplo 2.6 Seja \\(X\\) uma variável aleatória de uma população com distribuição normal com média \\(\\mu\\) e variância \\(1\\). Mostre que o \\(r+1\\)-ésimo momento da distribuição de \\(X\\) é dado por \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\]\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que \\[\\mathbb{E}X^r=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}\\,dx.\\] Daí,\n\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\mathbb{E}X^r&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial\\mu}\\left(x^re^{-\\frac12(x-\\mu)^2}\\right)\\,dx\n    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^re^{-\\frac12(x-\\mu)^2}(\\mu-x)\\,dx\\\\\n    &=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\mu x^re^{-\\frac12(x-\\mu)^2}\\,dx-\\underbrace{\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty x^{r+1}e^{-\\frac12(x-\\mu)^2}\\,dx}_{\\mathbb{E}X^{r+1}}.\n  \\end{align*}\\]\n\nPortanto, \\[\\mathbb{E}X^{r+1}=\\mu\\mathbb{E}X^r-\\frac{\\partial}{\\partial\\mu}\\mathbb{E}X^r.\\quad\\checkmark\\]\n\n\n\n\n\nTeorema 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). O valor esperado do \\(r\\)-ésimo momento amostral, em torno de zero, é igual ao \\(r\\)-ésimo momento populacional (se existe), i.e., \\[\\mathbb{E}[M_r']=\\mu_r'=\\mathbb{E}X^r.\\] Ainda, \\[\\mathrm{Var}[M_r']=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right].\\]\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPara provar o resultado do Teorema 2.1, sabemos da ?def-momento_amostral0 que \\(M_r'=\\frac1n\\sum_{i=1}^nX_i^r\\). Daí, \\[\\mathbb{E}M_r'=\\mathbb{E}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac1n\\sum_{i=1}^n\\mathbb{E}X_i^r=\\mu_r'.\\] Por outro lado, \\[\\mathrm{Var}[M_r']=\\mathrm{Var}\\left[\\frac1n\\sum_{i=1}^nX_i^r\\right]=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{Var}[X_i^r]=\\frac1n\\left[\\mathbb{E}X^{2r}-\\left(\\mathbb{E}X^r\\right)^2\\right]. \\quad \\blacksquare\\]\n\n\n\n\n\nCorolário 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Seja \\(\\bar{X}_n=\\frac1n\\sum_{i=1}^nX_i\\) a média amostral, então \\(\\mathbb{E}\\bar{X}_n=\\mu\\) e \\(\\mathrm{Var}[\\bar{X}_n]=\\frac{\\sigma^2}{n}\\).\n\n\n\nDefinição 2.1 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição que envolve um parâmetro \\(\\theta\\). A variância amostral define-se como: \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2, \\quad n&gt;1.\\]\n\n\n\n\n\n\n\nObservação\n\n\n\nEm particular, \\(M_2=\\frac1n\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\) também é considerada uma medida para quantificar a dispersão. A longo do texto, \\(S_n^2\\) será considerada a variância amostral porque seu valor esperado é a variância populacional. É importante saber que, quando o tamanho de amostra é suficientemente grande, as propriedades estatísticas de \\(M_2\\) e \\(S^2\\) são equivalentes.\n\n\n\nExemplo 2.7 Mostre que \\(S_n^{2}=\\frac{1}{2n(n-1)} \\sum_{i=1}^{n}\\sum_{j=1}^{n} (X_{i} - X_{j})^2\\), para \\(n&gt;1\\).\n\n\n\n\n\n\n\nSolução.\n\n\n\n\n\nSabemos que, \\(S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\), para \\(n&gt;1\\). Dessa forma, para algum \\(X_j\\), temos que\n\n\\[\\begin{align*}\n    (n-1)S_n^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\sum_{i=1}^n(X_i-X_j+X_j-\\bar{X}_n)^2\\\\\n    &=\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right].\n  \\end{align*}\\]\n\nSomando com respeito a \\(j\\), temos que:\n\n\\[\\begin{align*}\n    \\sum_{j=1}^n(n-1)S_n^2&=n(n-1)S_n^2=\\sum_{j=1}^n\\sum_{i=1}^n\\left[(X_i-X_j)^2+(X_j-\\bar{X}_n)^2+2(X_i-X_j)(X_j-\\bar{X}_n)\\right]\\\\\n    &=\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)^2+\\underbrace{\\sum_{j=1}^n\\sum_{i=1}^n(X_j-\\bar{X}_n)^2}_{n(n-1)S_n^2}+2\\sum_{j=1}^n\\sum_{i=1}^n(X_i-X_j)(X_j-\\bar{X}_n).\n  \\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n    \\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)^2&=-2\\sum_{i=1}^n\\sum_{j=1}^n(X_i-X_j)(X_j-\\bar{X}_n)\n    = 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n + \\bar{X}_n - X_{i})(X_{j} - \\bar{X}_n) \\\\\n    &= 2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)^{2} + 2 \\sum_{i=1}^{n} (\\bar{X}_n - X_{i}) \\underbrace{\\sum_{j=1}^{n} (X_{j} - \\bar{X}_n)}_{0} = 2n(n - 1)S_n^{2}. \\quad \\checkmark\n  \\end{align*}\\]\n\n\n\n\n\n\nTeorema 2.2 Seja \\(X_1,X_2,\\ldots,X_n\\) uma amostra aleatória de uma população com distribuição com média \\(\\mu\\) e variância \\(\\sigma^2\\). Considere \\(S_n^2\\), como na Definição 2.1. Então, \\(\\mathbb{E}S_n^2=\\sigma^2\\) e \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\), para \\(n&gt;1\\), onde \\(\\mu_4=\\mathbb{E}(X_i-\\mu)^4\\) e \\(\\mathbb{E}X_i=\\mu\\), para \\(i=1,2,3,\\ldots,n\\).\n\n\n\n\n\n\n\n\nProva.\n\n\n\n\n\nPrimeiramente, note que\n\n\\[\\begin{align*}\n  \\sum_{i=1}^n(X_i-\\mu)^2&=\\sum_{i=1}^n(X_i-\\bar{X}_n+\\bar{X}_n-\\mu)^2=\n  \\sum_{i=1}^n(X_i-\\bar{X}_n)^2+\\sum_{i=1}^n(\\bar{X}_n-\\mu)^2+2\\sum_{i=1}^n(X_i-\\bar{X}_n)(\\bar{X}_n-\\mu)\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2+2(\\bar{X}_n-\\mu)\\underbrace{\\sum_{i=1}^n(X_i-\\bar{X}_n)}_{0}\\\\\n  &=\\sum_{i=1}^n(X_i-\\bar{X}_n)^2+n(\\bar{X}_n-\\mu)^2.\n\\end{align*}\\]\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}S_n^2&=\\mathbb{E}\\left[\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2\\right]\n=\\frac{1}{n-1}\\mathbb{E}\\left[\\sum_{i=1}^n(X_i-\\mu)^2-n(\\bar{X}_n-\\mu)^2\\right]\\\\\n&=\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\mathbb{E}(X_i-\\mu)^2-n\\mathbb{E}(\\bar{X}_n-\\mu)^2\\right\\}=\n\\frac{1}{n-1}\\left\\{\\sum_{i=1}^n\\sigma^2-n\\frac1n\\sigma^2\\right\\}\\\\\n&=\\frac{1}{n-1}\\left\\{n\\sigma^2-\\sigma^2\\right\\}=\\sigma^2, \\quad n&gt;1. \\quad \\checkmark\n\\end{align*}\\]\n\nPor outra parte, para mostrar \\(\\mathrm{Var}[S_n^2]=\\frac1n\\left(\\mu_4-\\frac{n-3}{n-1}\\sigma^4\\right)\\) considere \\(Z_i=X_i-\\mu\\), para \\(i=1,2,\\ldots,n\\). A sequência \\(Z_1,Z_2,\\ldots,Z_n\\) representa uma amostra aleatória, tal que, para \\(i\\ne j\\ne k\\ne l\\), temos:\n\n\\[\\begin{align*}\n  \\mathbb{E}[Z_i]&=0; & \\mathbb{E}[Z_iZ_j]&=0; & \\mathbb{E}[Z_i^3Z_j]&=0; & \\mathbb{E}[Z_i^2Z_jZ_k]&=0;\\\\\n  \\mathbb{E}[Z_i^2]&=\\sigma^2; &\\mathbb{E}[Z_i^4]&=\\mu_4; & \\mathbb{E}[Z_i^2Z_j^2]&=\\mu_2^2; & \\mathbb{E}[Z_iZ_jZ_k,Z_l]&=0.\n\\end{align*}\\]\n\nPor outro lado, de acordo com o resultado da Seção A.1 do Apêndice A, temos que:\n\nO segundo momento de \\(\\sum_{i=1}^nZ_i^2\\) é dado por\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]^2&=\\mathrm{Var}\\left[\\sum_{i=1}^nZ_i^2\\right]+\\left(\\mathbb{E}\\left[\\sum_{i=1}^nZ_i^2\\right]\\right)^2=\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i^2\\right]+\\left(\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^2\\right]\\right)^2\\\\\n&=\\sum_{i=1}^n\\mathbb{E}\\left[Z_i^4\\right]-\\sum_{i=1}^n\\left(\\mathbb{E}\\left[Z_i^2\\right]\\right)^2+\\left(\\sum_{i=1}^n\\mathrm{Var}\\left[Z_i\\right]+\\sum_{i=1}^n\\left(\\mathbb{E}[Z_i]\\right)^2\\right)^2\\\\\n&=n\\mu_4-\\sum_{i=1}^n\\left(\\mathrm{Var}[Z_i]\\right)^2+\\left(\\sum_{i=1}^n\\mu_2\\right)^2=n\\mu_4-n\\mu_2^2+n^2\\mu_2^2=n\\mu_4+n(n-1)\\mu_2^2.  \\quad \\checkmark\n\\end{align*}\\]\n\nTambém temos que,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^n\\sum_{l=1}^nZ_iZ_jZ_kZ_l\\right].\n\\end{align*}\\]\n\nPara calcular a soma, devemos avaliar todos os casos possíveis onde os índices \\((i,j,k,l)\\) podem tomar diferentes valores:\n\nSe houver algum índice diferente de todos os outros índices, e.g, \\(i\\ne j=k=l\\), então o valor esperado é zero;\nSe todos os índices forem iguais, i.e. \\(i=j=k=l\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\). Ressaltando que existem \\(n\\) maneiras de isso acontecer;\nSe tiver dois pares distintos de índices iguais, e.g, \\(i=j\\ne k=l\\), nesse caso, suponha que \\(i=j\\) e \\(k=l\\), com \\(i&lt;k\\), então \\[\\mathbb{E}[Z_i^2Z_k^2]=\\mathbb{E}[Z_i^2]\\mathbb{E}[Z_k^2]=\\mu_2^2.\\] Ressaltando que existem \\(\\binom{n}{2}\\binom{4}{2}=\\frac{n!}{(n-2)!2!}\\frac{4!}{2!2!}=3n(n-1)\\) maneiras de acontecer dois pares de índices iguais.\n\nDessa forma, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right]^2=n\\mu_4+3n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\nAinda, temos que\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^n\\sum_{j=1}^nZ_iZ_j\\right)\\right]=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right].\n\\end{align*}\\]\n\nDe igual forma que no caso anterior, devem ser avaliados os casos onde os índices \\((i,j,k)\\) podem tomar valores diferentes:\n\nPara \\(j\\ne k\\), o valor esperado é \\(0\\);\nQuando \\(i=j=k\\), então \\(\\mathbb{E}Z_i^4=\\mu_4\\) e isso acontece de \\(n\\) maneiras;\nQuando \\(i\\ne j=k\\), o valor esperado é \\(\\mu_2^2\\), mas note que agora a ordem \\(i,j\\) importa, então, temos \\(n(n-1)\\) formas de obter esse valor esperado.\n\nPor tanto,\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]&=\\mathbb{E}\\left[\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k=1}^nZ_i^2Z_jZ_k\\right]=n\\mu_4+n(n-1)\\mu_2^2. \\quad \\checkmark\n\\end{align*}\\]\n\n\nFinalmente, para calcular a variância de \\[S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X}_n)^2=\\frac{n\\sum_{i=1}^nZ_i^2-\\left(\\sum_{i=1}^nZ_i\\right)^2}{n(n-1)},\\] sabemos que \\(\\mathrm{Var}[S_n^2]=\\mathbb{E}S_n^4-\\left(\\mathbb{E}S_n^2\\right)^2=\\mathbb{E}S_n^4-\\mu_2^2\\), onde\n\n\\[\\begin{align*}\n\\mathbb{E}\\left[S_n^4\\right]&=\\frac{n^2\\mathbb{E}\\left(\\sum_{i=1}^nZ_i^2\\right)^2-2n\\mathbb{E}\\left[\\left(\\sum_{i=1}^nZ_i^2\\right)\\left(\\sum_{i=1}^nZ_i\\right)^2\\right]+\\mathbb{E}\\left[\\sum_{i=1}^nZ_i\\right]^4}{n^2(n-1)^2}\\\\\n&=\\frac{n^2(n\\mu_4+n(n-1)\\mu_2^2)-2n(n\\mu_4+n(n-1)\\mu_2^2)+n\\mu_4+3n(n-1)\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{(n^3-2n^2+n)\\mu_4+(n^3(n-1)-2n^2(n-1)+3n(n-1))\\mu_2^2}{n^2(n-1)^2}\\\\\n&=\\frac{n(n-1)^2\\mu_4+n(n-1)(n^2-2n+3)\\mu_2^2}{n^2(n-1)^2}=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}.\n\\end{align*}\\]\n\nDaí,\n\n\\[\\begin{align*}\n\\mathrm{Var}\\left[S_n^2\\right]&=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2}{n(n-1)}-\\mu_2^2=\\frac{(n-1)\\mu_4+(n^2-2n+3)\\mu_2^2-n(n-1)\\mu_2^2}{n(n-1)}\\\\\n&=\\frac{(n-1)\\mu_4-(n-3)\\mu_2^2}{n(n-1)}=\\frac1n\\left\\{\\mu_4-\\frac{n-3}{n-1}\\mu_2^2\\right\\}. \\quad\\blacksquare\n\\end{align*}\\]",
    "crumbs": [
      "I. Elementos básicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objetos em `manim`</span>"
    ]
  }
]